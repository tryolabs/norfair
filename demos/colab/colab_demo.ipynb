{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/tryolabs/norfair/blob/master/demos/colab/colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw objects paths and track camera movement with Norfair\n",
    "\n",
    "Run a demo similar to the [Hugging Face Spaces Norfair demo](https://huggingface.co/spaces/tryolabs/norfair-demo) in this notebook.\n",
    "\n",
    "This demo uses the YOLOv7 model and shows the Norfair features to draw the object's paths and track camera movement.\n",
    "\n",
    "Tack camera movement is useful to improve the tracker and also to keep the paths fixed although the camera movements.\n",
    "\n",
    "The demo will use the following video by default, but you can change which video you use by changing the url in [this cell](#Download-Video-and-Preprocessing). We trim the video to only a few seconds due to limitations with video playback in Google Colab, but you can play with these limitations and see what you get.\n",
    "\n",
    "[![](https://user-images.githubusercontent.com/67343574/191318965-b7c224d7-73b0-49f7-840a-b1c9d8534a06.png)](https://user-images.githubusercontent.com/67343574/191318965-b7c224d7-73b0-49f7-840a-b1c9d8534a06.png)\n",
    "\n",
    "**Note**\n",
    "\n",
    "- Set the hardware accelerator setting of Colaboratory to **GPU** and execute it.\n",
    "(Runtime -> Change Runtime Type -> GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget \"https://raw.githubusercontent.com/tryolabs/norfair/master/demos/colab/requirements.txt\" -O requirements.txt\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download [Video](https://www.youtube.com/watch?v=aio9g9_xVio) and Preprocessing\n",
    "We cut the video short because it's too long to play in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget \"https://drive.google.com/u/0/uc?id=1Jc5TAiwOZ-yUO6R_tG0zSW9Niv_HKTPV&export=download\" -O sample.mp4\n",
    "! ffmpeg -i sample.mp4 -ss 7 -t 10 sample_10s.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and run the demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we only look at people, you can change this with the `classes` parameter following the Coco labels ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget \"https://raw.githubusercontent.com/tryolabs/norfair/master/demos/colab/demo.py\"\n",
    "! wget \"https://raw.githubusercontent.com/tryolabs/norfair/master/demos/colab/draw.py\"\n",
    "! wget \"https://raw.githubusercontent.com/tryolabs/norfair/master/demos/colab/yolo.py\"\n",
    "\n",
    "! python demo.py sample_10s.mp4 --classes 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert mp4 to webm\n",
    "\n",
    "\n",
    "Reference: [StackOverflow - python-opencv-video-format-play-in-browser](https://stackoverflow.com/questions/49530857/python-opencv-video-format-play-in-browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ffmpeg -i ./sample_10s_out.mp4 -vcodec vp9 ./sample.webm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the Drawing Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "with  io.open('sample.webm','r+b') as f:\n",
    "    mp4 = f.read()\n",
    "data_url = \"data:video/webm;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=800 controls>\n",
    "      <source src=\"%s\" type=\"video/webm\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('3.9.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1d45f7b56f6e27d41b86676aa8ae2293c110fadaa7f6b0b931d437bdf9db7e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
