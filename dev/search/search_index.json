{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Norfair is a customizable lightweight Python library for real-time multi-object tracking.</p> <p>Using Norfair, you can add tracking capabilities to any detector with just a few lines of code.</p> Tracking players with moving camera Tracking 3D objects"},{"location":"#features","title":"Features","text":"<ul> <li> <p>Any detector expressing its detections as a series of <code>(x, y)</code> coordinates can be used with Norfair. This includes detectors performing tasks such as object or keypoint detection (see examples).</p> </li> <li> <p>Modular. It can easily be inserted into complex video processing pipelines to add tracking to existing projects. At the same time, it is possible to build a video inference loop from scratch using just Norfair and a detector.</p> </li> <li> <p>Supports moving camera, re-identification with appearance embeddings, and n-dimensional object tracking (see Advanced features).</p> </li> <li> <p>Norfair provides several predefined distance functions to compare tracked objects and detections. The distance functions can also be defined by the user, enabling the implementation of different tracking strategies.</p> </li> <li> <p>Fast. The only thing bounding inference speed will be the detection network feeding detections to Norfair.</p> </li> </ul> <p>Norfair is built, used and maintained by Tryolabs.</p>"},{"location":"#installation","title":"Installation","text":"<p>Norfair currently supports Python 3.8+. The latest tested version to support Python 3.7 is Norfair 2.2.0. Later versions may work, but no specific support is planned.</p> <p>For the minimal version, install as:</p> <pre><code>pip install norfair\n</code></pre> <p>To make Norfair install the dependencies to support more features, install as:</p> <pre><code>pip install norfair[video]  # Adds several video helper features running on OpenCV\npip install norfair[metrics]  # Supports running MOT metrics evaluation\npip install norfair[metrics,video]  # Everything included\n</code></pre> <p>If the needed dependencies are already present in the system, installing the minimal version of Norfair is enough for enabling the extra features. This is particularly useful for embedded devices, where installing compiled dependencies can be difficult, but they can sometimes come preinstalled with the system.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Getting started guide.</p> <p>Official reference.</p>"},{"location":"#examples-demos","title":"Examples &amp; demos","text":"<p>We provide several examples of how Norfair can be used to add tracking capabilities to different detectors, and also showcase more advanced features.</p> <p>Note: for ease of reproducibility, we provide Dockerfiles for all the demos. Even though Norfair does not need a GPU, the default configuration of most demos requires a GPU to be able to run the detectors. For this, make sure you install NVIDIA Container Toolkit so that your GPU can be shared with Docker.</p> <p>It is possible to run several demos with a CPU, but you will have to modify the scripts or tinker with the installation of their dependencies.</p>"},{"location":"#adding-tracking-to-different-detectors","title":"Adding tracking to different detectors","text":"<p>Most tracking demos are showcased with vehicles and pedestrians, but the detectors are generally trained with many more classes from the COCO dataset.</p> <ol> <li>YOLOv7: tracking object centroids or bounding boxes.</li> <li>YOLOv5: tracking object centroids or bounding boxes.</li> <li>YOLOv4: tracking object centroids.</li> <li>Detectron2: tracking object centroids.</li> <li>AlphaPose: tracking human keypoints (pose estimation) and inserting Norfair into a complex existing pipeline using.</li> <li>OpenPose: tracking human keypoints.</li> <li>YOLOPv2: tracking with a model for traffic object detection, drivable road area segmentation, and lane line detection.</li> <li>YOLO-NAS: tracking object centroids or bounding boxes.</li> </ol>"},{"location":"#advanced-features","title":"Advanced features","text":"<ol> <li>Speed up pose estimation by extrapolating detections using OpenPose.</li> <li>Track both bounding boxes and human keypoints (multi-class), unifying the detections from a YOLO model and OpenPose.</li> <li>Re-identification (ReID) of tracked objects using appearance embeddings. This is a good starting point for scenarios with a lot of occlusion, in which the Kalman filter alone would struggle.</li> <li>Accurately track objects even if the camera is moving, by estimating camera motion potentially accounting for pan, tilt, rotation, movement in any direction, and zoom.</li> <li>Track points in 3D, using MediaPipe Objectron.</li> <li>Tracking of small objects, using SAHI: Slicing Aided Hyper Inference.</li> </ol>"},{"location":"#ros-integration","title":"ROS integration","text":"<p>To make it even easier to use Norfair in robotics projects, we now offer a version that integrates with the Robotic Operating System (ROS).</p> <p>We present a ROS package and a fully functional environment running on Docker to do the first steps with this package and start your first application easier.</p>"},{"location":"#benchmarking-and-profiling","title":"Benchmarking and profiling","text":"<ol> <li>Kalman filter and distance function profiling using TRT pose estimator.</li> <li>Computation of MOT17 scores using motmetrics4norfair.</li> </ol>"},{"location":"#how-it-works","title":"How it works","text":"<p>Norfair works by estimating the future position of each point based on its past positions. It then tries to match these estimated positions with newly detected points provided by the detector. For this matching to occur, Norfair can rely on any distance function. There are some predefined distances already integrated in Norfair, and the users can also define their own custom distances. Therefore, each object tracker can be made as simple or as complex as needed.</p> <p>As an example we use Detectron2 to get the single point detections to use with this distance function. We just use the centroids of the bounding boxes it produces around cars as our detections, and get the following results.</p> <p></p> <p>On the left you can see the points we get from Detectron2, and on the right how Norfair tracks them assigning a unique identifier through time. Even a straightforward distance function like this one can work when the tracking needed is simple.</p> <p>Norfair also provides several useful tools for creating a video inference loop. Here is what the full code for creating the previous example looks like, including the code needed to set up Detectron2:</p> <pre><code>import cv2\nimport numpy as np\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\n\nfrom norfair import Detection, Tracker, Video, draw_tracked_objects\n\n# Set up Detectron2 object detector\ncfg = get_cfg()\ncfg.merge_from_file(\"demos/faster_rcnn_R_50_FPN_3x.yaml\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\ncfg.MODEL.WEIGHTS = \"detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl\"\ndetector = DefaultPredictor(cfg)\n\n# Norfair\nvideo = Video(input_path=\"video.mp4\")\ntracker = Tracker(distance_function=\"euclidean\", distance_threshold=20)\n\nfor frame in video:\n    detections = detector(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    detections = [Detection(p) for p in detections['instances'].pred_boxes.get_centers().cpu().numpy()]\n    tracked_objects = tracker.update(detections=detections)\n    draw_tracked_objects(frame, tracked_objects)\n    video.write(frame)\n</code></pre> <p>The video and drawing tools use OpenCV frames, so they are compatible with most Python video code available online. The point tracking is based on SORT generalized to detections consisting of a dynamically changing number of points per detection.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Trying out the latest state-of-the-art detectors normally requires running repositories that weren't intended to be easy to use. These tend to be repositories associated with a research paper describing a novel new way of doing detection, and they are therefore intended to be run as a one-off evaluation script to get some result metric to publish on a particular research paper. This explains why they tend to not be easy to run as inference scripts, or why extracting the core model to use in another standalone script isn't always trivial.</p> <p>Norfair was born out of the need to quickly add a simple layer of tracking over a wide range of newly released SOTA detectors. It was designed to seamlessly be plugged into a complex, highly coupled code base, with minimum effort. Norfair provides a series of modular but compatible tools, which you can pick and choose to use in your project.</p>"},{"location":"#comparison-to-other-trackers","title":"Comparison to other trackers","text":"<p>Norfair's contribution to Python's object tracker library repertoire is its ability to work with any object detector by being able to work with a variable number of points per detection, and the ability for the user to heavily customize the tracker by creating their own distance function.</p> <p>If you are looking for a tracker, here are some other projects worth noting:</p> <ul> <li>OpenCV includes several tracking solutions like KCF Tracker and MedianFlow Tracker which are run by making the user select a part of the frame to track, and then letting the tracker follow that area. They tend not to be run on top of a detector and are not very robust.</li> <li>dlib includes a correlation single object tracker. You have to create your own multiple object tracker on top of it yourself if you want to track multiple objects with it.</li> <li>AlphaPose just released a new version of their human pose tracker. This tracker is tightly integrated into their code base, and to the task of tracking human poses.</li> <li>SORT and Deep SORT are similar to this repo in that they use Kalman filters (and a deep embedding for Deep SORT), but they are hardcoded to a fixed distance function and to tracking boxes. Norfair also adds some filtering when matching tracked objects with detections, and changes the Hungarian Algorithm for its own distance minimizer. Both these repos are also released under the GPL license, which might be an issue for some individuals or companies because the source code of derivative works needs to be published.</li> </ul>"},{"location":"#benchmarks","title":"Benchmarks","text":"<p>MOT17 and MOT20 results obtained using motmetrics4norfair demo script on the <code>train</code> split. We used detections obtained with ByteTrack's YOLOX object detection model.</p> MOT17 Train IDF1 IDP IDR Rcll Prcn MOTA MOTP MOT17-02 61.3% 63.6% 59.0% 86.8% 93.5% 79.9% 14.8% MOT17-04 93.3% 93.6% 93.0% 98.6% 99.3% 97.9% 07.9% MOT17-05 77.8% 77.7% 77.8% 85.9% 85.8% 71.2% 14.7% MOT17-09 65.0% 67.4% 62.9% 90.3% 96.8% 86.8% 12.2% MOT17-10 70.2% 72.5% 68.1% 87.3% 93.0% 80.1% 18.7% MOT17-11 80.2% 80.5% 80.0% 93.0% 93.6% 86.4% 11.3% MOT17-13 79.0% 79.6% 78.4% 90.6% 92.0% 82.4% 16.6% OVERALL 80.6% 81.8% 79.6% 92.9% 95.5% 88.1% 11.9% MOT20 Train IDF1 IDP IDR Rcll Prcn MOTA MOTP MOT20-01 85.9% 88.1% 83.8% 93.4% 98.2% 91.5% 12.6% MOT20-02 72.8% 74.6% 71.0% 93.2% 97.9% 91.0% 12.7% MOT20-03 93.0% 94.1% 92.0% 96.1% 98.3% 94.4% 13.7% MOT20-05 87.9% 88.9% 87.0% 96.0% 98.1% 94.1% 13.0% OVERALL 87.3% 88.4% 86.2% 95.6% 98.1% 93.7% 13.2%"},{"location":"#commercial-support","title":"Commercial support","text":"<p>Tryolabs can provide commercial support, implement new features in Norfair or build video analytics tools for solving your challenging problems. Norfair powers several video analytics applications, such as the face mask detection tool.</p> <p>If you are interested, please contact us.</p>"},{"location":"#citing-norfair","title":"Citing Norfair","text":"<p>For citations in academic publications, please export your desired citation format (BibTeX or other) from Zenodo.</p>"},{"location":"#license","title":"License","text":"<p>Copyright \u00a9 2022, Tryolabs. Released under the BSD 3-Clause.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Norfair's goal is to easily track multiple objects in videos based on the frame-by-frame detections of a user-defined model.</p>"},{"location":"getting_started/#model-or-detector","title":"Model or Detector","text":"<p>We recommend first deciding and setting up the model and then adding Norfair on top of it. Models trained for any form of object detection or keypoint detection (including pose estimation) are all supported. You can check some of the integrations we have as examples:</p> <ul> <li>Yolov7, Yolov5 and Yolov4</li> <li>Detectron2</li> <li>Alphapose</li> <li>Openpose</li> <li>MMDetection</li> </ul> <p>Any other model trained on one of the supported tasks is also supported and should be easy to integrate with Norfair, regardless of whether it uses Pytorch, TensorFlow, or other.</p> <p>If you are unsure of which model to use, Yolov7 is a good starting point since it's easy to set up and offers models of different sizes pre-trained on object detection and pose estimation.</p> <p>Note</p> <p>Norfair is a Detection-Based-Tracker (DBT) and as such, its performance is highly dependent on the performance of the model of choice.</p> <p>The detections from the model will need to be wrapped in an instance of Detection before passing them to Norfair.</p>"},{"location":"getting_started/#install","title":"Install","text":"<p>Installing Norfair is extremely easy, simply run <code>pip install norfair</code> to install the latest version from PyPI.</p> <p>You can also install the latest version from the master branch using <code>pip install git+https://github.com/tryolabs/norfair.git@master#egg=norfair</code></p>"},{"location":"getting_started/#video","title":"Video","text":"<p>Norfair offers optional functionality to process videos (mp4 and mov formats are supported) or capture a live feed from a camera. To use this functionality you need to install Norfair with the <code>video</code> extra using this command: <code>pip install norfair[video]</code>.</p> <p>Check the Video class for more info on how to use it.</p>"},{"location":"getting_started/#tracking","title":"Tracking","text":"<p>Let's dive right into a simple example in the following snippet:</p> <pre><code>from norfair import Detection, Tracker, Video, draw_tracked_objects\n\ndetector = MyDetector()  # Set up a detector\nvideo = Video(input_path=\"video.mp4\")\ntracker = Tracker(distance_function=\"euclidean\", distance_threshold=100)\n\nfor frame in video:\n   detections = detector(frame)\n   norfair_detections = [Detection(points) for points in detections]\n   tracked_objects = tracker.update(detections=norfair_detections)\n   draw_tracked_objects(frame, tracked_objects)\n   video.write(frame)\n</code></pre> <p>The tracker is created and then the detections are fed to it one frame at a time in order. This method is called online tracking and allows Norfair to be used in live feeds and real-time scenarios where future frames are not available.</p> <p>Norfair includes functionality for creating an output video with drawings which is useful for evaluating and debugging. We usually start with this simple setup and move from there.</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<p>The next steps depend a lot on your goal and the result of evaluating the output videos, nevertheless here are some pointers that might help you solve common problems</p>"},{"location":"getting_started/#detection-issues","title":"Detection Issues","text":"<p>Most common problem is that the tracking has errors or is not precise enough. In this case, the first thing to check is whether this is a detection error or a tracking error. As mentioned above if the detector fails the tracking will suffer.</p> <p>To debug this use <code>draw_points</code> or <code>draw_boxes</code> to inspect the detections and analyze if they are precise enough. If you are filtering the detections based on scores, this is a good time to tweak the threshold. If you decide that the detections are not good enough you can try a different architecture, a bigger version of the model, or consider fine-tuning the model on your domain.</p>"},{"location":"getting_started/#tracking-issues","title":"Tracking Issues","text":"<p>After inspecting the detections you might find issues with the tracking, several things can go wrong with tracking but here is a list of common errors and things to try:</p> <ul> <li>Objects take too long to start, this can have multiple causes:<ul> <li><code>initialization_delay</code> is too big on the Tracker. Makes the TrackedObject stay on initializing for too long, <code>3</code> is usually a good value to start with.</li> <li><code>distance_threshold</code> is too small on the Tracker. Prevents the Detections to be matched with the correct TrackedObject. The best value depends on the distance used.</li> <li>Incorrect <code>distance_function</code> on the Tracker. Some distances might not be valid in some cases, for instance, if using IoU but the objects in your video move so quickly that there is never an overlap between the detections of consecutive frames. Try different distances, <code>euclidean</code> or <code>create_normalized_mean_euclidean_distance</code> are good starting points.</li> </ul> </li> <li>Objects take too long to disappear. Lower <code>hit_counter_max</code> on the Tracker.</li> <li>Points or bounding boxes jitter too much. Increase <code>R</code> (measurement error) or lower <code>Q</code> (estimate or process error) on the <code>OptimizedKalmanFilterFactory</code> or <code>FilterPyKalmanFilterFactory</code>. This makes the Kalman Filter put less weight on the measurements and trust more on the estimate, stabilizing the result.</li> <li>Camera motion confuses the Tracker. If the camera moves, the apparent movement of objects can become too erratic for the Tracker. Use <code>MotionEstimator</code>.</li> <li>Incorrect matches between Detections and TrackedObjects, a couple of scenarios can cause this:<ul> <li><code>distance_threshold</code> is too big so the Tracker matches Detections to TrackedObjects that are simply too far. Lower the threshold until you fix the error, the correct value will depend on the distance function that you're using.</li> <li>Mismatches when objects overlap. In this case, tracking becomes more challenging, usually, the quality of the detection degrades causing one of the objects to be missed or creating a single big detection that includes both objects. On top of the detection issues, the tracker needs to decide which detection should be matched to which TrackedObject which can be error-prone if only considering spatial information. The solution is not easy but incorporating the notion of the appearance similarity based on some kind of embedding to your distance_function can help.</li> </ul> </li> <li>Can't recover an object after occlusions. Use ReID distance, see this demo for an example but for real-world use you will need a good ReID model that can provide good embeddings.</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>A customizable lightweight Python library for real-time multi-object tracking.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from norfair import Detection, Tracker, Video, draw_tracked_objects\n&gt;&gt;&gt; detector = MyDetector()  # Set up a detector\n&gt;&gt;&gt; video = Video(input_path=\"video.mp4\")\n&gt;&gt;&gt; tracker = Tracker(distance_function=\"euclidean\", distance_threshold=50)\n&gt;&gt;&gt; for frame in video:\n&gt;&gt;&gt;    detections = detector(frame)\n&gt;&gt;&gt;    norfair_detections = [Detection(points) for points in detections]\n&gt;&gt;&gt;    tracked_objects = tracker.update(detections=norfair_detections)\n&gt;&gt;&gt;    draw_tracked_objects(frame, tracked_objects)\n&gt;&gt;&gt;    video.write(frame)\n</code></pre>"},{"location":"reference/camera_motion/","title":"Camera Motion","text":"<p>Camera motion stimation module.</p>"},{"location":"reference/camera_motion/#norfair.camera_motion.CoordinatesTransformation","title":"<code>CoordinatesTransformation</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class representing a coordinate transformation.</p> <p>Detections' and tracked objects' coordinates can be interpreted in 2 reference:</p> <ul> <li>Relative: their position on the current frame, (0, 0) is top left</li> <li>Absolute: their position on an fixed space, (0, 0)     is the top left of the first frame of the video.</li> </ul> <p>Therefore, coordinate transformation in this context is a class that can transform coordinates in one reference to another.</p> Source code in <code>norfair/camera_motion.py</code> <pre><code>class CoordinatesTransformation(ABC):\n    \"\"\"\n    Abstract class representing a coordinate transformation.\n\n    Detections' and tracked objects' coordinates can be interpreted in 2 reference:\n\n    - _Relative_: their position on the current frame, (0, 0) is top left\n    - _Absolute_: their position on an fixed space, (0, 0)\n        is the top left of the first frame of the video.\n\n    Therefore, coordinate transformation in this context is a class that can transform\n    coordinates in one reference to another.\n    \"\"\"\n\n    @abstractmethod\n    def abs_to_rel(self, points: np.ndarray) -&gt; np.ndarray:\n        pass\n\n    @abstractmethod\n    def rel_to_abs(self, points: np.ndarray) -&gt; np.ndarray:\n        pass\n</code></pre>"},{"location":"reference/camera_motion/#norfair.camera_motion.TransformationGetter","title":"<code>TransformationGetter</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class representing a method for finding CoordinatesTransformation between 2 sets of points</p> Source code in <code>norfair/camera_motion.py</code> <pre><code>class TransformationGetter(ABC):\n    \"\"\"\n    Abstract class representing a method for finding CoordinatesTransformation between 2 sets of points\n    \"\"\"\n\n    @abstractmethod\n    def __call__(\n        self, curr_pts: np.ndarray, prev_pts: np.ndarray\n    ) -&gt; Tuple[bool, CoordinatesTransformation]:\n        pass\n</code></pre>"},{"location":"reference/camera_motion/#norfair.camera_motion.TranslationTransformation","title":"<code>TranslationTransformation</code>","text":"<p>             Bases: <code>CoordinatesTransformation</code></p> <p>Coordinate transformation between points using a simple translation</p> <p>Parameters:</p> Name Type Description Default <code>movement_vector</code> <code>ndarray</code> <p>The vector representing the translation.</p> required Source code in <code>norfair/camera_motion.py</code> <pre><code>class TranslationTransformation(CoordinatesTransformation):\n    \"\"\"\n    Coordinate transformation between points using a simple translation\n\n    Parameters\n    ----------\n    movement_vector : np.ndarray\n        The vector representing the translation.\n    \"\"\"\n\n    def __init__(self, movement_vector):\n        self.movement_vector = movement_vector\n\n    def abs_to_rel(self, points: np.ndarray):\n        return points + self.movement_vector\n\n    def rel_to_abs(self, points: np.ndarray):\n        return points - self.movement_vector\n</code></pre>"},{"location":"reference/camera_motion/#norfair.camera_motion.TranslationTransformationGetter","title":"<code>TranslationTransformationGetter</code>","text":"<p>             Bases: <code>TransformationGetter</code></p> <p>Calculates TranslationTransformation between points.</p> <p>The camera movement is calculated as the mode of optical flow between the previous reference frame and the current.</p> <p>Comparing consecutive frames can make differences too small to correctly estimate the translation, for this reason the reference frame is kept fixed as we progress through the video. Eventually, if the transformation is no longer able to match enough points, the reference frame is updated.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>Before calculatin the mode, optiocal flow is bucketized into bins of this size.</p> <code>0.2</code> <code>proportion_points_used_threshold</code> <code>float</code> <p>Proportion of points that must be matched, otherwise the reference frame must be updated.</p> <code>0.9</code> Source code in <code>norfair/camera_motion.py</code> <pre><code>class TranslationTransformationGetter(TransformationGetter):\n    \"\"\"\n    Calculates TranslationTransformation between points.\n\n    The camera movement is calculated as the mode of optical flow between the previous reference frame\n    and the current.\n\n    Comparing consecutive frames can make differences too small to correctly estimate the translation,\n    for this reason the reference frame is kept fixed as we progress through the video.\n    Eventually, if the transformation is no longer able to match enough points, the reference frame is updated.\n\n    Parameters\n    ----------\n    bin_size : float\n        Before calculatin the mode, optiocal flow is bucketized into bins of this size.\n    proportion_points_used_threshold: float\n        Proportion of points that must be matched, otherwise the reference frame must be updated.\n    \"\"\"\n\n    def __init__(\n        self, bin_size: float = 0.2, proportion_points_used_threshold: float = 0.9\n    ) -&gt; None:\n        self.bin_size = bin_size\n        self.proportion_points_used_threshold = proportion_points_used_threshold\n        self.data = None\n\n    def __call__(\n        self, curr_pts: np.ndarray, prev_pts: np.ndarray\n    ) -&gt; Tuple[bool, TranslationTransformation]:\n        # get flow\n        flow = curr_pts - prev_pts\n\n        # get mode\n        flow = np.around(flow / self.bin_size) * self.bin_size\n        unique_flows, counts = np.unique(flow, axis=0, return_counts=True)\n\n        max_index = counts.argmax()\n\n        proportion_points_used = counts[max_index] / len(prev_pts)\n        update_prvs = proportion_points_used &lt; self.proportion_points_used_threshold\n\n        flow_mode = unique_flows[max_index]\n\n        try:\n            flow_mode += self.data\n        except TypeError:\n            pass\n\n        if update_prvs:\n            self.data = flow_mode\n\n        return update_prvs, TranslationTransformation(flow_mode)\n</code></pre>"},{"location":"reference/camera_motion/#norfair.camera_motion.HomographyTransformation","title":"<code>HomographyTransformation</code>","text":"<p>             Bases: <code>CoordinatesTransformation</code></p> <p>Coordinate transformation beweent points using an homography</p> <p>Parameters:</p> Name Type Description Default <code>homography_matrix</code> <code>ndarray</code> <p>The matrix representing the homography</p> required Source code in <code>norfair/camera_motion.py</code> <pre><code>class HomographyTransformation(CoordinatesTransformation):\n    \"\"\"\n    Coordinate transformation beweent points using an homography\n\n    Parameters\n    ----------\n    homography_matrix : np.ndarray\n        The matrix representing the homography\n    \"\"\"\n\n    def __init__(self, homography_matrix: np.ndarray):\n        self.homography_matrix = homography_matrix\n        self.inverse_homography_matrix = np.linalg.inv(homography_matrix)\n\n    def abs_to_rel(self, points: np.ndarray):\n        ones = np.ones((len(points), 1))\n        points_with_ones = np.hstack((points, ones))\n        points_transformed = points_with_ones @ self.homography_matrix.T\n        last_column = points_transformed[:, -1]\n        last_column[last_column == 0] = 0.0000001\n        points_transformed = points_transformed / last_column.reshape(-1, 1)\n        new_points_transformed = points_transformed[:, :2]\n        return new_points_transformed\n\n    def rel_to_abs(self, points: np.ndarray):\n        ones = np.ones((len(points), 1))\n        points_with_ones = np.hstack((points, ones))\n        points_transformed = points_with_ones @ self.inverse_homography_matrix.T\n        last_column = points_transformed[:, -1]\n        last_column[last_column == 0] = 0.0000001\n        points_transformed = points_transformed / last_column.reshape(-1, 1)\n        return points_transformed[:, :2]\n</code></pre>"},{"location":"reference/camera_motion/#norfair.camera_motion.HomographyTransformationGetter","title":"<code>HomographyTransformationGetter</code>","text":"<p>             Bases: <code>TransformationGetter</code></p> <p>Calculates HomographyTransformation between points.</p> <p>The camera movement is represented as an homography that matches the optical flow between the previous reference frame and the current.</p> <p>Comparing consecutive frames can make differences too small to correctly estimate the homography, often resulting in the identity. For this reason the reference frame is kept fixed as we progress through the video. Eventually, if the transformation is no longer able to match enough points, the reference frame is updated.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Optional[int]</code> <p>One of openCV's method for finding homographies. Valid options are: <code>[0, cv.RANSAC, cv.LMEDS, cv.RHO]</code>, by default <code>cv.RANSAC</code></p> <code>None</code> <code>ransac_reproj_threshold</code> <code>int</code> <p>Maximum allowed reprojection error to treat a point pair as an inlier. More info in links below.</p> <code>3</code> <code>max_iters</code> <code>int</code> <p>The maximum number of RANSAC iterations.  More info in links below.</p> <code>2000</code> <code>confidence</code> <code>float</code> <p>Confidence level, must be between 0 and 1. More info in links below.</p> <code>0.995</code> <code>proportion_points_used_threshold</code> <code>float</code> <p>Proportion of points that must be matched, otherwise the reference frame must be updated.</p> <code>0.9</code> See Also <p>opencv.findHomography</p> Source code in <code>norfair/camera_motion.py</code> <pre><code>class HomographyTransformationGetter(TransformationGetter):\n    \"\"\"\n    Calculates HomographyTransformation between points.\n\n    The camera movement is represented as an homography that matches the optical flow between the previous reference frame\n    and the current.\n\n    Comparing consecutive frames can make differences too small to correctly estimate the homography, often resulting in the identity.\n    For this reason the reference frame is kept fixed as we progress through the video.\n    Eventually, if the transformation is no longer able to match enough points, the reference frame is updated.\n\n    Parameters\n    ----------\n    method : Optional[int], optional\n        One of openCV's method for finding homographies.\n        Valid options are: `[0, cv.RANSAC, cv.LMEDS, cv.RHO]`, by default `cv.RANSAC`\n    ransac_reproj_threshold : int, optional\n        Maximum allowed reprojection error to treat a point pair as an inlier. More info in links below.\n    max_iters : int, optional\n        The maximum number of RANSAC iterations.  More info in links below.\n    confidence : float, optional\n        Confidence level, must be between 0 and 1. More info in links below.\n    proportion_points_used_threshold : float, optional\n        Proportion of points that must be matched, otherwise the reference frame must be updated.\n\n    See Also\n    --------\n    [opencv.findHomography](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780)\n    \"\"\"\n\n    def __init__(\n        self,\n        method: Optional[int] = None,\n        ransac_reproj_threshold: int = 3,\n        max_iters: int = 2000,\n        confidence: float = 0.995,\n        proportion_points_used_threshold: float = 0.9,\n    ) -&gt; None:\n        self.data = None\n        if method is None:\n            method = cv2.RANSAC\n        self.method = method\n        self.ransac_reproj_threshold = ransac_reproj_threshold\n        self.max_iters = max_iters\n        self.confidence = confidence\n        self.proportion_points_used_threshold = proportion_points_used_threshold\n\n    def __call__(\n        self, curr_pts: np.ndarray, prev_pts: np.ndarray\n    ) -&gt; Tuple[bool, Optional[HomographyTransformation]]:\n\n        if not (\n            isinstance(prev_pts, np.ndarray)\n            and prev_pts.shape[0] &gt;= 4\n            and isinstance(curr_pts, np.ndarray)\n            and curr_pts.shape[0] &gt;= 4\n        ):\n            warning(\n                \"The homography couldn't be computed in this frame \"\n                \"due to low amount of points\"\n            )\n            if isinstance(self.data, np.ndarray):\n                return True, HomographyTransformation(self.data)\n            else:\n                return True, None\n\n        homography_matrix, points_used = cv2.findHomography(\n            prev_pts,\n            curr_pts,\n            method=self.method,\n            ransacReprojThreshold=self.ransac_reproj_threshold,\n            maxIters=self.max_iters,\n            confidence=self.confidence,\n        )\n\n        proportion_points_used = np.sum(points_used) / len(points_used)\n\n        update_prvs = proportion_points_used &lt; self.proportion_points_used_threshold\n\n        try:\n            homography_matrix = homography_matrix @ self.data\n        except (TypeError, ValueError):\n            pass\n\n        if update_prvs:\n            self.data = homography_matrix\n\n        return update_prvs, HomographyTransformation(homography_matrix)\n</code></pre>"},{"location":"reference/camera_motion/#norfair.camera_motion.MotionEstimator","title":"<code>MotionEstimator</code>","text":"<p>Estimator of the motion of the camera.</p> <p>Uses optical flow to estimate the motion of the camera from frame to frame. The optical flow is calculated on a sample of strong points (corners).</p> <p>Parameters:</p> Name Type Description Default <code>max_points</code> <code>int</code> <p>Maximum amount of points sampled. More points make the estimation process slower but more precise</p> <code>200</code> <code>min_distance</code> <code>int</code> <p>Minimum distance between the sample points.</p> <code>15</code> <code>block_size</code> <code>int</code> <p>Size of an average block when finding the corners. More info in links below.</p> <code>3</code> <code>transformations_getter</code> <code>TransformationGetter</code> <p>An instance of TransformationGetter. By default <code>HomographyTransformationGetter</code></p> <code>None</code> <code>draw_flow</code> <code>bool</code> <p>Draws the optical flow on the frame for debugging.</p> <code>False</code> <code>flow_color</code> <code>Optional[Tuple[int, int, int]]</code> <p>Color of the drawing, by default blue.</p> <code>None</code> <code>quality_level</code> <code>float</code> <p>Parameter characterizing the minimal accepted quality of image corners.</p> <code>0.01</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from norfair import Tracker, Video\n&gt;&gt;&gt; from norfair.camera_motion MotionEstimator\n&gt;&gt;&gt; video = Video(\"video.mp4\")\n&gt;&gt;&gt; tracker = Tracker(...)\n&gt;&gt;&gt; motion_estimator = MotionEstimator()\n&gt;&gt;&gt; for frame in video:\n&gt;&gt;&gt;    detections = get_detections(frame)  # runs detector and returns Detections\n&gt;&gt;&gt;    coord_transformation = motion_estimator.update(frame)\n&gt;&gt;&gt;    tracked_objects = tracker.update(detections, coord_transformations=coord_transformation)\n</code></pre> See Also <p>For more infor on how the points are sampled: OpenCV.goodFeaturesToTrack</p> Source code in <code>norfair/camera_motion.py</code> <pre><code>class MotionEstimator:\n    \"\"\"\n    Estimator of the motion of the camera.\n\n    Uses optical flow to estimate the motion of the camera from frame to frame.\n    The optical flow is calculated on a sample of strong points (corners).\n\n    Parameters\n    ----------\n    max_points : int, optional\n        Maximum amount of points sampled.\n        More points make the estimation process slower but more precise\n    min_distance : int, optional\n        Minimum distance between the sample points.\n    block_size : int, optional\n        Size of an average block when finding the corners. More info in links below.\n    transformations_getter : TransformationGetter, optional\n        An instance of TransformationGetter. By default [`HomographyTransformationGetter`][norfair.camera_motion.HomographyTransformationGetter]\n    draw_flow : bool, optional\n        Draws the optical flow on the frame for debugging.\n    flow_color : Optional[Tuple[int, int, int]], optional\n        Color of the drawing, by default blue.\n    quality_level : float, optional\n        Parameter characterizing the minimal accepted quality of image corners.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from norfair import Tracker, Video\n    &gt;&gt;&gt; from norfair.camera_motion MotionEstimator\n    &gt;&gt;&gt; video = Video(\"video.mp4\")\n    &gt;&gt;&gt; tracker = Tracker(...)\n    &gt;&gt;&gt; motion_estimator = MotionEstimator()\n    &gt;&gt;&gt; for frame in video:\n    &gt;&gt;&gt;    detections = get_detections(frame)  # runs detector and returns Detections\n    &gt;&gt;&gt;    coord_transformation = motion_estimator.update(frame)\n    &gt;&gt;&gt;    tracked_objects = tracker.update(detections, coord_transformations=coord_transformation)\n\n    See Also\n    --------\n    For more infor on how the points are sampled: [OpenCV.goodFeaturesToTrack](https://docs.opencv.org/3.4/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541)\n    \"\"\"\n\n    def __init__(\n        self,\n        max_points: int = 200,\n        min_distance: int = 15,\n        block_size: int = 3,\n        transformations_getter: TransformationGetter = None,\n        draw_flow: bool = False,\n        flow_color: Optional[Tuple[int, int, int]] = None,\n        quality_level: float = 0.01,\n    ):\n\n        self.max_points = max_points\n        self.min_distance = min_distance\n        self.block_size = block_size\n\n        self.draw_flow = draw_flow\n        if self.draw_flow and flow_color is None:\n            flow_color = [0, 0, 100]\n        self.flow_color = flow_color\n\n        self.gray_prvs = None\n        self.prev_pts = None\n        if transformations_getter is None:\n            transformations_getter = HomographyTransformationGetter()\n\n        self.transformations_getter = transformations_getter\n        self.transformations_getter_copy = copy.deepcopy(transformations_getter)\n\n        self.prev_mask = None\n        self.gray_next = None\n        self.quality_level = quality_level\n\n    def update(\n        self, frame: np.ndarray, mask: np.ndarray = None\n    ) -&gt; Optional[CoordinatesTransformation]:\n        \"\"\"\n        Estimate camera motion for each frame\n\n        Parameters\n        ----------\n        frame : np.ndarray\n            The frame.\n        mask : np.ndarray, optional\n            An optional mask to avoid areas of the frame when sampling the corner.\n            Must be an array of shape `(frame.shape[0], frame.shape[1])`, dtype same as frame,\n            and values in {0, 1}.\n\n            In general, the estimation will work best when it samples many points from the background;\n            with that intention, this parameters is usefull for masking out the detections/tracked objects,\n            forcing the MotionEstimator ignore the moving objects.\n            Can be used to mask static areas of the image, such as score overlays in sport transmisions or\n            timestamps in security cameras.\n\n        Returns\n        -------\n        CoordinatesTransformation\n            The CoordinatesTransformation that can transform coordinates on this frame to absolute coordinates\n            or vice versa.\n        \"\"\"\n\n        self.gray_next = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        if self.gray_prvs is None:\n            self.gray_prvs = self.gray_next\n            self.prev_mask = mask\n\n        curr_pts, prev_pts = None, None\n        try:\n            curr_pts, prev_pts = _get_sparse_flow(\n                self.gray_next,\n                self.gray_prvs,\n                self.prev_pts,\n                self.max_points,\n                self.min_distance,\n                self.block_size,\n                self.prev_mask,\n                quality_level=self.quality_level,\n            )\n            if self.draw_flow:\n                for (curr, prev) in zip(curr_pts, prev_pts):\n                    c = tuple(curr.astype(int).ravel())\n                    p = tuple(prev.astype(int).ravel())\n                    cv2.line(frame, c, p, self.flow_color, 2)\n                    cv2.circle(frame, c, 3, self.flow_color, -1)\n        except Exception as e:\n            warning(e)\n\n        update_prvs, coord_transformations = True, None\n        try:\n            update_prvs, coord_transformations = self.transformations_getter(\n                curr_pts, prev_pts\n            )\n        except Exception as e:\n            warning(e)\n            del self.transformations_getter\n            self.transformations_getter = copy.deepcopy(\n                self.transformations_getter_copy\n            )\n\n        if update_prvs:\n            self.gray_prvs = self.gray_next\n            self.prev_pts = None\n            self.prev_mask = mask\n        else:\n            self.prev_pts = prev_pts\n\n        return coord_transformations\n</code></pre>"},{"location":"reference/camera_motion/#norfair.camera_motion.MotionEstimator.update","title":"<code>update(frame, mask=None)</code>","text":"<p>Estimate camera motion for each frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The frame.</p> required <code>mask</code> <code>ndarray</code> <p>An optional mask to avoid areas of the frame when sampling the corner. Must be an array of shape <code>(frame.shape[0], frame.shape[1])</code>, dtype same as frame, and values in {0, 1}.</p> <p>In general, the estimation will work best when it samples many points from the background; with that intention, this parameters is usefull for masking out the detections/tracked objects, forcing the MotionEstimator ignore the moving objects. Can be used to mask static areas of the image, such as score overlays in sport transmisions or timestamps in security cameras.</p> <code>None</code> <p>Returns:</p> Type Description <code>CoordinatesTransformation</code> <p>The CoordinatesTransformation that can transform coordinates on this frame to absolute coordinates or vice versa.</p> Source code in <code>norfair/camera_motion.py</code> <pre><code>def update(\n    self, frame: np.ndarray, mask: np.ndarray = None\n) -&gt; Optional[CoordinatesTransformation]:\n    \"\"\"\n    Estimate camera motion for each frame\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The frame.\n    mask : np.ndarray, optional\n        An optional mask to avoid areas of the frame when sampling the corner.\n        Must be an array of shape `(frame.shape[0], frame.shape[1])`, dtype same as frame,\n        and values in {0, 1}.\n\n        In general, the estimation will work best when it samples many points from the background;\n        with that intention, this parameters is usefull for masking out the detections/tracked objects,\n        forcing the MotionEstimator ignore the moving objects.\n        Can be used to mask static areas of the image, such as score overlays in sport transmisions or\n        timestamps in security cameras.\n\n    Returns\n    -------\n    CoordinatesTransformation\n        The CoordinatesTransformation that can transform coordinates on this frame to absolute coordinates\n        or vice versa.\n    \"\"\"\n\n    self.gray_next = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    if self.gray_prvs is None:\n        self.gray_prvs = self.gray_next\n        self.prev_mask = mask\n\n    curr_pts, prev_pts = None, None\n    try:\n        curr_pts, prev_pts = _get_sparse_flow(\n            self.gray_next,\n            self.gray_prvs,\n            self.prev_pts,\n            self.max_points,\n            self.min_distance,\n            self.block_size,\n            self.prev_mask,\n            quality_level=self.quality_level,\n        )\n        if self.draw_flow:\n            for (curr, prev) in zip(curr_pts, prev_pts):\n                c = tuple(curr.astype(int).ravel())\n                p = tuple(prev.astype(int).ravel())\n                cv2.line(frame, c, p, self.flow_color, 2)\n                cv2.circle(frame, c, 3, self.flow_color, -1)\n    except Exception as e:\n        warning(e)\n\n    update_prvs, coord_transformations = True, None\n    try:\n        update_prvs, coord_transformations = self.transformations_getter(\n            curr_pts, prev_pts\n        )\n    except Exception as e:\n        warning(e)\n        del self.transformations_getter\n        self.transformations_getter = copy.deepcopy(\n            self.transformations_getter_copy\n        )\n\n    if update_prvs:\n        self.gray_prvs = self.gray_next\n        self.prev_pts = None\n        self.prev_mask = mask\n    else:\n        self.prev_pts = prev_pts\n\n    return coord_transformations\n</code></pre>"},{"location":"reference/distances/","title":"Distances","text":"<p>Predefined distances</p>"},{"location":"reference/distances/#norfair.distances.Distance","title":"<code>Distance</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class representing a distance.</p> <p>Subclasses must implement the method <code>get_distances</code></p> Source code in <code>norfair/distances.py</code> <pre><code>class Distance(ABC):\n    \"\"\"\n    Abstract class representing a distance.\n\n    Subclasses must implement the method `get_distances`\n    \"\"\"\n\n    @abstractmethod\n    def get_distances(\n        self,\n        objects: Sequence[\"TrackedObject\"],\n        candidates: Optional[Union[List[\"Detection\"], List[\"TrackedObject\"]]],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Method that calculates the distances between new candidates and objects.\n\n        Parameters\n        ----------\n        objects : Sequence[TrackedObject]\n            Sequence of [TrackedObject][norfair.tracker.TrackedObject] to be compared with potential [Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]\n            candidates.\n        candidates : Union[List[Detection], List[TrackedObject]], optional\n            List of candidates ([Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]) to be compared to [TrackedObject][norfair.tracker.TrackedObject].\n\n        Returns\n        -------\n        np.ndarray\n            A matrix containing the distances between objects and candidates.\n        \"\"\"\n</code></pre>"},{"location":"reference/distances/#norfair.distances.Distance.get_distances","title":"<code>get_distances(objects, candidates)</code>  <code>abstractmethod</code>","text":"<p>Method that calculates the distances between new candidates and objects.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>Sequence[TrackedObject]</code> <p>Sequence of TrackedObject to be compared with potential Detection or TrackedObject candidates.</p> required <code>candidates</code> <code>Union[List[Detection], List[TrackedObject]]</code> <p>List of candidates (Detection or TrackedObject) to be compared to TrackedObject.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix containing the distances between objects and candidates.</p> Source code in <code>norfair/distances.py</code> <pre><code>@abstractmethod\ndef get_distances(\n    self,\n    objects: Sequence[\"TrackedObject\"],\n    candidates: Optional[Union[List[\"Detection\"], List[\"TrackedObject\"]]],\n) -&gt; np.ndarray:\n    \"\"\"\n    Method that calculates the distances between new candidates and objects.\n\n    Parameters\n    ----------\n    objects : Sequence[TrackedObject]\n        Sequence of [TrackedObject][norfair.tracker.TrackedObject] to be compared with potential [Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]\n        candidates.\n    candidates : Union[List[Detection], List[TrackedObject]], optional\n        List of candidates ([Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]) to be compared to [TrackedObject][norfair.tracker.TrackedObject].\n\n    Returns\n    -------\n    np.ndarray\n        A matrix containing the distances between objects and candidates.\n    \"\"\"\n</code></pre>"},{"location":"reference/distances/#norfair.distances.ScalarDistance","title":"<code>ScalarDistance</code>","text":"<p>             Bases: <code>Distance</code></p> <p>ScalarDistance class represents a distance that is calculated pointwise.</p> <p>Parameters:</p> Name Type Description Default <code>distance_function</code> <code>Union[Callable[[Detection, TrackedObject], float], Callable[[TrackedObject, TrackedObject], float]]</code> <p>Distance function used to determine the pointwise distance between new candidates and objects. This function should take 2 input arguments, the first being a <code>Union[Detection, TrackedObject]</code>, and the second TrackedObject. It has to return a <code>float</code> with the distance it calculates.</p> required Source code in <code>norfair/distances.py</code> <pre><code>class ScalarDistance(Distance):\n    \"\"\"\n    ScalarDistance class represents a distance that is calculated pointwise.\n\n    Parameters\n    ----------\n    distance_function : Union[Callable[[\"Detection\", \"TrackedObject\"], float], Callable[[\"TrackedObject\", \"TrackedObject\"], float]]\n        Distance function used to determine the pointwise distance between new candidates and objects.\n        This function should take 2 input arguments, the first being a `Union[Detection, TrackedObject]`,\n        and the second [TrackedObject][norfair.tracker.TrackedObject]. It has to return a `float` with the distance it calculates.\n    \"\"\"\n\n    def __init__(\n        self,\n        distance_function: Union[\n            Callable[[\"Detection\", \"TrackedObject\"], float],\n            Callable[[\"TrackedObject\", \"TrackedObject\"], float],\n        ],\n    ):\n        self.distance_function = distance_function\n\n    def get_distances(\n        self,\n        objects: Sequence[\"TrackedObject\"],\n        candidates: Optional[Union[List[\"Detection\"], List[\"TrackedObject\"]]],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Method that calculates the distances between new candidates and objects.\n\n        Parameters\n        ----------\n        objects : Sequence[TrackedObject]\n            Sequence of [TrackedObject][norfair.tracker.TrackedObject] to be compared with potential [Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]\n            candidates.\n        candidates : Union[List[Detection], List[TrackedObject]], optional\n            List of candidates ([Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]) to be compared to [TrackedObject][norfair.tracker.TrackedObject].\n\n        Returns\n        -------\n        np.ndarray\n            A matrix containing the distances between objects and candidates.\n        \"\"\"\n        distance_matrix = np.full(\n            (len(candidates), len(objects)),\n            fill_value=np.inf,\n            dtype=np.float32,\n        )\n        if not objects or not candidates:\n            return distance_matrix\n        for c, candidate in enumerate(candidates):\n            for o, obj in enumerate(objects):\n                if candidate.label != obj.label:\n                    if (candidate.label is None) or (obj.label is None):\n                        print(\"\\nThere are detections with and without label!\")\n                    continue\n                distance = self.distance_function(candidate, obj)\n                distance_matrix[c, o] = distance\n        return distance_matrix\n</code></pre>"},{"location":"reference/distances/#norfair.distances.ScalarDistance.get_distances","title":"<code>get_distances(objects, candidates)</code>","text":"<p>Method that calculates the distances between new candidates and objects.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>Sequence[TrackedObject]</code> <p>Sequence of TrackedObject to be compared with potential Detection or TrackedObject candidates.</p> required <code>candidates</code> <code>Union[List[Detection], List[TrackedObject]]</code> <p>List of candidates (Detection or TrackedObject) to be compared to TrackedObject.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix containing the distances between objects and candidates.</p> Source code in <code>norfair/distances.py</code> <pre><code>def get_distances(\n    self,\n    objects: Sequence[\"TrackedObject\"],\n    candidates: Optional[Union[List[\"Detection\"], List[\"TrackedObject\"]]],\n) -&gt; np.ndarray:\n    \"\"\"\n    Method that calculates the distances between new candidates and objects.\n\n    Parameters\n    ----------\n    objects : Sequence[TrackedObject]\n        Sequence of [TrackedObject][norfair.tracker.TrackedObject] to be compared with potential [Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]\n        candidates.\n    candidates : Union[List[Detection], List[TrackedObject]], optional\n        List of candidates ([Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]) to be compared to [TrackedObject][norfair.tracker.TrackedObject].\n\n    Returns\n    -------\n    np.ndarray\n        A matrix containing the distances between objects and candidates.\n    \"\"\"\n    distance_matrix = np.full(\n        (len(candidates), len(objects)),\n        fill_value=np.inf,\n        dtype=np.float32,\n    )\n    if not objects or not candidates:\n        return distance_matrix\n    for c, candidate in enumerate(candidates):\n        for o, obj in enumerate(objects):\n            if candidate.label != obj.label:\n                if (candidate.label is None) or (obj.label is None):\n                    print(\"\\nThere are detections with and without label!\")\n                continue\n            distance = self.distance_function(candidate, obj)\n            distance_matrix[c, o] = distance\n    return distance_matrix\n</code></pre>"},{"location":"reference/distances/#norfair.distances.VectorizedDistance","title":"<code>VectorizedDistance</code>","text":"<p>             Bases: <code>Distance</code></p> <p>VectorizedDistance class represents a distance that is calculated in a vectorized way. This means that instead of going through every pair and explicitly calculating its distance, VectorizedDistance uses the entire vectors to compare to each other in a single operation.</p> <p>Parameters:</p> Name Type Description Default <code>distance_function</code> <code>Callable[[ndarray, ndarray], ndarray]</code> <p>Distance function used to determine the distances between new candidates and objects. This function should take 2 input arguments, the first being a <code>np.ndarray</code> and the second <code>np.ndarray</code>. It has to return a <code>np.ndarray</code> with the distance matrix it calculates.</p> required Source code in <code>norfair/distances.py</code> <pre><code>class VectorizedDistance(Distance):\n    \"\"\"\n    VectorizedDistance class represents a distance that is calculated in a vectorized way. This means\n    that instead of going through every pair and explicitly calculating its distance, VectorizedDistance\n    uses the entire vectors to compare to each other in a single operation.\n\n    Parameters\n    ----------\n    distance_function : Callable[[np.ndarray, np.ndarray], np.ndarray]\n        Distance function used to determine the distances between new candidates and objects.\n        This function should take 2 input arguments, the first being a `np.ndarray` and the second\n        `np.ndarray`. It has to return a `np.ndarray` with the distance matrix it calculates.\n    \"\"\"\n\n    def __init__(\n        self,\n        distance_function: Callable[[np.ndarray, np.ndarray], np.ndarray],\n    ):\n        self.distance_function = distance_function\n\n    def get_distances(\n        self,\n        objects: Sequence[\"TrackedObject\"],\n        candidates: Optional[Union[List[\"Detection\"], List[\"TrackedObject\"]]],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Method that calculates the distances between new candidates and objects.\n\n        Parameters\n        ----------\n        objects : Sequence[TrackedObject]\n            Sequence of [TrackedObject][norfair.tracker.TrackedObject] to be compared with potential [Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]\n            candidates.\n        candidates : Union[List[Detection], List[TrackedObject]], optional\n            List of candidates ([Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]) to be compared to [TrackedObject][norfair.tracker.TrackedObject].\n\n        Returns\n        -------\n        np.ndarray\n            A matrix containing the distances between objects and candidates.\n        \"\"\"\n        distance_matrix = np.full(\n            (len(candidates), len(objects)),\n            fill_value=np.inf,\n            dtype=np.float32,\n        )\n        if not objects or not candidates:\n            return distance_matrix\n\n        object_labels = np.array([o.label for o in objects]).astype(str)\n        candidate_labels = np.array([c.label for c in candidates]).astype(str)\n\n        # iterate over labels that are present both in objects and detections\n        for label in np.intersect1d(\n            np.unique(object_labels), np.unique(candidate_labels)\n        ):\n            # generate masks of the subset of object and detections for this label\n            obj_mask = object_labels == label\n            cand_mask = candidate_labels == label\n\n            stacked_objects = []\n            for o in objects:\n                if str(o.label) == label:\n                    stacked_objects.append(o.estimate.ravel())\n            stacked_objects = np.stack(stacked_objects)\n\n            stacked_candidates = []\n            for c in candidates:\n                if str(c.label) == label:\n                    if \"Detection\" in str(type(c)):\n                        stacked_candidates.append(c.points.ravel())\n                    else:\n                        stacked_candidates.append(c.estimate.ravel())\n            stacked_candidates = np.stack(stacked_candidates)\n\n            # calculate the pairwise distances between objects and candidates with this label\n            # and assign the result to the correct positions inside distance_matrix\n            distance_matrix[np.ix_(cand_mask, obj_mask)] = self._compute_distance(\n                stacked_candidates, stacked_objects\n            )\n\n        return distance_matrix\n\n    def _compute_distance(\n        self, stacked_candidates: np.ndarray, stacked_objects: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Method that computes the pairwise distances between new candidates and objects.\n        It is intended to use the entire vectors to compare to each other in a single operation.\n\n        Parameters\n        ----------\n        stacked_candidates : np.ndarray\n            np.ndarray containing a stack of candidates to be compared with the stacked_objects.\n        stacked_objects : np.ndarray\n            np.ndarray containing a stack of objects to be compared with the stacked_objects.\n\n        Returns\n        -------\n        np.ndarray\n            A matrix containing the distances between objects and candidates.\n        \"\"\"\n        return self.distance_function(stacked_candidates, stacked_objects)\n</code></pre>"},{"location":"reference/distances/#norfair.distances.VectorizedDistance.get_distances","title":"<code>get_distances(objects, candidates)</code>","text":"<p>Method that calculates the distances between new candidates and objects.</p> <p>Parameters:</p> Name Type Description Default <code>objects</code> <code>Sequence[TrackedObject]</code> <p>Sequence of TrackedObject to be compared with potential Detection or TrackedObject candidates.</p> required <code>candidates</code> <code>Union[List[Detection], List[TrackedObject]]</code> <p>List of candidates (Detection or TrackedObject) to be compared to TrackedObject.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A matrix containing the distances between objects and candidates.</p> Source code in <code>norfair/distances.py</code> <pre><code>def get_distances(\n    self,\n    objects: Sequence[\"TrackedObject\"],\n    candidates: Optional[Union[List[\"Detection\"], List[\"TrackedObject\"]]],\n) -&gt; np.ndarray:\n    \"\"\"\n    Method that calculates the distances between new candidates and objects.\n\n    Parameters\n    ----------\n    objects : Sequence[TrackedObject]\n        Sequence of [TrackedObject][norfair.tracker.TrackedObject] to be compared with potential [Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]\n        candidates.\n    candidates : Union[List[Detection], List[TrackedObject]], optional\n        List of candidates ([Detection][norfair.tracker.Detection] or [TrackedObject][norfair.tracker.TrackedObject]) to be compared to [TrackedObject][norfair.tracker.TrackedObject].\n\n    Returns\n    -------\n    np.ndarray\n        A matrix containing the distances between objects and candidates.\n    \"\"\"\n    distance_matrix = np.full(\n        (len(candidates), len(objects)),\n        fill_value=np.inf,\n        dtype=np.float32,\n    )\n    if not objects or not candidates:\n        return distance_matrix\n\n    object_labels = np.array([o.label for o in objects]).astype(str)\n    candidate_labels = np.array([c.label for c in candidates]).astype(str)\n\n    # iterate over labels that are present both in objects and detections\n    for label in np.intersect1d(\n        np.unique(object_labels), np.unique(candidate_labels)\n    ):\n        # generate masks of the subset of object and detections for this label\n        obj_mask = object_labels == label\n        cand_mask = candidate_labels == label\n\n        stacked_objects = []\n        for o in objects:\n            if str(o.label) == label:\n                stacked_objects.append(o.estimate.ravel())\n        stacked_objects = np.stack(stacked_objects)\n\n        stacked_candidates = []\n        for c in candidates:\n            if str(c.label) == label:\n                if \"Detection\" in str(type(c)):\n                    stacked_candidates.append(c.points.ravel())\n                else:\n                    stacked_candidates.append(c.estimate.ravel())\n        stacked_candidates = np.stack(stacked_candidates)\n\n        # calculate the pairwise distances between objects and candidates with this label\n        # and assign the result to the correct positions inside distance_matrix\n        distance_matrix[np.ix_(cand_mask, obj_mask)] = self._compute_distance(\n            stacked_candidates, stacked_objects\n        )\n\n    return distance_matrix\n</code></pre>"},{"location":"reference/distances/#norfair.distances.ScipyDistance","title":"<code>ScipyDistance</code>","text":"<p>             Bases: <code>VectorizedDistance</code></p> <p>ScipyDistance class extends VectorizedDistance for the use of Scipy's vectorized distances.</p> <p>This class uses <code>scipy.spatial.distance.cdist</code> to calculate distances between two <code>np.ndarray</code>.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>str</code> <p>Defines the specific Scipy metric to use to calculate the pairwise distances between new candidates and objects.</p> <code>'euclidean'</code> <code>Other</code> required See Also <p><code>scipy.spatial.distance.cdist</code></p> Source code in <code>norfair/distances.py</code> <pre><code>class ScipyDistance(VectorizedDistance):\n    \"\"\"\n    ScipyDistance class extends VectorizedDistance for the use of Scipy's vectorized distances.\n\n    This class uses `scipy.spatial.distance.cdist` to calculate distances between two `np.ndarray`.\n\n    Parameters\n    ----------\n    metric : str, optional\n        Defines the specific Scipy metric to use to calculate the pairwise distances between\n        new candidates and objects.\n\n    Other kwargs are passed through to cdist\n\n    See Also\n    --------\n    [`scipy.spatial.distance.cdist`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html)\n    \"\"\"\n\n    def __init__(self, metric: str = \"euclidean\", **kwargs):\n        self.metric = metric\n        super().__init__(distance_function=partial(cdist, metric=self.metric, **kwargs))\n</code></pre>"},{"location":"reference/distances/#norfair.distances.frobenius","title":"<code>frobenius(detection, tracked_object)</code>","text":"<p>Frobernius norm on the difference of the points in detection and the estimates in tracked_object.</p> <p>The Frobenius distance and norm are given by:</p> \\[ d_f(a, b) = ||a - b||_F \\] \\[ ||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2} \\] <p>Parameters:</p> Name Type Description Default <code>detection</code> <code>Detection</code> <p>A detection.</p> required <code>tracked_object</code> <code>TrackedObject</code> <p>A tracked object.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The distance.</p> See Also <p><code>np.linalg.norm</code></p> Source code in <code>norfair/distances.py</code> <pre><code>def frobenius(detection: \"Detection\", tracked_object: \"TrackedObject\") -&gt; float:\n    \"\"\"\n    Frobernius norm on the difference of the points in detection and the estimates in tracked_object.\n\n    The Frobenius distance and norm are given by:\n\n    $$\n    d_f(a, b) = ||a - b||_F\n    $$\n\n    $$\n    ||A||_F = [\\\\sum_{i,j} abs(a_{i,j})^2]^{1/2}\n    $$\n\n    Parameters\n    ----------\n    detection : Detection\n        A detection.\n    tracked_object : TrackedObject\n        A tracked object.\n\n    Returns\n    -------\n    float\n        The distance.\n\n    See Also\n    --------\n    [`np.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)\n    \"\"\"\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n</code></pre>"},{"location":"reference/distances/#norfair.distances.mean_euclidean","title":"<code>mean_euclidean(detection, tracked_object)</code>","text":"<p>Average euclidean distance between the points in detection and estimates in tracked_object.</p> \\[ d(a, b) = \\frac{\\sum_{i=0}^N ||a_i - b_i||_2}{N} \\] <p>Parameters:</p> Name Type Description Default <code>detection</code> <code>Detection</code> <p>A detection.</p> required <code>tracked_object</code> <code>TrackedObject</code> <p>A tracked object</p> required <p>Returns:</p> Type Description <code>float</code> <p>The distance.</p> See Also <p><code>np.linalg.norm</code></p> Source code in <code>norfair/distances.py</code> <pre><code>def mean_euclidean(detection: \"Detection\", tracked_object: \"TrackedObject\") -&gt; float:\n    \"\"\"\n    Average euclidean distance between the points in detection and estimates in tracked_object.\n\n    $$\n    d(a, b) = \\\\frac{\\\\sum_{i=0}^N ||a_i - b_i||_2}{N}\n    $$\n\n    Parameters\n    ----------\n    detection : Detection\n        A detection.\n    tracked_object : TrackedObject\n        A tracked object\n\n    Returns\n    -------\n    float\n        The distance.\n\n    See Also\n    --------\n    [`np.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)\n    \"\"\"\n    return np.linalg.norm(detection.points - tracked_object.estimate, axis=1).mean()\n</code></pre>"},{"location":"reference/distances/#norfair.distances.mean_manhattan","title":"<code>mean_manhattan(detection, tracked_object)</code>","text":"<p>Average manhattan distance between the points in detection and the estimates in tracked_object</p> <p>Given by:</p> \\[ d(a, b) = \\frac{\\sum_{i=0}^N ||a_i - b_i||_1}{N} \\] <p>Where \\(||a||_1\\) is the manhattan norm.</p> <p>Parameters:</p> Name Type Description Default <code>detection</code> <code>Detection</code> <p>A detection.</p> required <code>tracked_object</code> <code>TrackedObject</code> <p>a tracked object.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The distance.</p> See Also <p><code>np.linalg.norm</code></p> Source code in <code>norfair/distances.py</code> <pre><code>def mean_manhattan(detection: \"Detection\", tracked_object: \"TrackedObject\") -&gt; float:\n    \"\"\"\n    Average manhattan distance between the points in detection and the estimates in tracked_object\n\n    Given by:\n\n    $$\n    d(a, b) = \\\\frac{\\\\sum_{i=0}^N ||a_i - b_i||_1}{N}\n    $$\n\n    Where $||a||_1$ is the manhattan norm.\n\n    Parameters\n    ----------\n    detection : Detection\n        A detection.\n    tracked_object : TrackedObject\n        a tracked object.\n\n    Returns\n    -------\n    float\n        The distance.\n\n    See Also\n    --------\n    [`np.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)\n    \"\"\"\n    return np.linalg.norm(\n        detection.points - tracked_object.estimate, ord=1, axis=1\n    ).mean()\n</code></pre>"},{"location":"reference/distances/#norfair.distances.iou","title":"<code>iou(candidates, objects)</code>","text":"<p>Calculate IoU between two sets of bounding boxes. Both sets of boxes are expected to be in <code>[x_min, y_min, x_max, y_max]</code> format.</p> <p>Normal IoU is 1 when the boxes are the same and 0 when they don't overlap, to transform that into a distance that makes sense we return <code>1 - iou</code>.</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>ndarray</code> <p>(N, 4) numpy.ndarray containing candidates bounding boxes.</p> required <code>objects</code> <code>ndarray</code> <p>(K, 4) numpy.ndarray containing objects bounding boxes.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>(N, K) numpy.ndarray of <code>1 - iou</code> between candidates and objects.</p> Source code in <code>norfair/distances.py</code> <pre><code>def iou(candidates: np.ndarray, objects: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Calculate IoU between two sets of bounding boxes. Both sets of boxes are expected\n    to be in `[x_min, y_min, x_max, y_max]` format.\n\n    Normal IoU is 1 when the boxes are the same and 0 when they don't overlap,\n    to transform that into a distance that makes sense we return `1 - iou`.\n\n    Parameters\n    ----------\n    candidates : numpy.ndarray\n        (N, 4) numpy.ndarray containing candidates bounding boxes.\n    objects : numpy.ndarray\n        (K, 4) numpy.ndarray containing objects bounding boxes.\n\n    Returns\n    -------\n    numpy.ndarray\n        (N, K) numpy.ndarray of `1 - iou` between candidates and objects.\n    \"\"\"\n    _validate_bboxes(candidates)\n\n    area_candidates = _boxes_area(candidates.T)\n    area_objects = _boxes_area(objects.T)\n\n    top_left = np.maximum(candidates[:, None, :2], objects[:, :2])\n    bottom_right = np.minimum(candidates[:, None, 2:], objects[:, 2:])\n\n    area_intersection = np.prod(\n        np.clip(bottom_right - top_left, a_min=0, a_max=None), 2\n    )\n    return 1 - area_intersection / (\n        area_candidates[:, None] + area_objects - area_intersection\n    )\n</code></pre>"},{"location":"reference/distances/#norfair.distances.get_distance_by_name","title":"<code>get_distance_by_name(name)</code>","text":"<p>Select a distance by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>A string defining the metric to get.</p> required <p>Returns:</p> Type Description <code>Distance</code> <p>The distance object.</p> Source code in <code>norfair/distances.py</code> <pre><code>def get_distance_by_name(name: str) -&gt; Distance:\n    \"\"\"\n    Select a distance by name.\n\n    Parameters\n    ----------\n    name : str\n        A string defining the metric to get.\n\n    Returns\n    -------\n    Distance\n        The distance object.\n    \"\"\"\n\n    if name in _SCALAR_DISTANCE_FUNCTIONS:\n        warning(\n            \"You are using a scalar distance function. If you want to speed up the\"\n            \" tracking process please consider using a vectorized distance function\"\n            f\" such as {AVAILABLE_VECTORIZED_DISTANCES}.\"\n        )\n        distance = _SCALAR_DISTANCE_FUNCTIONS[name]\n        distance_function = ScalarDistance(distance)\n    elif name in _SCIPY_DISTANCE_FUNCTIONS:\n        distance_function = ScipyDistance(name)\n    elif name in _VECTORIZED_DISTANCE_FUNCTIONS:\n        if name == \"iou_opt\":\n            warning(\"iou_opt is deprecated, use iou instead\")\n        distance = _VECTORIZED_DISTANCE_FUNCTIONS[name]\n        distance_function = VectorizedDistance(distance)\n    else:\n        raise ValueError(\n            f\"Invalid distance '{name}', expecting one of\"\n            f\" {list(_SCALAR_DISTANCE_FUNCTIONS.keys()) + AVAILABLE_VECTORIZED_DISTANCES}\"\n        )\n\n    return distance_function\n</code></pre>"},{"location":"reference/distances/#norfair.distances.create_keypoints_voting_distance","title":"<code>create_keypoints_voting_distance(keypoint_distance_threshold, detection_threshold)</code>","text":"<p>Construct a keypoint voting distance function configured with the thresholds.</p> <p>Count how many points in a detection match the with a tracked_object. A match is considered when distance between the points is &lt; <code>keypoint_distance_threshold</code> and the score of the last_detection of the tracked_object is &gt; <code>detection_threshold</code>. Notice the if multiple points are tracked, the ith point in detection can only match the ith point in the tracked object.</p> <p>Distance is 1 if no point matches and approximates 0 as more points are matched.</p> <p>Parameters:</p> Name Type Description Default <code>keypoint_distance_threshold</code> <code>float</code> <p>Points closer than this threshold are considered a match.</p> required <code>detection_threshold</code> <code>float</code> <p>Detections and objects with score lower than this threshold are ignored.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The distance funtion that must be passed to the Tracker.</p> Source code in <code>norfair/distances.py</code> <pre><code>def create_keypoints_voting_distance(\n    keypoint_distance_threshold: float, detection_threshold: float\n) -&gt; Callable[[\"Detection\", \"TrackedObject\"], float]:\n    \"\"\"\n    Construct a keypoint voting distance function configured with the thresholds.\n\n    Count how many points in a detection match the with a tracked_object.\n    A match is considered when distance between the points is &lt; `keypoint_distance_threshold`\n    and the score of the last_detection of the tracked_object is &gt; `detection_threshold`.\n    Notice the if multiple points are tracked, the ith point in detection can only match the ith\n    point in the tracked object.\n\n    Distance is 1 if no point matches and approximates 0 as more points are matched.\n\n    Parameters\n    ----------\n    keypoint_distance_threshold: float\n        Points closer than this threshold are considered a match.\n    detection_threshold: float\n        Detections and objects with score lower than this threshold are ignored.\n\n    Returns\n    -------\n    Callable\n        The distance funtion that must be passed to the Tracker.\n    \"\"\"\n\n    def keypoints_voting_distance(\n        detection: \"Detection\", tracked_object: \"TrackedObject\"\n    ) -&gt; float:\n        distances = np.linalg.norm(detection.points - tracked_object.estimate, axis=1)\n        match_num = np.count_nonzero(\n            (distances &lt; keypoint_distance_threshold)\n            * (detection.scores &gt; detection_threshold)\n            * (tracked_object.last_detection.scores &gt; detection_threshold)\n        )\n        return 1 / (1 + match_num)\n\n    return keypoints_voting_distance\n</code></pre>"},{"location":"reference/distances/#norfair.distances.create_normalized_mean_euclidean_distance","title":"<code>create_normalized_mean_euclidean_distance(height, width)</code>","text":"<p>Construct a normalized mean euclidean distance function configured with the max height and width.</p> <p>The result distance is bound to [0, 1] where 1 indicates oposite corners of the image.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the image.</p> required <code>width</code> <code>int</code> <p>Width of the image.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The distance funtion that must be passed to the Tracker.</p> Source code in <code>norfair/distances.py</code> <pre><code>def create_normalized_mean_euclidean_distance(\n    height: int, width: int\n) -&gt; Callable[[\"Detection\", \"TrackedObject\"], float]:\n    \"\"\"\n    Construct a normalized mean euclidean distance function configured with the max height and width.\n\n    The result distance is bound to [0, 1] where 1 indicates oposite corners of the image.\n\n    Parameters\n    ----------\n    height: int\n        Height of the image.\n    width: int\n        Width of the image.\n\n    Returns\n    -------\n    Callable\n        The distance funtion that must be passed to the Tracker.\n    \"\"\"\n\n    def normalized__mean_euclidean_distance(\n        detection: \"Detection\", tracked_object: \"TrackedObject\"\n    ) -&gt; float:\n        \"\"\"Normalized mean euclidean distance\"\"\"\n        # calculate distances and normalized it by width and height\n        difference = (detection.points - tracked_object.estimate).astype(float)\n        difference[:, 0] /= width\n        difference[:, 1] /= height\n\n        # calculate eucledean distance and average\n        return np.linalg.norm(difference, axis=1).mean()\n\n    return normalized__mean_euclidean_distance\n</code></pre>"},{"location":"reference/drawing/","title":"Drawing","text":"<p>Collection of drawing functions</p>"},{"location":"reference/drawing/#norfair.drawing.draw_points","title":"<code>draw_points</code>","text":""},{"location":"reference/drawing/#norfair.drawing.draw_points.draw_points","title":"<code>draw_points(frame, drawables=None, radius=None, thickness=None, color='by_id', color_by_label=None, draw_labels=True, text_size=None, draw_ids=True, draw_points=True, text_thickness=None, text_color=None, hide_dead_points=True, detections=None, label_size=None, draw_scores=False)</code>","text":"<p>Draw the points included in a list of Detections or TrackedObjects.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The OpenCV frame to draw on. Modified in place.</p> required <code>drawables</code> <code>Union[Sequence[Detection], Sequence[TrackedObject]]</code> <p>List of objects to draw, Detections and TrackedObjects are accepted.</p> <code>None</code> <code>radius</code> <code>Optional[int]</code> <p>Radius of the circles representing each point. By default a sensible value is picked considering the frame size.</p> <code>None</code> <code>thickness</code> <code>Optional[int]</code> <p>Thickness or width of the line.</p> <code>None</code> <code>color</code> <code>ColorLike</code> <p>This parameter can take:</p> <ol> <li>A color as a tuple of ints describing the BGR <code>(0, 0, 255)</code></li> <li>A 6-digit hex string <code>\"#FF0000\"</code></li> <li>One of the defined color names <code>\"red\"</code></li> <li> <p>A string defining the strategy to choose colors from the Palette:</p> <ol> <li>based on the id of the objects <code>\"by_id\"</code></li> <li>based on the label of the objects <code>\"by_label\"</code></li> <li>random choice <code>\"random\"</code></li> </ol> </li> </ol> <p>If using <code>by_id</code> or <code>by_label</code> strategy but your objects don't have that field defined (Detections never have ids) the selected color will be the same for all objects (Palette's default Color).</p> <code>'by_id'</code> <code>color_by_label</code> <code>bool</code> <p>Deprecated. set <code>color=\"by_label\"</code>.</p> <code>None</code> <code>draw_labels</code> <code>bool</code> <p>If set to True, the label is added to a title that is drawn on top of the box. If an object doesn't have a label this parameter is ignored.</p> <code>True</code> <code>draw_scores</code> <code>bool</code> <p>If set to True, the score is added to a title that is drawn on top of the box. If an object doesn't have a label this parameter is ignored.</p> <code>False</code> <code>text_size</code> <code>Optional[int]</code> <p>Size of the title, the value is used as a multiplier of the base size of the font. By default the size is scaled automatically based on the frame size.</p> <code>None</code> <code>draw_ids</code> <code>bool</code> <p>If set to True, the id is added to a title that is drawn on top of the box. If an object doesn't have an id this parameter is ignored.</p> <code>True</code> <code>draw_points</code> <code>bool</code> <p>Set to False to hide the points and just draw the text.</p> <code>True</code> <code>text_thickness</code> <code>Optional[int]</code> <p>Thickness of the font. By default it's scaled with the <code>text_size</code>.</p> <code>None</code> <code>text_color</code> <code>Optional[ColorLike]</code> <p>Color of the text. By default the same color as the box is used.</p> <code>None</code> <code>hide_dead_points</code> <code>bool</code> <p>Set this param to False to always draw all points, even the ones considered \"dead\". A point is \"dead\" when the corresponding value of <code>TrackedObject.live_points</code> is set to False. If all objects are dead the object is not drawn. All points of a detection are considered to be alive.</p> <code>True</code> <code>detections</code> <code>Sequence[Detection]</code> <p>Deprecated. use drawables.</p> <code>None</code> <code>label_size</code> <code>Optional[int]</code> <p>Deprecated. text_size.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The resulting frame.</p> Source code in <code>norfair/drawing/draw_points.py</code> <pre><code>def draw_points(\n    frame: np.ndarray,\n    drawables: Union[Sequence[Detection], Sequence[TrackedObject]] = None,\n    radius: Optional[int] = None,\n    thickness: Optional[int] = None,\n    color: ColorLike = \"by_id\",\n    color_by_label: bool = None,  # deprecated\n    draw_labels: bool = True,\n    text_size: Optional[int] = None,\n    draw_ids: bool = True,\n    draw_points: bool = True,  # pylint: disable=redefined-outer-name\n    text_thickness: Optional[int] = None,\n    text_color: Optional[ColorLike] = None,\n    hide_dead_points: bool = True,\n    detections: Sequence[\"Detection\"] = None,  # deprecated\n    label_size: Optional[int] = None,  # deprecated\n    draw_scores: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Draw the points included in a list of Detections or TrackedObjects.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The OpenCV frame to draw on. Modified in place.\n    drawables : Union[Sequence[Detection], Sequence[TrackedObject]], optional\n        List of objects to draw, Detections and TrackedObjects are accepted.\n    radius : Optional[int], optional\n        Radius of the circles representing each point.\n        By default a sensible value is picked considering the frame size.\n    thickness : Optional[int], optional\n        Thickness or width of the line.\n    color : ColorLike, optional\n        This parameter can take:\n\n        1. A color as a tuple of ints describing the BGR `(0, 0, 255)`\n        2. A 6-digit hex string `\"#FF0000\"`\n        3. One of the defined color names `\"red\"`\n        4. A string defining the strategy to choose colors from the Palette:\n\n            1. based on the id of the objects `\"by_id\"`\n            2. based on the label of the objects `\"by_label\"`\n            3. random choice `\"random\"`\n\n        If using `by_id` or `by_label` strategy but your objects don't\n        have that field defined (Detections never have ids) the\n        selected color will be the same for all objects (Palette's default Color).\n    color_by_label : bool, optional\n        **Deprecated**. set `color=\"by_label\"`.\n    draw_labels : bool, optional\n        If set to True, the label is added to a title that is drawn on top of the box.\n        If an object doesn't have a label this parameter is ignored.\n    draw_scores : bool, optional\n        If set to True, the score is added to a title that is drawn on top of the box.\n        If an object doesn't have a label this parameter is ignored.\n    text_size : Optional[int], optional\n        Size of the title, the value is used as a multiplier of the base size of the font.\n        By default the size is scaled automatically based on the frame size.\n    draw_ids : bool, optional\n        If set to True, the id is added to a title that is drawn on top of the box.\n        If an object doesn't have an id this parameter is ignored.\n    draw_points : bool, optional\n        Set to False to hide the points and just draw the text.\n    text_thickness : Optional[int], optional\n        Thickness of the font. By default it's scaled with the `text_size`.\n    text_color : Optional[ColorLike], optional\n        Color of the text. By default the same color as the box is used.\n    hide_dead_points : bool, optional\n        Set this param to False to always draw all points, even the ones considered \"dead\".\n        A point is \"dead\" when the corresponding value of `TrackedObject.live_points`\n        is set to False. If all objects are dead the object is not drawn.\n        All points of a detection are considered to be alive.\n    detections : Sequence[Detection], optional\n        **Deprecated**. use drawables.\n    label_size : Optional[int], optional\n        **Deprecated**. text_size.\n\n    Returns\n    -------\n    np.ndarray\n        The resulting frame.\n    \"\"\"\n    #\n    # handle deprecated parameters\n    #\n    if color_by_label is not None:\n        warn_once(\n            'Parameter \"color_by_label\" on function draw_points is deprecated, set `color=\"by_label\"` instead'\n        )\n        color = \"by_label\"\n    if detections is not None:\n        warn_once(\n            \"Parameter 'detections' on function draw_points is deprecated, use 'drawables' instead\"\n        )\n        drawables = detections\n    if label_size is not None:\n        warn_once(\n            \"Parameter 'label_size' on function draw_points is deprecated, use 'text_size' instead\"\n        )\n        text_size = label_size\n    # end\n\n    if drawables is None:\n        return\n\n    if text_color is not None:\n        text_color = parse_color(text_color)\n\n    if color is None:\n        color = \"by_id\"\n    if thickness is None:\n        thickness = -1\n    if radius is None:\n        radius = int(round(max(max(frame.shape) * 0.002, 1)))\n\n    for o in drawables:\n        if not isinstance(o, Drawable):\n            d = Drawable(o)\n        else:\n            d = o\n\n        if hide_dead_points and not d.live_points.any():\n            continue\n\n        if color == \"by_id\":\n            obj_color = Palette.choose_color(d.id)\n        elif color == \"by_label\":\n            obj_color = Palette.choose_color(d.label)\n        elif color == \"random\":\n            obj_color = Palette.choose_color(np.random.rand())\n        else:\n            obj_color = parse_color(color)\n\n        if text_color is None:\n            obj_text_color = obj_color\n        else:\n            obj_text_color = text_color\n\n        if draw_points:\n            for point, live in zip(d.points, d.live_points):\n                if live or not hide_dead_points:\n                    Drawer.circle(\n                        frame,\n                        tuple(point.astype(int)),\n                        radius=radius,\n                        color=obj_color,\n                        thickness=thickness,\n                    )\n\n        if draw_labels or draw_ids or draw_scores:\n            position = d.points[d.live_points].mean(axis=0)\n            position -= radius\n            text = _build_text(\n                d, draw_labels=draw_labels, draw_ids=draw_ids, draw_scores=draw_scores\n            )\n\n            Drawer.text(\n                frame,\n                text,\n                tuple(position.astype(int)),\n                size=text_size,\n                color=obj_text_color,\n                thickness=text_thickness,\n            )\n\n    return frame\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.draw_points.draw_tracked_objects","title":"<code>draw_tracked_objects(frame, objects, radius=None, color=None, id_size=None, id_thickness=None, draw_points=True, color_by_label=False, draw_labels=False, label_size=None)</code>","text":"<p>Deprecated use <code>draw_points</code></p> Source code in <code>norfair/drawing/draw_points.py</code> <pre><code>def draw_tracked_objects(\n    frame: np.ndarray,\n    objects: Sequence[\"TrackedObject\"],\n    radius: Optional[int] = None,\n    color: Optional[ColorLike] = None,\n    id_size: Optional[float] = None,\n    id_thickness: Optional[int] = None,\n    draw_points: bool = True,  # pylint: disable=redefined-outer-name\n    color_by_label: bool = False,\n    draw_labels: bool = False,\n    label_size: Optional[int] = None,\n):\n    \"\"\"\n    **Deprecated** use [`draw_points`][norfair.drawing.draw_points.draw_points]\n    \"\"\"\n    warn_once(\"draw_tracked_objects is deprecated, use draw_points instead\")\n\n    frame_scale = frame.shape[0] / 100\n    if radius is None:\n        radius = int(frame_scale * 0.5)\n    if id_size is None:\n        id_size = frame_scale / 10\n    if id_thickness is None:\n        id_thickness = int(frame_scale / 5)\n    if label_size is None:\n        label_size = int(max(frame_scale / 100, 1))\n\n    _draw_points_alias(\n        frame=frame,\n        drawables=objects,\n        color=\"by_label\" if color_by_label else color,\n        radius=radius,\n        thickness=None,\n        draw_labels=draw_labels,\n        draw_ids=id_size is not None and id_size &gt; 0,\n        draw_points=draw_points,\n        text_size=label_size or id_size,\n        text_thickness=id_thickness,\n        text_color=None,\n        hide_dead_points=True,\n    )\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.draw_boxes","title":"<code>draw_boxes</code>","text":""},{"location":"reference/drawing/#norfair.drawing.draw_boxes.draw_boxes","title":"<code>draw_boxes(frame, drawables=None, color='by_id', thickness=None, random_color=None, color_by_label=None, draw_labels=False, text_size=None, draw_ids=True, text_color=None, text_thickness=None, draw_box=True, detections=None, line_color=None, line_width=None, label_size=None, draw_scores=False)</code>","text":"<p>Draw bounding boxes corresponding to Detections or TrackedObjects.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The OpenCV frame to draw on. Modified in place.</p> required <code>drawables</code> <code>Union[Sequence[Detection], Sequence[TrackedObject]]</code> <p>List of objects to draw, Detections and TrackedObjects are accepted. This objects are assumed to contain 2 bi-dimensional points defining the bounding box as <code>[[x0, y0], [x1, y1]]</code>.</p> <code>None</code> <code>color</code> <code>ColorLike</code> <p>This parameter can take:</p> <ol> <li>A color as a tuple of ints describing the BGR <code>(0, 0, 255)</code></li> <li>A 6-digit hex string <code>\"#FF0000\"</code></li> <li>One of the defined color names <code>\"red\"</code></li> <li> <p>A string defining the strategy to choose colors from the Palette:</p> <ol> <li>based on the id of the objects <code>\"by_id\"</code></li> <li>based on the label of the objects <code>\"by_label\"</code></li> <li>random choice <code>\"random\"</code></li> </ol> </li> </ol> <p>If using <code>by_id</code> or <code>by_label</code> strategy but your objects don't have that field defined (Detections never have ids) the selected color will be the same for all objects (Palette's default Color).</p> <code>'by_id'</code> <code>thickness</code> <code>Optional[int]</code> <p>Thickness or width of the line.</p> <code>None</code> <code>random_color</code> <code>bool</code> <p>Deprecated. Set color=\"random\".</p> <code>None</code> <code>color_by_label</code> <code>bool</code> <p>Deprecated. Set color=\"by_label\".</p> <code>None</code> <code>draw_labels</code> <code>bool</code> <p>If set to True, the label is added to a title that is drawn on top of the box. If an object doesn't have a label this parameter is ignored.</p> <code>False</code> <code>draw_scores</code> <code>bool</code> <p>If set to True, the score is added to a title that is drawn on top of the box. If an object doesn't have a label this parameter is ignored.</p> <code>False</code> <code>text_size</code> <code>Optional[float]</code> <p>Size of the title, the value is used as a multiplier of the base size of the font. By default the size is scaled automatically based on the frame size.</p> <code>None</code> <code>draw_ids</code> <code>bool</code> <p>If set to True, the id is added to a title that is drawn on top of the box. If an object doesn't have an id this parameter is ignored.</p> <code>True</code> <code>text_color</code> <code>Optional[ColorLike]</code> <p>Color of the text. By default the same color as the box is used.</p> <code>None</code> <code>text_thickness</code> <code>Optional[int]</code> <p>Thickness of the font. By default it's scaled with the <code>text_size</code>.</p> <code>None</code> <code>draw_box</code> <code>bool</code> <p>Set to False to hide the box and just draw the text.</p> <code>True</code> <code>detections</code> <code>Sequence[Detection]</code> <p>Deprecated. Use drawables.</p> <code>None</code> <code>line_color</code> <code>Optional[ColorLike]</code> <p>Deprecated. Use color.</p> <code>None</code> <code>line_width</code> <code>Optional[int]</code> <p>Deprecated. Use thickness.</p> <code>None</code> <code>label_size</code> <code>Optional[int]</code> <p>Deprecated. Use text_size.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The resulting frame.</p> Source code in <code>norfair/drawing/draw_boxes.py</code> <pre><code>def draw_boxes(\n    frame: np.ndarray,\n    drawables: Union[Sequence[Detection], Sequence[TrackedObject]] = None,\n    color: ColorLike = \"by_id\",\n    thickness: Optional[int] = None,\n    random_color: bool = None,  # Deprecated\n    color_by_label: bool = None,  # Deprecated\n    draw_labels: bool = False,\n    text_size: Optional[float] = None,\n    draw_ids: bool = True,\n    text_color: Optional[ColorLike] = None,\n    text_thickness: Optional[int] = None,\n    draw_box: bool = True,\n    detections: Sequence[\"Detection\"] = None,  # Deprecated\n    line_color: Optional[ColorLike] = None,  # Deprecated\n    line_width: Optional[int] = None,  # Deprecated\n    label_size: Optional[int] = None,  # Deprecated\u00b4\n    draw_scores: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Draw bounding boxes corresponding to Detections or TrackedObjects.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The OpenCV frame to draw on. Modified in place.\n    drawables : Union[Sequence[Detection], Sequence[TrackedObject]], optional\n        List of objects to draw, Detections and TrackedObjects are accepted.\n        This objects are assumed to contain 2 bi-dimensional points defining\n        the bounding box as `[[x0, y0], [x1, y1]]`.\n    color : ColorLike, optional\n        This parameter can take:\n\n        1. A color as a tuple of ints describing the BGR `(0, 0, 255)`\n        2. A 6-digit hex string `\"#FF0000\"`\n        3. One of the defined color names `\"red\"`\n        4. A string defining the strategy to choose colors from the Palette:\n\n            1. based on the id of the objects `\"by_id\"`\n            2. based on the label of the objects `\"by_label\"`\n            3. random choice `\"random\"`\n\n        If using `by_id` or `by_label` strategy but your objects don't\n        have that field defined (Detections never have ids) the\n        selected color will be the same for all objects (Palette's default Color).\n    thickness : Optional[int], optional\n        Thickness or width of the line.\n    random_color : bool, optional\n        **Deprecated**. Set color=\"random\".\n    color_by_label : bool, optional\n        **Deprecated**. Set color=\"by_label\".\n    draw_labels : bool, optional\n        If set to True, the label is added to a title that is drawn on top of the box.\n        If an object doesn't have a label this parameter is ignored.\n    draw_scores : bool, optional\n        If set to True, the score is added to a title that is drawn on top of the box.\n        If an object doesn't have a label this parameter is ignored.\n    text_size : Optional[float], optional\n        Size of the title, the value is used as a multiplier of the base size of the font.\n        By default the size is scaled automatically based on the frame size.\n    draw_ids : bool, optional\n        If set to True, the id is added to a title that is drawn on top of the box.\n        If an object doesn't have an id this parameter is ignored.\n    text_color : Optional[ColorLike], optional\n        Color of the text. By default the same color as the box is used.\n    text_thickness : Optional[int], optional\n        Thickness of the font. By default it's scaled with the `text_size`.\n    draw_box : bool, optional\n        Set to False to hide the box and just draw the text.\n    detections : Sequence[Detection], optional\n        **Deprecated**. Use drawables.\n    line_color: Optional[ColorLike], optional\n        **Deprecated**. Use color.\n    line_width: Optional[int], optional\n        **Deprecated**. Use thickness.\n    label_size: Optional[int], optional\n        **Deprecated**. Use text_size.\n\n    Returns\n    -------\n    np.ndarray\n        The resulting frame.\n    \"\"\"\n    #\n    # handle deprecated parameters\n    #\n    if random_color is not None:\n        warn_once(\n            'Parameter \"random_color\" is deprecated, set `color=\"random\"` instead'\n        )\n        color = \"random\"\n    if color_by_label is not None:\n        warn_once(\n            'Parameter \"color_by_label\" is deprecated, set `color=\"by_label\"` instead'\n        )\n        color = \"by_label\"\n    if detections is not None:\n        warn_once('Parameter \"detections\" is deprecated, use \"drawables\" instead')\n        drawables = detections\n    if line_color is not None:\n        warn_once('Parameter \"line_color\" is deprecated, use \"color\" instead')\n        color = line_color\n    if line_width is not None:\n        warn_once('Parameter \"line_width\" is deprecated, use \"thickness\" instead')\n        thickness = line_width\n    if label_size is not None:\n        warn_once('Parameter \"label_size\" is deprecated, use \"text_size\" instead')\n        text_size = label_size\n    # end\n\n    if color is None:\n        color = \"by_id\"\n    if thickness is None:\n        thickness = int(max(frame.shape) / 500)\n\n    if drawables is None:\n        return frame\n\n    if text_color is not None:\n        text_color = parse_color(text_color)\n\n    for obj in drawables:\n        if not isinstance(obj, Drawable):\n            d = Drawable(obj)\n        else:\n            d = obj\n\n        if color == \"by_id\":\n            obj_color = Palette.choose_color(d.id)\n        elif color == \"by_label\":\n            obj_color = Palette.choose_color(d.label)\n        elif color == \"random\":\n            obj_color = Palette.choose_color(np.random.rand())\n        else:\n            obj_color = parse_color(color)\n\n        points = d.points.astype(int)\n        if draw_box:\n            Drawer.rectangle(\n                frame,\n                tuple(points),\n                color=obj_color,\n                thickness=thickness,\n            )\n\n        text = _build_text(\n            d, draw_labels=draw_labels, draw_ids=draw_ids, draw_scores=draw_scores\n        )\n        if text:\n            if text_color is None:\n                obj_text_color = obj_color\n            else:\n                obj_text_color = text_color\n            # the anchor will become the bottom-left of the text,\n            # we select-top left of the bbox compensating for the thickness of the box\n            text_anchor = (\n                points[0, 0] - thickness // 2,\n                points[0, 1] - thickness // 2 - 1,\n            )\n            frame = Drawer.text(\n                frame,\n                text,\n                position=text_anchor,\n                size=text_size,\n                color=obj_text_color,\n                thickness=text_thickness,\n            )\n\n    return frame\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.draw_boxes.draw_tracked_boxes","title":"<code>draw_tracked_boxes(frame, objects, border_colors=None, border_width=None, id_size=None, id_thickness=None, draw_box=True, color_by_label=False, draw_labels=False, label_size=None, label_width=None)</code>","text":"<p>Deprecated. Use <code>draw_box</code></p> Source code in <code>norfair/drawing/draw_boxes.py</code> <pre><code>def draw_tracked_boxes(\n    frame: np.ndarray,\n    objects: Sequence[\"TrackedObject\"],\n    border_colors: Optional[Tuple[int, int, int]] = None,\n    border_width: Optional[int] = None,\n    id_size: Optional[int] = None,\n    id_thickness: Optional[int] = None,\n    draw_box: bool = True,\n    color_by_label: bool = False,\n    draw_labels: bool = False,\n    label_size: Optional[int] = None,\n    label_width: Optional[int] = None,\n) -&gt; np.array:\n    \"**Deprecated**. Use [`draw_box`][norfair.drawing.draw_boxes.draw_boxes]\"\n    warn_once(\"draw_tracked_boxes is deprecated, use draw_box instead\")\n    return draw_boxes(\n        frame=frame,\n        drawables=objects,\n        color=\"by_label\" if color_by_label else border_colors,\n        thickness=border_width,\n        text_size=label_size or id_size,\n        text_thickness=id_thickness or label_width,\n        draw_labels=draw_labels,\n        draw_ids=id_size is not None and id_size &gt; 0,\n        draw_box=draw_box,\n    )\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.color","title":"<code>color</code>","text":""},{"location":"reference/drawing/#norfair.drawing.color.Color","title":"<code>Color</code>","text":"<p>Contains predefined colors.</p> <p>Colors are defined as a Tuple of integers between 0 and 255 expressing the values in BGR This is the format opencv uses.</p> Source code in <code>norfair/drawing/color.py</code> <pre><code>class Color:\n    \"\"\"\n    Contains predefined colors.\n\n    Colors are defined as a Tuple of integers between 0 and 255 expressing the values in BGR\n    This is the format opencv uses.\n    \"\"\"\n\n    # from PIL.ImageColors.colormap\n    aliceblue = hex_to_bgr(\"#f0f8ff\")\n    antiquewhite = hex_to_bgr(\"#faebd7\")\n    aqua = hex_to_bgr(\"#00ffff\")\n    aquamarine = hex_to_bgr(\"#7fffd4\")\n    azure = hex_to_bgr(\"#f0ffff\")\n    beige = hex_to_bgr(\"#f5f5dc\")\n    bisque = hex_to_bgr(\"#ffe4c4\")\n    black = hex_to_bgr(\"#000000\")\n    blanchedalmond = hex_to_bgr(\"#ffebcd\")\n    blue = hex_to_bgr(\"#0000ff\")\n    blueviolet = hex_to_bgr(\"#8a2be2\")\n    brown = hex_to_bgr(\"#a52a2a\")\n    burlywood = hex_to_bgr(\"#deb887\")\n    cadetblue = hex_to_bgr(\"#5f9ea0\")\n    chartreuse = hex_to_bgr(\"#7fff00\")\n    chocolate = hex_to_bgr(\"#d2691e\")\n    coral = hex_to_bgr(\"#ff7f50\")\n    cornflowerblue = hex_to_bgr(\"#6495ed\")\n    cornsilk = hex_to_bgr(\"#fff8dc\")\n    crimson = hex_to_bgr(\"#dc143c\")\n    cyan = hex_to_bgr(\"#00ffff\")\n    darkblue = hex_to_bgr(\"#00008b\")\n    darkcyan = hex_to_bgr(\"#008b8b\")\n    darkgoldenrod = hex_to_bgr(\"#b8860b\")\n    darkgray = hex_to_bgr(\"#a9a9a9\")\n    darkgrey = hex_to_bgr(\"#a9a9a9\")\n    darkgreen = hex_to_bgr(\"#006400\")\n    darkkhaki = hex_to_bgr(\"#bdb76b\")\n    darkmagenta = hex_to_bgr(\"#8b008b\")\n    darkolivegreen = hex_to_bgr(\"#556b2f\")\n    darkorange = hex_to_bgr(\"#ff8c00\")\n    darkorchid = hex_to_bgr(\"#9932cc\")\n    darkred = hex_to_bgr(\"#8b0000\")\n    darksalmon = hex_to_bgr(\"#e9967a\")\n    darkseagreen = hex_to_bgr(\"#8fbc8f\")\n    darkslateblue = hex_to_bgr(\"#483d8b\")\n    darkslategray = hex_to_bgr(\"#2f4f4f\")\n    darkslategrey = hex_to_bgr(\"#2f4f4f\")\n    darkturquoise = hex_to_bgr(\"#00ced1\")\n    darkviolet = hex_to_bgr(\"#9400d3\")\n    deeppink = hex_to_bgr(\"#ff1493\")\n    deepskyblue = hex_to_bgr(\"#00bfff\")\n    dimgray = hex_to_bgr(\"#696969\")\n    dimgrey = hex_to_bgr(\"#696969\")\n    dodgerblue = hex_to_bgr(\"#1e90ff\")\n    firebrick = hex_to_bgr(\"#b22222\")\n    floralwhite = hex_to_bgr(\"#fffaf0\")\n    forestgreen = hex_to_bgr(\"#228b22\")\n    fuchsia = hex_to_bgr(\"#ff00ff\")\n    gainsboro = hex_to_bgr(\"#dcdcdc\")\n    ghostwhite = hex_to_bgr(\"#f8f8ff\")\n    gold = hex_to_bgr(\"#ffd700\")\n    goldenrod = hex_to_bgr(\"#daa520\")\n    gray = hex_to_bgr(\"#808080\")\n    grey = hex_to_bgr(\"#808080\")\n    green = (0, 128, 0)\n    greenyellow = hex_to_bgr(\"#adff2f\")\n    honeydew = hex_to_bgr(\"#f0fff0\")\n    hotpink = hex_to_bgr(\"#ff69b4\")\n    indianred = hex_to_bgr(\"#cd5c5c\")\n    indigo = hex_to_bgr(\"#4b0082\")\n    ivory = hex_to_bgr(\"#fffff0\")\n    khaki = hex_to_bgr(\"#f0e68c\")\n    lavender = hex_to_bgr(\"#e6e6fa\")\n    lavenderblush = hex_to_bgr(\"#fff0f5\")\n    lawngreen = hex_to_bgr(\"#7cfc00\")\n    lemonchiffon = hex_to_bgr(\"#fffacd\")\n    lightblue = hex_to_bgr(\"#add8e6\")\n    lightcoral = hex_to_bgr(\"#f08080\")\n    lightcyan = hex_to_bgr(\"#e0ffff\")\n    lightgoldenrodyellow = hex_to_bgr(\"#fafad2\")\n    lightgreen = hex_to_bgr(\"#90ee90\")\n    lightgray = hex_to_bgr(\"#d3d3d3\")\n    lightgrey = hex_to_bgr(\"#d3d3d3\")\n    lightpink = hex_to_bgr(\"#ffb6c1\")\n    lightsalmon = hex_to_bgr(\"#ffa07a\")\n    lightseagreen = hex_to_bgr(\"#20b2aa\")\n    lightskyblue = hex_to_bgr(\"#87cefa\")\n    lightslategray = hex_to_bgr(\"#778899\")\n    lightslategrey = hex_to_bgr(\"#778899\")\n    lightsteelblue = hex_to_bgr(\"#b0c4de\")\n    lightyellow = hex_to_bgr(\"#ffffe0\")\n    lime = hex_to_bgr(\"#00ff00\")\n    limegreen = hex_to_bgr(\"#32cd32\")\n    linen = hex_to_bgr(\"#faf0e6\")\n    magenta = hex_to_bgr(\"#ff00ff\")\n    maroon = hex_to_bgr(\"#800000\")\n    mediumaquamarine = hex_to_bgr(\"#66cdaa\")\n    mediumblue = hex_to_bgr(\"#0000cd\")\n    mediumorchid = hex_to_bgr(\"#ba55d3\")\n    mediumpurple = hex_to_bgr(\"#9370db\")\n    mediumseagreen = hex_to_bgr(\"#3cb371\")\n    mediumslateblue = hex_to_bgr(\"#7b68ee\")\n    mediumspringgreen = hex_to_bgr(\"#00fa9a\")\n    mediumturquoise = hex_to_bgr(\"#48d1cc\")\n    mediumvioletred = hex_to_bgr(\"#c71585\")\n    midnightblue = hex_to_bgr(\"#191970\")\n    mintcream = hex_to_bgr(\"#f5fffa\")\n    mistyrose = hex_to_bgr(\"#ffe4e1\")\n    moccasin = hex_to_bgr(\"#ffe4b5\")\n    navajowhite = hex_to_bgr(\"#ffdead\")\n    navy = hex_to_bgr(\"#000080\")\n    oldlace = hex_to_bgr(\"#fdf5e6\")\n    olive = hex_to_bgr(\"#808000\")\n    olivedrab = hex_to_bgr(\"#6b8e23\")\n    orange = hex_to_bgr(\"#ffa500\")\n    orangered = hex_to_bgr(\"#ff4500\")\n    orchid = hex_to_bgr(\"#da70d6\")\n    palegoldenrod = hex_to_bgr(\"#eee8aa\")\n    palegreen = hex_to_bgr(\"#98fb98\")\n    paleturquoise = hex_to_bgr(\"#afeeee\")\n    palevioletred = hex_to_bgr(\"#db7093\")\n    papayawhip = hex_to_bgr(\"#ffefd5\")\n    peachpuff = hex_to_bgr(\"#ffdab9\")\n    peru = hex_to_bgr(\"#cd853f\")\n    pink = hex_to_bgr(\"#ffc0cb\")\n    plum = hex_to_bgr(\"#dda0dd\")\n    powderblue = hex_to_bgr(\"#b0e0e6\")\n    purple = hex_to_bgr(\"#800080\")\n    rebeccapurple = hex_to_bgr(\"#663399\")\n    red = hex_to_bgr(\"#ff0000\")\n    rosybrown = hex_to_bgr(\"#bc8f8f\")\n    royalblue = hex_to_bgr(\"#4169e1\")\n    saddlebrown = hex_to_bgr(\"#8b4513\")\n    salmon = hex_to_bgr(\"#fa8072\")\n    sandybrown = hex_to_bgr(\"#f4a460\")\n    seagreen = hex_to_bgr(\"#2e8b57\")\n    seashell = hex_to_bgr(\"#fff5ee\")\n    sienna = hex_to_bgr(\"#a0522d\")\n    silver = hex_to_bgr(\"#c0c0c0\")\n    skyblue = hex_to_bgr(\"#87ceeb\")\n    slateblue = hex_to_bgr(\"#6a5acd\")\n    slategray = hex_to_bgr(\"#708090\")\n    slategrey = hex_to_bgr(\"#708090\")\n    snow = hex_to_bgr(\"#fffafa\")\n    springgreen = hex_to_bgr(\"#00ff7f\")\n    steelblue = hex_to_bgr(\"#4682b4\")\n    tan = hex_to_bgr(\"#d2b48c\")\n    teal = hex_to_bgr(\"#008080\")\n    thistle = hex_to_bgr(\"#d8bfd8\")\n    tomato = hex_to_bgr(\"#ff6347\")\n    turquoise = hex_to_bgr(\"#40e0d0\")\n    violet = hex_to_bgr(\"#ee82ee\")\n    wheat = hex_to_bgr(\"#f5deb3\")\n    white = hex_to_bgr(\"#ffffff\")\n    whitesmoke = hex_to_bgr(\"#f5f5f5\")\n    yellow = hex_to_bgr(\"#ffff00\")\n    yellowgreen = hex_to_bgr(\"#9acd32\")\n\n    # seaborn tab20 colors\n    tab1 = hex_to_bgr(\"#1f77b4\")\n    tab2 = hex_to_bgr(\"#aec7e8\")\n    tab3 = hex_to_bgr(\"#ff7f0e\")\n    tab4 = hex_to_bgr(\"#ffbb78\")\n    tab5 = hex_to_bgr(\"#2ca02c\")\n    tab6 = hex_to_bgr(\"#98df8a\")\n    tab7 = hex_to_bgr(\"#d62728\")\n    tab8 = hex_to_bgr(\"#ff9896\")\n    tab9 = hex_to_bgr(\"#9467bd\")\n    tab10 = hex_to_bgr(\"#c5b0d5\")\n    tab11 = hex_to_bgr(\"#8c564b\")\n    tab12 = hex_to_bgr(\"#c49c94\")\n    tab13 = hex_to_bgr(\"#e377c2\")\n    tab14 = hex_to_bgr(\"#f7b6d2\")\n    tab15 = hex_to_bgr(\"#7f7f7f\")\n    tab16 = hex_to_bgr(\"#c7c7c7\")\n    tab17 = hex_to_bgr(\"#bcbd22\")\n    tab18 = hex_to_bgr(\"#dbdb8d\")\n    tab19 = hex_to_bgr(\"#17becf\")\n    tab20 = hex_to_bgr(\"#9edae5\")\n    # seaborn colorblind\n    cb1 = hex_to_bgr(\"#0173b2\")\n    cb2 = hex_to_bgr(\"#de8f05\")\n    cb3 = hex_to_bgr(\"#029e73\")\n    cb4 = hex_to_bgr(\"#d55e00\")\n    cb5 = hex_to_bgr(\"#cc78bc\")\n    cb6 = hex_to_bgr(\"#ca9161\")\n    cb7 = hex_to_bgr(\"#fbafe4\")\n    cb8 = hex_to_bgr(\"#949494\")\n    cb9 = hex_to_bgr(\"#ece133\")\n    cb10 = hex_to_bgr(\"#56b4e9\")\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.color.Palette","title":"<code>Palette</code>","text":"<p>Class to control the color pallete for drawing.</p> <p>Examples:</p> <p>Change palette:</p> <pre><code>&gt;&gt;&gt; from norfair import Palette\n&gt;&gt;&gt; Palette.set(\"colorblind\")\n&gt;&gt;&gt; # or a custom palette\n&gt;&gt;&gt; from norfair import Color\n&gt;&gt;&gt; Palette.set([Color.red, Color.blue, \"#ffeeff\"])\n</code></pre> Source code in <code>norfair/drawing/color.py</code> <pre><code>class Palette:\n    \"\"\"\n    Class to control the color pallete for drawing.\n\n    Examples\n    --------\n    Change palette:\n    &gt;&gt;&gt; from norfair import Palette\n    &gt;&gt;&gt; Palette.set(\"colorblind\")\n    &gt;&gt;&gt; # or a custom palette\n    &gt;&gt;&gt; from norfair import Color\n    &gt;&gt;&gt; Palette.set([Color.red, Color.blue, \"#ffeeff\"])\n    \"\"\"\n\n    _colors = PALETTES[\"tab10\"]\n    _default_color = Color.black\n\n    @classmethod\n    def set(cls, palette: Union[str, Iterable[ColorLike]]):\n        \"\"\"\n        Selects a color palette.\n\n        Parameters\n        ----------\n        palette : Union[str, Iterable[ColorLike]]\n            can be either\n            - the name of one of the predefined palettes `tab10`, `tab20`, or `colorblind`\n            - a list of ColorLike objects that can be parsed by [`parse_color`][norfair.drawing.color.parse_color]\n        \"\"\"\n        if isinstance(palette, str):\n            try:\n                cls._colors = PALETTES[palette]\n            except KeyError as e:\n                raise ValueError(\n                    f\"Invalid palette name '{palette}', valid values are {PALETTES.keys()}\"\n                ) from e\n        else:\n            colors = []\n            for c in palette:\n                colors.append(parse_color(c))\n\n            cls._colors = colors\n\n    @classmethod\n    def set_default_color(cls, color: ColorLike):\n        \"\"\"\n        Selects the default color of `choose_color` when hashable is None.\n\n        Parameters\n        ----------\n        color : ColorLike\n            The new default color.\n        \"\"\"\n        cls._default_color = parse_color(color)\n\n    @classmethod\n    def choose_color(cls, hashable: Hashable) -&gt; ColorType:\n        if hashable is None:\n            return cls._default_color\n        return cls._colors[abs(hash(hashable)) % len(cls._colors)]\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.color.Palette.set","title":"<code>set(palette)</code>  <code>classmethod</code>","text":"<p>Selects a color palette.</p> <p>Parameters:</p> Name Type Description Default <code>palette</code> <code>Union[str, Iterable[ColorLike]]</code> <p>can be either - the name of one of the predefined palettes <code>tab10</code>, <code>tab20</code>, or <code>colorblind</code> - a list of ColorLike objects that can be parsed by <code>parse_color</code></p> required Source code in <code>norfair/drawing/color.py</code> <pre><code>@classmethod\ndef set(cls, palette: Union[str, Iterable[ColorLike]]):\n    \"\"\"\n    Selects a color palette.\n\n    Parameters\n    ----------\n    palette : Union[str, Iterable[ColorLike]]\n        can be either\n        - the name of one of the predefined palettes `tab10`, `tab20`, or `colorblind`\n        - a list of ColorLike objects that can be parsed by [`parse_color`][norfair.drawing.color.parse_color]\n    \"\"\"\n    if isinstance(palette, str):\n        try:\n            cls._colors = PALETTES[palette]\n        except KeyError as e:\n            raise ValueError(\n                f\"Invalid palette name '{palette}', valid values are {PALETTES.keys()}\"\n            ) from e\n    else:\n        colors = []\n        for c in palette:\n            colors.append(parse_color(c))\n\n        cls._colors = colors\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.color.Palette.set_default_color","title":"<code>set_default_color(color)</code>  <code>classmethod</code>","text":"<p>Selects the default color of <code>choose_color</code> when hashable is None.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>ColorLike</code> <p>The new default color.</p> required Source code in <code>norfair/drawing/color.py</code> <pre><code>@classmethod\ndef set_default_color(cls, color: ColorLike):\n    \"\"\"\n    Selects the default color of `choose_color` when hashable is None.\n\n    Parameters\n    ----------\n    color : ColorLike\n        The new default color.\n    \"\"\"\n    cls._default_color = parse_color(color)\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.color.hex_to_bgr","title":"<code>hex_to_bgr(hex_value)</code>","text":"<p>Converts conventional 6 digits hex colors to BGR tuples</p> <p>Parameters:</p> Name Type Description Default <code>hex_value</code> <code>str</code> <p>hex value with leading <code>#</code> for instance <code>\"#ff0000\"</code></p> required <p>Returns:</p> Type Description <code>Tuple[int, int, int]</code> <p>BGR values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the string is invalid</p> Source code in <code>norfair/drawing/color.py</code> <pre><code>def hex_to_bgr(hex_value: str) -&gt; ColorType:\n    \"\"\"Converts conventional 6 digits hex colors to BGR tuples\n\n    Parameters\n    ----------\n    hex_value : str\n        hex value with leading `#` for instance `\"#ff0000\"`\n\n    Returns\n    -------\n    Tuple[int, int, int]\n        BGR values\n\n    Raises\n    ------\n    ValueError\n        if the string is invalid\n    \"\"\"\n    if re.match(\"#[a-f0-9]{6}$\", hex_value):\n        return (\n            int(hex_value[5:7], 16),\n            int(hex_value[3:5], 16),\n            int(hex_value[1:3], 16),\n        )\n\n    if re.match(\"#[a-f0-9]{3}$\", hex_value):\n        return (\n            int(hex_value[3] * 2, 16),\n            int(hex_value[2] * 2, 16),\n            int(hex_value[1] * 2, 16),\n        )\n    raise ValueError(f\"'{hex_value}' is not a valid color\")\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.color.parse_color","title":"<code>parse_color(color_like)</code>","text":"<p>Makes best effort to parse the given value to a Color</p> <p>Parameters:</p> Name Type Description Default <code>color_like</code> <code>ColorLike</code> <p>Can be one of:</p> <ol> <li>a string with the 6 digits hex value (<code>\"#ff0000\"</code>)</li> <li>a string with one of the names defined in Colors (<code>\"red\"</code>)</li> <li>a BGR tuple (<code>(0, 0, 255)</code>)</li> </ol> required <p>Returns:</p> Type Description <code>Color</code> <p>The BGR tuple.</p> Source code in <code>norfair/drawing/color.py</code> <pre><code>def parse_color(color_like: ColorLike) -&gt; ColorType:\n    \"\"\"Makes best effort to parse the given value to a Color\n\n    Parameters\n    ----------\n    color_like : ColorLike\n        Can be one of:\n\n        1. a string with the 6 digits hex value (`\"#ff0000\"`)\n        2. a string with one of the names defined in Colors (`\"red\"`)\n        3. a BGR tuple (`(0, 0, 255)`)\n\n    Returns\n    -------\n    Color\n        The BGR tuple.\n    \"\"\"\n    if isinstance(color_like, str):\n        if color_like.startswith(\"#\"):\n            return hex_to_bgr(color_like)\n        else:\n            return getattr(Color, color_like)\n    # TODO: validate?\n    return tuple([int(v) for v in color_like])\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.path","title":"<code>path</code>","text":""},{"location":"reference/drawing/#norfair.drawing.path.Paths","title":"<code>Paths</code>","text":"<p>Class that draws the paths taken by a set of points of interest defined from the coordinates of each tracker estimation.</p> <p>Parameters:</p> Name Type Description Default <code>get_points_to_draw</code> <code>Optional[Callable[[array], array]]</code> <p>Function that takes a list of points (the <code>.estimate</code> attribute of a <code>TrackedObject</code>) and returns a list of points for which we want to draw their paths.</p> <p>By default it is the mean point of all the points in the tracker.</p> <code>None</code> <code>thickness</code> <code>Optional[int]</code> <p>Thickness of the circles representing the paths of interest.</p> <code>None</code> <code>color</code> <code>Optional[Tuple[int, int, int]]</code> <p>Color of the circles representing the paths of interest.</p> <code>None</code> <code>radius</code> <code>Optional[int]</code> <p>Radius of the circles representing the paths of interest.</p> <code>None</code> <code>attenuation</code> <code>float</code> <p>A float number in [0, 1] that dictates the speed at which the path is erased. if it is <code>0</code> then the path is never erased.</p> <code>0.01</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from norfair import Tracker, Video, Path\n&gt;&gt;&gt; video = Video(\"video.mp4\")\n&gt;&gt;&gt; tracker = Tracker(...)\n&gt;&gt;&gt; path_drawer = Path()\n&gt;&gt;&gt; for frame in video:\n&gt;&gt;&gt;    detections = get_detections(frame)  # runs detector and returns Detections\n&gt;&gt;&gt;    tracked_objects = tracker.update(detections)\n&gt;&gt;&gt;    frame = path_drawer.draw(frame, tracked_objects)\n&gt;&gt;&gt;    video.write(frame)\n</code></pre> Source code in <code>norfair/drawing/path.py</code> <pre><code>class Paths:\n    \"\"\"\n    Class that draws the paths taken by a set of points of interest defined from the coordinates of each tracker estimation.\n\n    Parameters\n    ----------\n    get_points_to_draw : Optional[Callable[[np.array], np.array]], optional\n        Function that takes a list of points (the `.estimate` attribute of a [`TrackedObject`][norfair.tracker.TrackedObject])\n        and returns a list of points for which we want to draw their paths.\n\n        By default it is the mean point of all the points in the tracker.\n    thickness : Optional[int], optional\n        Thickness of the circles representing the paths of interest.\n    color : Optional[Tuple[int, int, int]], optional\n        [Color][norfair.drawing.Color] of the circles representing the paths of interest.\n    radius : Optional[int], optional\n        Radius of the circles representing the paths of interest.\n    attenuation : float, optional\n        A float number in [0, 1] that dictates the speed at which the path is erased.\n        if it is `0` then the path is never erased.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from norfair import Tracker, Video, Path\n    &gt;&gt;&gt; video = Video(\"video.mp4\")\n    &gt;&gt;&gt; tracker = Tracker(...)\n    &gt;&gt;&gt; path_drawer = Path()\n    &gt;&gt;&gt; for frame in video:\n    &gt;&gt;&gt;    detections = get_detections(frame)  # runs detector and returns Detections\n    &gt;&gt;&gt;    tracked_objects = tracker.update(detections)\n    &gt;&gt;&gt;    frame = path_drawer.draw(frame, tracked_objects)\n    &gt;&gt;&gt;    video.write(frame)\n    \"\"\"\n\n    def __init__(\n        self,\n        get_points_to_draw: Optional[Callable[[np.array], np.array]] = None,\n        thickness: Optional[int] = None,\n        color: Optional[Tuple[int, int, int]] = None,\n        radius: Optional[int] = None,\n        attenuation: float = 0.01,\n    ):\n        if get_points_to_draw is None:\n\n            def get_points_to_draw(points):\n                return [np.mean(np.array(points), axis=0)]\n\n        self.get_points_to_draw = get_points_to_draw\n\n        self.radius = radius\n        self.thickness = thickness\n        self.color = color\n        self.mask = None\n        self.attenuation_factor = 1 - attenuation\n\n    def draw(\n        self, frame: np.ndarray, tracked_objects: Sequence[TrackedObject]\n    ) -&gt; np.array:\n        \"\"\"\n        Draw the paths of the points interest on a frame.\n\n        !!! warning\n            This method does **not** draw frames in place as other drawers do, the resulting frame is returned.\n\n        Parameters\n        ----------\n        frame : np.ndarray\n            The OpenCV frame to draw on.\n        tracked_objects : Sequence[TrackedObject]\n            List of [`TrackedObject`][norfair.tracker.TrackedObject] to get the points of interest in order to update the paths.\n\n        Returns\n        -------\n        np.array\n            The resulting frame.\n        \"\"\"\n        if self.mask is None:\n            frame_scale = frame.shape[0] / 100\n\n            if self.radius is None:\n                self.radius = int(max(frame_scale * 0.7, 1))\n            if self.thickness is None:\n                self.thickness = int(max(frame_scale / 7, 1))\n\n            self.mask = np.zeros(frame.shape, np.uint8)\n\n        self.mask = (self.mask * self.attenuation_factor).astype(\"uint8\")\n\n        for obj in tracked_objects:\n            if obj.abs_to_rel is not None:\n                warn_once(\n                    \"It seems that your using the Path drawer together with MotionEstimator. This is not fully supported and the results will not be what's expected\"\n                )\n\n            if self.color is None:\n                color = Palette.choose_color(obj.id)\n            else:\n                color = self.color\n\n            points_to_draw = self.get_points_to_draw(obj.estimate)\n\n            for point in points_to_draw:\n                self.mask = Drawer.circle(\n                    self.mask,\n                    position=tuple(point.astype(int)),\n                    radius=self.radius,\n                    color=color,\n                    thickness=self.thickness,\n                )\n\n        return Drawer.alpha_blend(self.mask, frame, alpha=1, beta=1)\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.path.Paths.draw","title":"<code>draw(frame, tracked_objects)</code>","text":"<p>Draw the paths of the points interest on a frame.</p> <p>Warning</p> <p>This method does not draw frames in place as other drawers do, the resulting frame is returned.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The OpenCV frame to draw on.</p> required <code>tracked_objects</code> <code>Sequence[TrackedObject]</code> <p>List of <code>TrackedObject</code> to get the points of interest in order to update the paths.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The resulting frame.</p> Source code in <code>norfair/drawing/path.py</code> <pre><code>def draw(\n    self, frame: np.ndarray, tracked_objects: Sequence[TrackedObject]\n) -&gt; np.array:\n    \"\"\"\n    Draw the paths of the points interest on a frame.\n\n    !!! warning\n        This method does **not** draw frames in place as other drawers do, the resulting frame is returned.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The OpenCV frame to draw on.\n    tracked_objects : Sequence[TrackedObject]\n        List of [`TrackedObject`][norfair.tracker.TrackedObject] to get the points of interest in order to update the paths.\n\n    Returns\n    -------\n    np.array\n        The resulting frame.\n    \"\"\"\n    if self.mask is None:\n        frame_scale = frame.shape[0] / 100\n\n        if self.radius is None:\n            self.radius = int(max(frame_scale * 0.7, 1))\n        if self.thickness is None:\n            self.thickness = int(max(frame_scale / 7, 1))\n\n        self.mask = np.zeros(frame.shape, np.uint8)\n\n    self.mask = (self.mask * self.attenuation_factor).astype(\"uint8\")\n\n    for obj in tracked_objects:\n        if obj.abs_to_rel is not None:\n            warn_once(\n                \"It seems that your using the Path drawer together with MotionEstimator. This is not fully supported and the results will not be what's expected\"\n            )\n\n        if self.color is None:\n            color = Palette.choose_color(obj.id)\n        else:\n            color = self.color\n\n        points_to_draw = self.get_points_to_draw(obj.estimate)\n\n        for point in points_to_draw:\n            self.mask = Drawer.circle(\n                self.mask,\n                position=tuple(point.astype(int)),\n                radius=self.radius,\n                color=color,\n                thickness=self.thickness,\n            )\n\n    return Drawer.alpha_blend(self.mask, frame, alpha=1, beta=1)\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.path.AbsolutePaths","title":"<code>AbsolutePaths</code>","text":"<p>Class that draws the absolute paths taken by a set of points.</p> <p>Works just like <code>Paths</code> but supports camera motion.</p> <p>Warning</p> <p>This drawer is not optimized so it can be stremely slow. Performance degrades linearly with <code>max_history * number_of_tracked_objects</code>.</p> <p>Parameters:</p> Name Type Description Default <code>get_points_to_draw</code> <code>Optional[Callable[[array], array]]</code> <p>Function that takes a list of points (the <code>.estimate</code> attribute of a <code>TrackedObject</code>) and returns a list of points for which we want to draw their paths.</p> <p>By default it is the mean point of all the points in the tracker.</p> <code>None</code> <code>thickness</code> <code>Optional[int]</code> <p>Thickness of the circles representing the paths of interest.</p> <code>None</code> <code>color</code> <code>Optional[Tuple[int, int, int]]</code> <p>Color of the circles representing the paths of interest.</p> <code>None</code> <code>radius</code> <code>Optional[int]</code> <p>Radius of the circles representing the paths of interest.</p> <code>None</code> <code>max_history</code> <code>int</code> <p>Number of past points to include in the path. High values make the drawing slower</p> <code>20</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from norfair import Tracker, Video, Path\n&gt;&gt;&gt; video = Video(\"video.mp4\")\n&gt;&gt;&gt; tracker = Tracker(...)\n&gt;&gt;&gt; path_drawer = Path()\n&gt;&gt;&gt; for frame in video:\n&gt;&gt;&gt;    detections = get_detections(frame)  # runs detector and returns Detections\n&gt;&gt;&gt;    tracked_objects = tracker.update(detections)\n&gt;&gt;&gt;    frame = path_drawer.draw(frame, tracked_objects)\n&gt;&gt;&gt;    video.write(frame)\n</code></pre> Source code in <code>norfair/drawing/path.py</code> <pre><code>class AbsolutePaths:\n    \"\"\"\n    Class that draws the absolute paths taken by a set of points.\n\n    Works just like [`Paths`][norfair.drawing.Paths] but supports camera motion.\n\n    !!! warning\n        This drawer is not optimized so it can be stremely slow. Performance degrades linearly with\n        `max_history * number_of_tracked_objects`.\n\n    Parameters\n    ----------\n    get_points_to_draw : Optional[Callable[[np.array], np.array]], optional\n        Function that takes a list of points (the `.estimate` attribute of a [`TrackedObject`][norfair.tracker.TrackedObject])\n        and returns a list of points for which we want to draw their paths.\n\n        By default it is the mean point of all the points in the tracker.\n    thickness : Optional[int], optional\n        Thickness of the circles representing the paths of interest.\n    color : Optional[Tuple[int, int, int]], optional\n        [Color][norfair.drawing.Color] of the circles representing the paths of interest.\n    radius : Optional[int], optional\n        Radius of the circles representing the paths of interest.\n    max_history : int, optional\n        Number of past points to include in the path. High values make the drawing slower\n\n    Examples\n    --------\n    &gt;&gt;&gt; from norfair import Tracker, Video, Path\n    &gt;&gt;&gt; video = Video(\"video.mp4\")\n    &gt;&gt;&gt; tracker = Tracker(...)\n    &gt;&gt;&gt; path_drawer = Path()\n    &gt;&gt;&gt; for frame in video:\n    &gt;&gt;&gt;    detections = get_detections(frame)  # runs detector and returns Detections\n    &gt;&gt;&gt;    tracked_objects = tracker.update(detections)\n    &gt;&gt;&gt;    frame = path_drawer.draw(frame, tracked_objects)\n    &gt;&gt;&gt;    video.write(frame)\n    \"\"\"\n\n    def __init__(\n        self,\n        get_points_to_draw: Optional[Callable[[np.array], np.array]] = None,\n        thickness: Optional[int] = None,\n        color: Optional[Tuple[int, int, int]] = None,\n        radius: Optional[int] = None,\n        max_history=20,\n    ):\n\n        if get_points_to_draw is None:\n\n            def get_points_to_draw(points):\n                return [np.mean(np.array(points), axis=0)]\n\n        self.get_points_to_draw = get_points_to_draw\n\n        self.radius = radius\n        self.thickness = thickness\n        self.color = color\n        self.past_points = defaultdict(lambda: [])\n        self.max_history = max_history\n        self.alphas = np.linspace(0.99, 0.01, max_history)\n\n    def draw(self, frame, tracked_objects, coord_transform=None):\n        frame_scale = frame.shape[0] / 100\n\n        if self.radius is None:\n            self.radius = int(max(frame_scale * 0.7, 1))\n        if self.thickness is None:\n            self.thickness = int(max(frame_scale / 7, 1))\n        for obj in tracked_objects:\n            if not obj.live_points.any():\n                continue\n\n            if self.color is None:\n                color = Palette.choose_color(obj.id)\n            else:\n                color = self.color\n\n            points_to_draw = self.get_points_to_draw(obj.get_estimate(absolute=True))\n\n            for point in coord_transform.abs_to_rel(points_to_draw):\n                Drawer.circle(\n                    frame,\n                    position=tuple(point.astype(int)),\n                    radius=self.radius,\n                    color=color,\n                    thickness=self.thickness,\n                )\n\n            last = points_to_draw\n            for i, past_points in enumerate(self.past_points[obj.id]):\n                overlay = frame.copy()\n                last = coord_transform.abs_to_rel(last)\n                for j, point in enumerate(coord_transform.abs_to_rel(past_points)):\n                    Drawer.line(\n                        overlay,\n                        tuple(last[j].astype(int)),\n                        tuple(point.astype(int)),\n                        color=color,\n                        thickness=self.thickness,\n                    )\n                last = past_points\n\n                alpha = self.alphas[i]\n                frame = Drawer.alpha_blend(overlay, frame, alpha=alpha)\n            self.past_points[obj.id].insert(0, points_to_draw)\n            self.past_points[obj.id] = self.past_points[obj.id][: self.max_history]\n        return frame\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.fixed_camera","title":"<code>fixed_camera</code>","text":""},{"location":"reference/drawing/#norfair.drawing.fixed_camera.FixedCamera","title":"<code>FixedCamera</code>","text":"<p>Class used to stabilize video based on the camera motion.</p> <p>Starts with a larger frame, where the original frame is drawn on top of a black background. As the camera moves, the smaller frame moves in the opposite direction, stabilizing the objects in it.</p> <p>Useful for debugging or demoing the camera motion. </p> <p>Warning</p> <p>This only works with <code>TranslationTransformation</code>, using <code>HomographyTransformation</code> will result in unexpected behaviour.</p> <p>Warning</p> <p>If using other drawers, always apply this one last. Using other drawers on the scaled up frame will not work as expected.</p> <p>Note</p> <p>Sometimes the camera moves so far from the original point that the result won't fit in the scaled-up frame. In this case, a warning will be logged and the frames will be cropped to avoid errors.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The resulting video will have a resolution of <code>scale * (H, W)</code> where HxW is the resolution of the original video. Use a bigger scale if the camera is moving too much.</p> <code>2</code> <code>attenuation</code> <code>float</code> <p>Controls how fast the older frames fade to black.</p> <code>0.05</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # setup\n&gt;&gt;&gt; tracker = Tracker(\"frobenious\", 100)\n&gt;&gt;&gt; motion_estimator = MotionEstimator()\n&gt;&gt;&gt; video = Video(input_path=\"video.mp4\")\n&gt;&gt;&gt; fixed_camera = FixedCamera()\n&gt;&gt;&gt; # process video\n&gt;&gt;&gt; for frame in video:\n&gt;&gt;&gt;     coord_transformations = motion_estimator.update(frame)\n&gt;&gt;&gt;     detections = get_detections(frame)\n&gt;&gt;&gt;     tracked_objects = tracker.update(detections, coord_transformations)\n&gt;&gt;&gt;     draw_tracked_objects(frame, tracked_objects)  # fixed_camera should always be the last drawer\n&gt;&gt;&gt;     bigger_frame = fixed_camera.adjust_frame(frame, coord_transformations)\n&gt;&gt;&gt;     video.write(bigger_frame)\n</code></pre> Source code in <code>norfair/drawing/fixed_camera.py</code> <pre><code>class FixedCamera:\n    \"\"\"\n    Class used to stabilize video based on the camera motion.\n\n    Starts with a larger frame, where the original frame is drawn on top of a black background.\n    As the camera moves, the smaller frame moves in the opposite direction, stabilizing the objects in it.\n\n    Useful for debugging or demoing the camera motion.\n    ![Example GIF](../../videos/camera_stabilization.gif)\n\n    !!! Warning\n        This only works with [`TranslationTransformation`][norfair.camera_motion.TranslationTransformation],\n        using [`HomographyTransformation`][norfair.camera_motion.HomographyTransformation] will result in\n        unexpected behaviour.\n\n    !!! Warning\n        If using other drawers, always apply this one last. Using other drawers on the scaled up frame will not work as expected.\n\n    !!! Note\n        Sometimes the camera moves so far from the original point that the result won't fit in the scaled-up frame.\n        In this case, a warning will be logged and the frames will be cropped to avoid errors.\n\n    Parameters\n    ----------\n    scale : float, optional\n        The resulting video will have a resolution of `scale * (H, W)` where HxW is the resolution of the original video.\n        Use a bigger scale if the camera is moving too much.\n    attenuation : float, optional\n        Controls how fast the older frames fade to black.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # setup\n    &gt;&gt;&gt; tracker = Tracker(\"frobenious\", 100)\n    &gt;&gt;&gt; motion_estimator = MotionEstimator()\n    &gt;&gt;&gt; video = Video(input_path=\"video.mp4\")\n    &gt;&gt;&gt; fixed_camera = FixedCamera()\n    &gt;&gt;&gt; # process video\n    &gt;&gt;&gt; for frame in video:\n    &gt;&gt;&gt;     coord_transformations = motion_estimator.update(frame)\n    &gt;&gt;&gt;     detections = get_detections(frame)\n    &gt;&gt;&gt;     tracked_objects = tracker.update(detections, coord_transformations)\n    &gt;&gt;&gt;     draw_tracked_objects(frame, tracked_objects)  # fixed_camera should always be the last drawer\n    &gt;&gt;&gt;     bigger_frame = fixed_camera.adjust_frame(frame, coord_transformations)\n    &gt;&gt;&gt;     video.write(bigger_frame)\n    \"\"\"\n\n    def __init__(self, scale: float = 2, attenuation: float = 0.05):\n        self.scale = scale\n        self._background = None\n        self._attenuation_factor = 1 - attenuation\n\n    def adjust_frame(\n        self, frame: np.ndarray, coord_transformation: TranslationTransformation\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Render scaled up frame.\n\n        Parameters\n        ----------\n        frame : np.ndarray\n            The OpenCV frame.\n        coord_transformation : TranslationTransformation\n            The coordinate transformation as returned by the [`MotionEstimator`][norfair.camera_motion.MotionEstimator]\n\n        Returns\n        -------\n        np.ndarray\n            The new bigger frame with the original frame drawn on it.\n        \"\"\"\n\n        # initialize background if necessary\n        if self._background is None:\n            original_size = (\n                frame.shape[1],\n                frame.shape[0],\n            )  # OpenCV format is (width, height)\n\n            scaled_size = tuple(\n                (np.array(original_size) * np.array(self.scale)).round().astype(int)\n            )\n            self._background = np.zeros(\n                [scaled_size[1], scaled_size[0], frame.shape[-1]],\n                frame.dtype,\n            )\n        else:\n            self._background = (self._background * self._attenuation_factor).astype(\n                frame.dtype\n            )\n\n        # top_left is the anchor coordinate from where we start drawing the fame on top of the background\n        # aim to draw it in the center of the background but transformations will move this point\n        top_left = (\n            np.array(self._background.shape[:2]) // 2 - np.array(frame.shape[:2]) // 2\n        )\n        top_left = (\n            coord_transformation.rel_to_abs(top_left[::-1]).round().astype(int)[::-1]\n        )\n        # box of the background that will be updated and the limits of it\n        background_y0, background_y1 = (top_left[0], top_left[0] + frame.shape[0])\n        background_x0, background_x1 = (top_left[1], top_left[1] + frame.shape[1])\n        background_size_y, background_size_x = self._background.shape[:2]\n\n        # define box of the frame that will be used\n        # if the scale is not enough to support the movement, warn the user but keep drawing\n        # cropping the frame so that the operation doesn't fail\n        frame_y0, frame_y1, frame_x0, frame_x1 = (0, frame.shape[0], 0, frame.shape[1])\n        if (\n            background_y0 &lt; 0\n            or background_x0 &lt; 0\n            or background_y1 &gt; background_size_y\n            or background_x1 &gt; background_size_x\n        ):\n            warn_once(\n                \"moving_camera_scale is not enough to cover the range of camera movement, frame will be cropped\"\n            )\n            # crop left or top of the frame if necessary\n            frame_y0 = max(-background_y0, 0)\n            frame_x0 = max(-background_x0, 0)\n            # crop right or bottom of the frame if necessary\n            frame_y1 = max(\n                min(background_size_y - background_y0, background_y1 - background_y0), 0\n            )\n            frame_x1 = max(\n                min(background_size_x - background_x0, background_x1 - background_x0), 0\n            )\n            # handle cases where the limits of the background become negative which numpy will interpret incorrectly\n            background_y0 = max(background_y0, 0)\n            background_x0 = max(background_x0, 0)\n            background_y1 = max(background_y1, 0)\n            background_x1 = max(background_x1, 0)\n        self._background[\n            background_y0:background_y1, background_x0:background_x1, :\n        ] = frame[frame_y0:frame_y1, frame_x0:frame_x1, :]\n        return self._background\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.fixed_camera.FixedCamera.adjust_frame","title":"<code>adjust_frame(frame, coord_transformation)</code>","text":"<p>Render scaled up frame.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The OpenCV frame.</p> required <code>coord_transformation</code> <code>TranslationTransformation</code> <p>The coordinate transformation as returned by the <code>MotionEstimator</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The new bigger frame with the original frame drawn on it.</p> Source code in <code>norfair/drawing/fixed_camera.py</code> <pre><code>def adjust_frame(\n    self, frame: np.ndarray, coord_transformation: TranslationTransformation\n) -&gt; np.ndarray:\n    \"\"\"\n    Render scaled up frame.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The OpenCV frame.\n    coord_transformation : TranslationTransformation\n        The coordinate transformation as returned by the [`MotionEstimator`][norfair.camera_motion.MotionEstimator]\n\n    Returns\n    -------\n    np.ndarray\n        The new bigger frame with the original frame drawn on it.\n    \"\"\"\n\n    # initialize background if necessary\n    if self._background is None:\n        original_size = (\n            frame.shape[1],\n            frame.shape[0],\n        )  # OpenCV format is (width, height)\n\n        scaled_size = tuple(\n            (np.array(original_size) * np.array(self.scale)).round().astype(int)\n        )\n        self._background = np.zeros(\n            [scaled_size[1], scaled_size[0], frame.shape[-1]],\n            frame.dtype,\n        )\n    else:\n        self._background = (self._background * self._attenuation_factor).astype(\n            frame.dtype\n        )\n\n    # top_left is the anchor coordinate from where we start drawing the fame on top of the background\n    # aim to draw it in the center of the background but transformations will move this point\n    top_left = (\n        np.array(self._background.shape[:2]) // 2 - np.array(frame.shape[:2]) // 2\n    )\n    top_left = (\n        coord_transformation.rel_to_abs(top_left[::-1]).round().astype(int)[::-1]\n    )\n    # box of the background that will be updated and the limits of it\n    background_y0, background_y1 = (top_left[0], top_left[0] + frame.shape[0])\n    background_x0, background_x1 = (top_left[1], top_left[1] + frame.shape[1])\n    background_size_y, background_size_x = self._background.shape[:2]\n\n    # define box of the frame that will be used\n    # if the scale is not enough to support the movement, warn the user but keep drawing\n    # cropping the frame so that the operation doesn't fail\n    frame_y0, frame_y1, frame_x0, frame_x1 = (0, frame.shape[0], 0, frame.shape[1])\n    if (\n        background_y0 &lt; 0\n        or background_x0 &lt; 0\n        or background_y1 &gt; background_size_y\n        or background_x1 &gt; background_size_x\n    ):\n        warn_once(\n            \"moving_camera_scale is not enough to cover the range of camera movement, frame will be cropped\"\n        )\n        # crop left or top of the frame if necessary\n        frame_y0 = max(-background_y0, 0)\n        frame_x0 = max(-background_x0, 0)\n        # crop right or bottom of the frame if necessary\n        frame_y1 = max(\n            min(background_size_y - background_y0, background_y1 - background_y0), 0\n        )\n        frame_x1 = max(\n            min(background_size_x - background_x0, background_x1 - background_x0), 0\n        )\n        # handle cases where the limits of the background become negative which numpy will interpret incorrectly\n        background_y0 = max(background_y0, 0)\n        background_x0 = max(background_x0, 0)\n        background_y1 = max(background_y1, 0)\n        background_x1 = max(background_x1, 0)\n    self._background[\n        background_y0:background_y1, background_x0:background_x1, :\n    ] = frame[frame_y0:frame_y1, frame_x0:frame_x1, :]\n    return self._background\n</code></pre>"},{"location":"reference/drawing/#norfair.drawing.absolute_grid","title":"<code>absolute_grid</code>","text":""},{"location":"reference/drawing/#norfair.drawing.absolute_grid.draw_absolute_grid","title":"<code>draw_absolute_grid(frame, coord_transformations, grid_size=20, radius=2, thickness=1, color=Color.black, polar=False)</code>","text":"<p>Draw a grid of points in absolute coordinates.</p> <p>Useful for debugging camera motion.</p> <p>The points are drawn as if the camera were in the center of a sphere and points are drawn in the intersection of latitude and longitude lines over the surface of the sphere.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The OpenCV frame to draw on.</p> required <code>coord_transformations</code> <code>CoordinatesTransformation</code> <p>The coordinate transformation as returned by the <code>MotionEstimator</code></p> required <code>grid_size</code> <code>int</code> <p>How many points to draw.</p> <code>20</code> <code>radius</code> <code>int</code> <p>Size of each point.</p> <code>2</code> <code>thickness</code> <code>int</code> <p>Thickness of each point</p> <code>1</code> <code>color</code> <code>ColorType</code> <p>Color of the points.</p> <code>black</code> <code>polar</code> <code>Bool</code> <p>If True, the points on the first frame are drawn as if the camera were pointing to a pole (viewed from the center of the earth). By default, False is used which means the points are drawn as if the camera were pointing to the Equator.</p> <code>False</code> Source code in <code>norfair/drawing/absolute_grid.py</code> <pre><code>def draw_absolute_grid(\n    frame: np.ndarray,\n    coord_transformations: CoordinatesTransformation,\n    grid_size: int = 20,\n    radius: int = 2,\n    thickness: int = 1,\n    color: ColorType = Color.black,\n    polar: bool = False,\n):\n    \"\"\"\n    Draw a grid of points in absolute coordinates.\n\n    Useful for debugging camera motion.\n\n    The points are drawn as if the camera were in the center of a sphere and points are drawn in the intersection\n    of latitude and longitude lines over the surface of the sphere.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The OpenCV frame to draw on.\n    coord_transformations : CoordinatesTransformation\n        The coordinate transformation as returned by the [`MotionEstimator`][norfair.camera_motion.MotionEstimator]\n    grid_size : int, optional\n        How many points to draw.\n    radius : int, optional\n        Size of each point.\n    thickness : int, optional\n        Thickness of each point\n    color : ColorType, optional\n        Color of the points.\n    polar : Bool, optional\n        If True, the points on the first frame are drawn as if the camera were pointing to a pole (viewed from the center of the earth).\n        By default, False is used which means the points are drawn as if the camera were pointing to the Equator.\n    \"\"\"\n    h, w, _ = frame.shape\n\n    # get absolute points grid\n    points = _get_grid(grid_size, w, h, polar=polar)\n\n    # transform the points to relative coordinates\n    if coord_transformations is None:\n        points_transformed = points\n    else:\n        points_transformed = coord_transformations.abs_to_rel(points)\n\n    # filter points that are not visible\n    visible_points = points_transformed[\n        (points_transformed &lt;= np.array([w, h])).all(axis=1)\n        &amp; (points_transformed &gt;= 0).all(axis=1)\n    ]\n    for point in visible_points:\n        Drawer.cross(\n            frame, point.astype(int), radius=radius, thickness=thickness, color=color\n        )\n</code></pre>"},{"location":"reference/filter/","title":"Filter","text":""},{"location":"reference/filter/#norfair.filter.FilterPyKalmanFilterFactory","title":"<code>FilterPyKalmanFilterFactory</code>","text":"<p>             Bases: <code>FilterFactory</code></p> <p>This class can be used either to change some parameters of the KalmanFilter that the tracker uses, or to fully customize the predictive filter implementation to use (as long as the methods and properties are compatible).</p> <p>The former case only requires changing the default parameters upon tracker creation: <code>tracker = Tracker(..., filter_factory=FilterPyKalmanFilterFactory(R=100))</code>, while the latter requires creating your own class extending <code>FilterPyKalmanFilterFactory</code>, and rewriting its <code>create_filter</code> method to return your own customized filter.</p> <p>Parameters:</p> Name Type Description Default <code>R</code> <code>float</code> <p>Multiplier for the sensor measurement noise matrix, by default 4.0</p> <code>4.0</code> <code>Q</code> <code>float</code> <p>Multiplier for the process uncertainty, by default 0.1</p> <code>0.1</code> <code>P</code> <code>float</code> <p>Multiplier for the initial covariance matrix estimation, only in the entries that correspond to position (not speed) variables, by default 10.0</p> <code>10.0</code> See Also <p><code>filterpy.KalmanFilter</code>.</p> Source code in <code>norfair/filter.py</code> <pre><code>class FilterPyKalmanFilterFactory(FilterFactory):\n    \"\"\"\n    This class can be used either to change some parameters of the [KalmanFilter](https://filterpy.readthedocs.io/en/latest/kalman/KalmanFilter.html)\n    that the tracker uses, or to fully customize the predictive filter implementation to use (as long as the methods and properties are compatible).\n\n    The former case only requires changing the default parameters upon tracker creation: `tracker = Tracker(..., filter_factory=FilterPyKalmanFilterFactory(R=100))`,\n    while the latter requires creating your own class extending `FilterPyKalmanFilterFactory`, and rewriting its `create_filter` method to return your own customized filter.\n\n    Parameters\n    ----------\n    R : float, optional\n        Multiplier for the sensor measurement noise matrix, by default 4.0\n    Q : float, optional\n        Multiplier for the process uncertainty, by default 0.1\n    P : float, optional\n        Multiplier for the initial covariance matrix estimation, only in the entries that correspond to position (not speed) variables, by default 10.0\n\n    See Also\n    --------\n    [`filterpy.KalmanFilter`](https://filterpy.readthedocs.io/en/latest/kalman/KalmanFilter.html).\n    \"\"\"\n\n    def __init__(self, R: float = 4.0, Q: float = 0.1, P: float = 10.0):\n        self.R = R\n        self.Q = Q\n        self.P = P\n\n    def create_filter(self, initial_detection: np.ndarray) -&gt; KalmanFilter:\n        \"\"\"\n        This method returns a new predictive filter instance with the current setup, to be used by each new [`TrackedObject`][norfair.tracker.TrackedObject] that is created.\n        This predictive filter will be used to estimate speed and future positions of the object, to better match the detections during its trajectory.\n\n        Parameters\n        ----------\n        initial_detection : np.ndarray\n            numpy array of shape `(number of points per object, 2)`, corresponding to the [`Detection.points`][norfair.tracker.Detection] of the tracked object being born,\n            which shall be used as initial position estimation for it.\n\n        Returns\n        -------\n        KalmanFilter\n            The kalman filter\n        \"\"\"\n        num_points = initial_detection.shape[0]\n        dim_points = initial_detection.shape[1]\n        dim_z = dim_points * num_points\n        dim_x = 2 * dim_z  # We need to accommodate for velocities\n\n        filter = KalmanFilter(dim_x=dim_x, dim_z=dim_z)\n\n        # State transition matrix (models physics): numpy.array()\n        filter.F = np.eye(dim_x)\n        dt = 1  # At each step we update pos with v * dt\n\n        filter.F[:dim_z, dim_z:] = dt * np.eye(dim_z)\n\n        # Measurement function: numpy.array(dim_z, dim_x)\n        filter.H = np.eye(\n            dim_z,\n            dim_x,\n        )\n\n        # Measurement uncertainty (sensor noise): numpy.array(dim_z, dim_z)\n        filter.R *= self.R\n\n        # Process uncertainty: numpy.array(dim_x, dim_x)\n        # Don't decrease it too much or trackers pay too little attention to detections\n        filter.Q[dim_z:, dim_z:] *= self.Q\n\n        # Initial state: numpy.array(dim_x, 1)\n        filter.x[:dim_z] = np.expand_dims(initial_detection.flatten(), 0).T\n        filter.x[dim_z:] = 0\n\n        # Estimation uncertainty: numpy.array(dim_x, dim_x)\n        filter.P[dim_z:, dim_z:] *= self.P\n\n        return filter\n</code></pre>"},{"location":"reference/filter/#norfair.filter.FilterPyKalmanFilterFactory.create_filter","title":"<code>create_filter(initial_detection)</code>","text":"<p>This method returns a new predictive filter instance with the current setup, to be used by each new <code>TrackedObject</code> that is created. This predictive filter will be used to estimate speed and future positions of the object, to better match the detections during its trajectory.</p> <p>Parameters:</p> Name Type Description Default <code>initial_detection</code> <code>ndarray</code> <p>numpy array of shape <code>(number of points per object, 2)</code>, corresponding to the <code>Detection.points</code> of the tracked object being born, which shall be used as initial position estimation for it.</p> required <p>Returns:</p> Type Description <code>KalmanFilter</code> <p>The kalman filter</p> Source code in <code>norfair/filter.py</code> <pre><code>def create_filter(self, initial_detection: np.ndarray) -&gt; KalmanFilter:\n    \"\"\"\n    This method returns a new predictive filter instance with the current setup, to be used by each new [`TrackedObject`][norfair.tracker.TrackedObject] that is created.\n    This predictive filter will be used to estimate speed and future positions of the object, to better match the detections during its trajectory.\n\n    Parameters\n    ----------\n    initial_detection : np.ndarray\n        numpy array of shape `(number of points per object, 2)`, corresponding to the [`Detection.points`][norfair.tracker.Detection] of the tracked object being born,\n        which shall be used as initial position estimation for it.\n\n    Returns\n    -------\n    KalmanFilter\n        The kalman filter\n    \"\"\"\n    num_points = initial_detection.shape[0]\n    dim_points = initial_detection.shape[1]\n    dim_z = dim_points * num_points\n    dim_x = 2 * dim_z  # We need to accommodate for velocities\n\n    filter = KalmanFilter(dim_x=dim_x, dim_z=dim_z)\n\n    # State transition matrix (models physics): numpy.array()\n    filter.F = np.eye(dim_x)\n    dt = 1  # At each step we update pos with v * dt\n\n    filter.F[:dim_z, dim_z:] = dt * np.eye(dim_z)\n\n    # Measurement function: numpy.array(dim_z, dim_x)\n    filter.H = np.eye(\n        dim_z,\n        dim_x,\n    )\n\n    # Measurement uncertainty (sensor noise): numpy.array(dim_z, dim_z)\n    filter.R *= self.R\n\n    # Process uncertainty: numpy.array(dim_x, dim_x)\n    # Don't decrease it too much or trackers pay too little attention to detections\n    filter.Q[dim_z:, dim_z:] *= self.Q\n\n    # Initial state: numpy.array(dim_x, 1)\n    filter.x[:dim_z] = np.expand_dims(initial_detection.flatten(), 0).T\n    filter.x[dim_z:] = 0\n\n    # Estimation uncertainty: numpy.array(dim_x, dim_x)\n    filter.P[dim_z:, dim_z:] *= self.P\n\n    return filter\n</code></pre>"},{"location":"reference/filter/#norfair.filter.OptimizedKalmanFilterFactory","title":"<code>OptimizedKalmanFilterFactory</code>","text":"<p>             Bases: <code>FilterFactory</code></p> <p>Creates faster Filters than <code>FilterPyKalmanFilterFactory</code>.</p> <p>It allows the user to create Kalman Filter optimized for tracking and set its parameters.</p> <p>Parameters:</p> Name Type Description Default <code>R</code> <code>float</code> <p>Multiplier for the sensor measurement noise matrix.</p> <code>4.0</code> <code>Q</code> <code>float</code> <p>Multiplier for the process uncertainty.</p> <code>0.1</code> <code>pos_variance</code> <code>float</code> <p>Multiplier for the initial covariance matrix estimation, only in the entries that correspond to position (not speed) variables.</p> <code>10</code> <code>pos_vel_covariance</code> <code>float</code> <p>Multiplier for the initial covariance matrix estimation, only in the entries that correspond to the covariance between position and speed.</p> <code>0</code> <code>vel_variance</code> <code>float</code> <p>Multiplier for the initial covariance matrix estimation, only in the entries that correspond to velocity (not position) variables.</p> <code>1</code> Source code in <code>norfair/filter.py</code> <pre><code>class OptimizedKalmanFilterFactory(FilterFactory):\n    \"\"\"\n    Creates faster Filters than [`FilterPyKalmanFilterFactory`][norfair.filter.FilterPyKalmanFilterFactory].\n\n    It allows the user to create Kalman Filter optimized for tracking and set its parameters.\n\n    Parameters\n    ----------\n    R : float, optional\n        Multiplier for the sensor measurement noise matrix.\n    Q : float, optional\n        Multiplier for the process uncertainty.\n    pos_variance : float, optional\n        Multiplier for the initial covariance matrix estimation, only in the entries that correspond to position (not speed) variables.\n    pos_vel_covariance : float, optional\n        Multiplier for the initial covariance matrix estimation, only in the entries that correspond to the covariance between position and speed.\n    vel_variance : float, optional\n        Multiplier for the initial covariance matrix estimation, only in the entries that correspond to velocity (not position) variables.\n    \"\"\"\n\n    def __init__(\n        self,\n        R: float = 4.0,\n        Q: float = 0.1,\n        pos_variance: float = 10,\n        pos_vel_covariance: float = 0,\n        vel_variance: float = 1,\n    ):\n        self.R = R\n        self.Q = Q\n\n        # entrances P matrix of KF\n        self.pos_variance = pos_variance\n        self.pos_vel_covariance = pos_vel_covariance\n        self.vel_variance = vel_variance\n\n    def create_filter(self, initial_detection: np.ndarray):\n        num_points = initial_detection.shape[0]\n        dim_points = initial_detection.shape[1]\n        dim_z = dim_points * num_points  # flattened positions\n        dim_x = 2 * dim_z  # We need to accommodate for velocities\n\n        custom_filter = OptimizedKalmanFilter(\n            dim_x,\n            dim_z,\n            pos_variance=self.pos_variance,\n            pos_vel_covariance=self.pos_vel_covariance,\n            vel_variance=self.vel_variance,\n            q=self.Q,\n            r=self.R,\n        )\n        custom_filter.x[:dim_z] = np.expand_dims(initial_detection.flatten(), 0).T\n\n        return custom_filter\n</code></pre>"},{"location":"reference/metrics/","title":"Metrics","text":""},{"location":"reference/metrics/#norfair.metrics.PredictionsTextFile","title":"<code>PredictionsTextFile</code>","text":"<p>Generates a text file with your predicted tracked objects, in the MOTChallenge format. It needs the 'input_path', which is the path to the sequence being processed, the 'save_path', and optionally the 'information_file' (in case you don't give an 'information_file', is assumed there is one in the input_path folder).</p> Source code in <code>norfair/metrics.py</code> <pre><code>class PredictionsTextFile:\n    \"\"\"Generates a text file with your predicted tracked objects, in the MOTChallenge format.\n    It needs the 'input_path', which is the path to the sequence being processed,\n    the 'save_path', and optionally the 'information_file' (in case you don't give an\n    'information_file', is assumed there is one in the input_path folder).\n    \"\"\"\n\n    def __init__(self, input_path, save_path=\".\", information_file=None):\n\n        file_name = os.path.split(input_path)[1]\n\n        if information_file is None:\n            seqinfo_path = os.path.join(input_path, \"seqinfo.ini\")\n            information_file = InformationFile(file_path=seqinfo_path)\n\n        self.length = information_file.search(variable_name=\"seqLength\")\n\n        predictions_folder = os.path.join(save_path, \"predictions\")\n        if not os.path.exists(predictions_folder):\n            os.makedirs(predictions_folder)\n\n        out_file_name = os.path.join(predictions_folder, file_name + \".txt\")\n        self.text_file = open(out_file_name, \"w+\")\n\n        self.frame_number = 1\n\n    def update(self, predictions, frame_number=None):\n        if frame_number is None:\n            frame_number = self.frame_number\n        \"\"\"\n        Write tracked object information in the output file (for this frame), in the format\n        frame_number, id, bb_left, bb_top, bb_width, bb_height, -1, -1, -1, -1\n        \"\"\"\n        for obj in predictions:\n            frame_str = str(int(frame_number))\n            id_str = str(int(obj.id))\n            bb_left_str = str((obj.estimate[0, 0]))\n            bb_top_str = str((obj.estimate[0, 1]))  # [0,1]\n            bb_width_str = str((obj.estimate[1, 0] - obj.estimate[0, 0]))\n            bb_height_str = str((obj.estimate[1, 1] - obj.estimate[0, 1]))\n            row_text_out = (\n                frame_str\n                + \",\"\n                + id_str\n                + \",\"\n                + bb_left_str\n                + \",\"\n                + bb_top_str\n                + \",\"\n                + bb_width_str\n                + \",\"\n                + bb_height_str\n                + \",-1,-1,-1,-1\"\n            )\n            self.text_file.write(row_text_out)\n            self.text_file.write(\"\\n\")\n\n        self.frame_number += 1\n\n        if self.frame_number &gt; self.length:\n            self.text_file.close()\n</code></pre>"},{"location":"reference/metrics/#norfair.metrics.DetectionFileParser","title":"<code>DetectionFileParser</code>","text":"<p>Get Norfair detections from MOTChallenge text files containing detections</p> Source code in <code>norfair/metrics.py</code> <pre><code>class DetectionFileParser:\n    \"\"\"Get Norfair detections from MOTChallenge text files containing detections\"\"\"\n\n    def __init__(self, input_path, information_file=None):\n        self.frame_number = 1\n\n        # Get detecions matrix data with rows corresponding to:\n        # frame, id, bb_left, bb_top, bb_right, bb_down, conf, x, y, z\n        detections_path = os.path.join(input_path, \"det/det.txt\")\n\n        self.matrix_detections = np.loadtxt(detections_path, dtype=\"f\", delimiter=\",\")\n        row_order = np.argsort(self.matrix_detections[:, 0])\n        self.matrix_detections = self.matrix_detections[row_order]\n        # Coordinates refer to box corners\n        self.matrix_detections[:, 4] = (\n            self.matrix_detections[:, 2] + self.matrix_detections[:, 4]\n        )\n        self.matrix_detections[:, 5] = (\n            self.matrix_detections[:, 3] + self.matrix_detections[:, 5]\n        )\n\n        if information_file is None:\n            seqinfo_path = os.path.join(input_path, \"seqinfo.ini\")\n            information_file = InformationFile(file_path=seqinfo_path)\n        self.length = information_file.search(variable_name=\"seqLength\")\n\n        self.sorted_by_frame = []\n        for frame_number in range(1, self.length + 1):\n            self.sorted_by_frame.append(self.get_dets_from_frame(frame_number))\n\n    def get_dets_from_frame(self, frame_number):\n        \"\"\"this function returns a list of norfair Detections class, corresponding to frame=frame_number\"\"\"\n\n        indexes = np.argwhere(self.matrix_detections[:, 0] == frame_number)\n        detections = []\n        if len(indexes) &gt; 0:\n            actual_det = self.matrix_detections[indexes]\n            actual_det.shape = [actual_det.shape[0], actual_det.shape[2]]\n            for det in actual_det:\n                points = np.array([[det[2], det[3]], [det[4], det[5]]])\n                conf = det[6]\n                new_detection = Detection(points, np.array([conf, conf]))\n                detections.append(new_detection)\n        self.actual_detections = detections\n        return detections\n\n    def __iter__(self):\n        self.frame_number = 1\n        return self\n\n    def __next__(self):\n        if self.frame_number &lt;= self.length:\n            self.frame_number += 1\n            # Frame_number is always 1 unit bigger than the corresponding index in self.sorted_by_frame, and\n            # also we just incremented the frame_number, so now is 2 units bigger than the corresponding index\n            return self.sorted_by_frame[self.frame_number - 2]\n\n        raise StopIteration()\n</code></pre>"},{"location":"reference/metrics/#norfair.metrics.DetectionFileParser.get_dets_from_frame","title":"<code>get_dets_from_frame(frame_number)</code>","text":"<p>this function returns a list of norfair Detections class, corresponding to frame=frame_number</p> Source code in <code>norfair/metrics.py</code> <pre><code>def get_dets_from_frame(self, frame_number):\n    \"\"\"this function returns a list of norfair Detections class, corresponding to frame=frame_number\"\"\"\n\n    indexes = np.argwhere(self.matrix_detections[:, 0] == frame_number)\n    detections = []\n    if len(indexes) &gt; 0:\n        actual_det = self.matrix_detections[indexes]\n        actual_det.shape = [actual_det.shape[0], actual_det.shape[2]]\n        for det in actual_det:\n            points = np.array([[det[2], det[3]], [det[4], det[5]]])\n            conf = det[6]\n            new_detection = Detection(points, np.array([conf, conf]))\n            detections.append(new_detection)\n    self.actual_detections = detections\n    return detections\n</code></pre>"},{"location":"reference/metrics/#norfair.metrics.load_motchallenge","title":"<code>load_motchallenge(matrix_data, min_confidence=-1)</code>","text":"<p>Load MOT challenge data.</p> <p>This is a modification of the function load_motchallenge from the py-motmetrics library, defined in io.py In this version, the pandas dataframe is generated from a numpy array (matrix_data) instead of a text file.</p> Params <p>matrix_data : array  of float that has [frame, id, X, Y, width, height, conf, cassId, visibility] in each row, for each prediction on a particular video</p> <p>min_confidence : float     Rows with confidence less than this threshold are removed.     Defaults to -1. You should set this to 1 when loading     ground truth MOTChallenge data, so that invalid rectangles in     the ground truth are not considered during matching.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The returned dataframe has the following columns     'X', 'Y', 'Width', 'Height', 'Confidence', 'ClassId', 'Visibility' The dataframe is indexed by ('FrameId', 'Id')</p> Source code in <code>norfair/metrics.py</code> <pre><code>def load_motchallenge(matrix_data, min_confidence=-1):\n    \"\"\"Load MOT challenge data.\n\n    This is a modification of the function load_motchallenge from the py-motmetrics library, defined in io.py\n    In this version, the pandas dataframe is generated from a numpy array (matrix_data) instead of a text file.\n\n    Params\n    ------\n    matrix_data : array  of float that has [frame, id, X, Y, width, height, conf, cassId, visibility] in each row, for each prediction on a particular video\n\n    min_confidence : float\n        Rows with confidence less than this threshold are removed.\n        Defaults to -1. You should set this to 1 when loading\n        ground truth MOTChallenge data, so that invalid rectangles in\n        the ground truth are not considered during matching.\n\n    Returns\n    ------\n    df : pandas.DataFrame\n        The returned dataframe has the following columns\n            'X', 'Y', 'Width', 'Height', 'Confidence', 'ClassId', 'Visibility'\n        The dataframe is indexed by ('FrameId', 'Id')\n    \"\"\"\n\n    df = pd.DataFrame(\n        data=matrix_data,\n        columns=[\n            \"FrameId\",\n            \"Id\",\n            \"X\",\n            \"Y\",\n            \"Width\",\n            \"Height\",\n            \"Confidence\",\n            \"ClassId\",\n            \"Visibility\",\n            \"unused\",\n        ],\n    )\n    df = df.set_index([\"FrameId\", \"Id\"])\n    # Account for matlab convention.\n    df[[\"X\", \"Y\"]] -= (1, 1)\n\n    # Removed trailing column\n    del df[\"unused\"]\n\n    # Remove all rows without sufficient confidence\n    return df[df[\"Confidence\"] &gt;= min_confidence]\n</code></pre>"},{"location":"reference/metrics/#norfair.metrics.compare_dataframes","title":"<code>compare_dataframes(gts, ts)</code>","text":"<p>Builds accumulator for each sequence.</p> Source code in <code>norfair/metrics.py</code> <pre><code>def compare_dataframes(gts, ts):\n    \"\"\"Builds accumulator for each sequence.\"\"\"\n    accs = []\n    names = []\n    for k, tsacc in ts.items():\n        print(\"Comparing \", k, \"...\")\n        if k in gts:\n            accs.append(\n                mm.utils.compare_to_groundtruth(gts[k], tsacc, \"iou\", distth=0.5)\n            )\n            names.append(k)\n\n    return accs, names\n</code></pre>"},{"location":"reference/tracker/","title":"Tracker","text":""},{"location":"reference/tracker/#norfair.tracker.Tracker","title":"<code>Tracker</code>","text":"<p>The class in charge of performing the tracking of the detections produced by a detector.</p> <p>Parameters:</p> Name Type Description Default <code>distance_function</code> <code>Union[str, Callable[[Detection, TrackedObject], float]]</code> <p>Function used by the tracker to determine the distance between newly detected objects and the objects that are currently being tracked. This function should take 2 input arguments, the first being a Detection, and the second a TrackedObject. It has to return a <code>float</code> with the distance it calculates. Some common distances are implemented in distances, as a shortcut the tracker accepts the name of these predefined distances. Scipy's predefined distances are also accepted. A <code>str</code> with one of the available metrics in <code>scipy.spatial.distance.cdist</code>.</p> required <code>distance_threshold</code> <code>float</code> <p>Defines what is the maximum distance that can constitute a match. Detections and tracked objects whose distances are above this threshold won't be matched by the tracker.</p> required <code>hit_counter_max</code> <code>int</code> <p>Each tracked objects keeps an internal hit counter which tracks how often it's getting matched to a detection, each time it gets a match this counter goes up, and each time it doesn't it goes down.</p> <p>If it goes below 0 the object gets destroyed. This argument defines how large this inertia can grow, and therefore defines how long an object can live without getting matched to any detections, before it is displaced as a dead object, if no ReID distance function is implemented it will be destroyed.</p> <code>15</code> <code>initialization_delay</code> <code>Optional[int]</code> <p>Determines how large the object's hit counter must be in order to be considered as initialized, and get returned to the user as a real object.  It must be smaller than <code>hit_counter_max</code> or otherwise the object would never be initialized.</p> <p>If set to 0, objects will get returned to the user as soon as they are detected for the first time,  which can be problematic as this can result in objects appearing and immediately dissapearing.</p> <p>Defaults to <code>hit_counter_max / 2</code></p> <code>None</code> <code>pointwise_hit_counter_max</code> <code>int</code> <p>Each tracked object keeps track of how often the points it's tracking have been getting matched. Points that are getting matched (<code>pointwise_hit_counter &gt; 0</code>) are said to be live, and points which aren't (<code>pointwise_hit_counter = 0</code>) are said to not be live.</p> <p>This is used to determine things like which individual points in a tracked object get drawn by <code>draw_tracked_objects</code> and which don't. This argument defines how large the inertia for each point of a tracker can grow.</p> <code>4</code> <code>detection_threshold</code> <code>float</code> <p>Sets the threshold at which the scores of the points in a detection being fed into the tracker must dip below to be ignored by the tracker.</p> <code>0</code> <code>filter_factory</code> <code>FilterFactory</code> <p>This parameter can be used to change what filter the <code>TrackedObject</code> instances created by the tracker will use. Defaults to <code>OptimizedKalmanFilterFactory()</code></p> <code>OptimizedKalmanFilterFactory()</code> <code>past_detections_length</code> <code>int</code> <p>How many past detections to save for each tracked object. Norfair tries to distribute these past detections uniformly through the object's lifetime so they're more representative. Very useful if you want to add metric learning to your model, as you can associate an embedding to each detection and access them in your distance function.</p> <code>4</code> <code>reid_distance_function</code> <code>Optional[Callable[[TrackedObject, TrackedObject], float]]</code> <p>Function used by the tracker to determine the ReID distance between newly detected trackers and unmatched trackers by the distance function.</p> <p>This function should take 2 input arguments, the first being tracked objects in the initialization phase of type <code>TrackedObject</code>, and the second being tracked objects that have been unmatched of type <code>TrackedObject</code>. It returns a <code>float</code> with the distance it calculates.</p> <code>None</code> <code>reid_distance_threshold</code> <code>float</code> <p>Defines what is the maximum ReID distance that can constitute a match.</p> <p>Tracked objects whose distance is above this threshold won't be merged, if they are the oldest tracked object will be maintained with the position of the new tracked object.</p> <code>0</code> <code>reid_hit_counter_max</code> <code>Optional[int]</code> <p>Each tracked object keeps an internal ReID hit counter which tracks how often it's getting recognized by another tracker, each time it gets a match this counter goes up, and each time it doesn't it goes down. If it goes below 0 the object gets destroyed. If used, this argument (<code>reid_hit_counter_max</code>) defines how long an object can live without getting matched to any detections, before it is destroyed.</p> <code>None</code> Source code in <code>norfair/tracker.py</code> <pre><code>class Tracker:\n    \"\"\"\n    The class in charge of performing the tracking of the detections produced by a detector.\n\n    Parameters\n    ----------\n    distance_function : Union[str, Callable[[Detection, TrackedObject], float]]\n        Function used by the tracker to determine the distance between newly detected objects and the objects that are currently being tracked.\n        This function should take 2 input arguments, the first being a [Detection][norfair.tracker.Detection], and the second a [TrackedObject][norfair.tracker.TrackedObject].\n        It has to return a `float` with the distance it calculates.\n        Some common distances are implemented in [distances][], as a shortcut the tracker accepts the name of these [predefined distances][norfair.distances.get_distance_by_name].\n        Scipy's predefined distances are also accepted. A `str` with one of the available metrics in\n        [`scipy.spatial.distance.cdist`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html).\n    distance_threshold : float\n        Defines what is the maximum distance that can constitute a match.\n        Detections and tracked objects whose distances are above this threshold won't be matched by the tracker.\n    hit_counter_max : int, optional\n        Each tracked objects keeps an internal hit counter which tracks how often it's getting matched to a detection,\n        each time it gets a match this counter goes up, and each time it doesn't it goes down.\n\n        If it goes below 0 the object gets destroyed. This argument defines how large this inertia can grow,\n        and therefore defines how long an object can live without getting matched to any detections, before it is displaced as a dead object, if no ReID distance function is implemented it will be destroyed.\n    initialization_delay : Optional[int], optional\n         Determines how large the object's hit counter must be in order to be considered as initialized, and get returned to the user as a real object.\n         It must be smaller than `hit_counter_max` or otherwise the object would never be initialized.\n\n         If set to 0, objects will get returned to the user as soon as they are detected for the first time,\n         which can be problematic as this can result in objects appearing and immediately dissapearing.\n\n         Defaults to `hit_counter_max / 2`\n    pointwise_hit_counter_max : int, optional\n        Each tracked object keeps track of how often the points it's tracking have been getting matched.\n        Points that are getting matched (`pointwise_hit_counter &gt; 0`) are said to be live, and points which aren't (`pointwise_hit_counter = 0`)\n        are said to not be live.\n\n        This is used to determine things like which individual points in a tracked object get drawn by [`draw_tracked_objects`][norfair.drawing.draw_tracked_objects] and which don't.\n        This argument defines how large the inertia for each point of a tracker can grow.\n    detection_threshold : float, optional\n        Sets the threshold at which the scores of the points in a detection being fed into the tracker must dip below to be ignored by the tracker.\n    filter_factory : FilterFactory, optional\n        This parameter can be used to change what filter the [`TrackedObject`][norfair.tracker.TrackedObject] instances created by the tracker will use.\n        Defaults to [`OptimizedKalmanFilterFactory()`][norfair.filter.OptimizedKalmanFilterFactory]\n    past_detections_length : int, optional\n        How many past detections to save for each tracked object.\n        Norfair tries to distribute these past detections uniformly through the object's lifetime so they're more representative.\n        Very useful if you want to add metric learning to your model, as you can associate an embedding to each detection and access them in your distance function.\n    reid_distance_function: Optional[Callable[[\"TrackedObject\", \"TrackedObject\"], float]]\n        Function used by the tracker to determine the ReID distance between newly detected trackers and unmatched trackers by the distance function.\n\n        This function should take 2 input arguments, the first being tracked objects in the initialization phase of type [`TrackedObject`][norfair.tracker.TrackedObject],\n        and the second being tracked objects that have been unmatched of type [`TrackedObject`][norfair.tracker.TrackedObject]. It returns a `float` with the distance it\n        calculates.\n    reid_distance_threshold: float\n        Defines what is the maximum ReID distance that can constitute a match.\n\n        Tracked objects whose distance is above this threshold won't be merged, if they are the oldest tracked object will be maintained\n        with the position of the new tracked object.\n    reid_hit_counter_max: Optional[int]\n        Each tracked object keeps an internal ReID hit counter which tracks how often it's getting recognized by another tracker,\n        each time it gets a match this counter goes up, and each time it doesn't it goes down. If it goes below 0 the object gets destroyed.\n        If used, this argument (`reid_hit_counter_max`) defines how long an object can live without getting matched to any detections, before it is destroyed.\n    \"\"\"\n\n    def __init__(\n        self,\n        distance_function: Union[str, Callable[[\"Detection\", \"TrackedObject\"], float]],\n        distance_threshold: float,\n        hit_counter_max: int = 15,\n        initialization_delay: Optional[int] = None,\n        pointwise_hit_counter_max: int = 4,\n        detection_threshold: float = 0,\n        filter_factory: FilterFactory = OptimizedKalmanFilterFactory(),\n        past_detections_length: int = 4,\n        reid_distance_function: Optional[\n            Callable[[\"TrackedObject\", \"TrackedObject\"], float]\n        ] = None,\n        reid_distance_threshold: float = 0,\n        reid_hit_counter_max: Optional[int] = None,\n    ):\n        self.tracked_objects: Sequence[\"TrackedObject\"] = []\n\n        if isinstance(distance_function, str):\n            distance_function = get_distance_by_name(distance_function)\n        elif isinstance(distance_function, Callable):\n            warning(\n                \"You are using a scalar distance function. If you want to speed up the\"\n                \" tracking process please consider using a vectorized distance\"\n                f\" function such as {AVAILABLE_VECTORIZED_DISTANCES}.\"\n            )\n            distance_function = ScalarDistance(distance_function)\n        else:\n            raise ValueError(\n                \"Argument `distance_function` should be a string or function but is\"\n                f\" {type(distance_function)} instead.\"\n            )\n        self.distance_function = distance_function\n\n        self.hit_counter_max = hit_counter_max\n        self.reid_hit_counter_max = reid_hit_counter_max\n        self.pointwise_hit_counter_max = pointwise_hit_counter_max\n        self.filter_factory = filter_factory\n        if past_detections_length &gt;= 0:\n            self.past_detections_length = past_detections_length\n        else:\n            raise ValueError(\n                f\"Argument `past_detections_length` is {past_detections_length} and should be larger than 0.\"\n            )\n\n        if initialization_delay is None:\n            self.initialization_delay = int(self.hit_counter_max / 2)\n        elif initialization_delay &lt; 0 or initialization_delay &gt;= self.hit_counter_max:\n            raise ValueError(\n                f\"Argument 'initialization_delay' for 'Tracker' class should be an int between 0 and (hit_counter_max = {hit_counter_max}). The selected value is {initialization_delay}.\\n\"\n            )\n        else:\n            self.initialization_delay = initialization_delay\n\n        self.distance_threshold = distance_threshold\n        self.detection_threshold = detection_threshold\n        if reid_distance_function is not None:\n            self.reid_distance_function = ScalarDistance(reid_distance_function)\n        else:\n            self.reid_distance_function = reid_distance_function\n        self.reid_distance_threshold = reid_distance_threshold\n        self._obj_factory = _TrackedObjectFactory()\n\n    def update(\n        self,\n        detections: Optional[List[\"Detection\"]] = None,\n        period: int = 1,\n        coord_transformations: Optional[CoordinatesTransformation] = None,\n    ) -&gt; List[\"TrackedObject\"]:\n        \"\"\"\n        Process detections found in each frame.\n\n        The detections can be matched to previous tracked objects or new ones will be created\n        according to the configuration of the Tracker.\n        The currently alive and initialized tracked objects are returned\n\n        Parameters\n        ----------\n        detections : Optional[List[Detection]], optional\n            A list of [`Detection`][norfair.tracker.Detection] which represent the detections found in the current frame being processed.\n\n            If no detections have been found in the current frame, or the user is purposely skipping frames to improve video processing time,\n            this argument should be set to None or ignored, as the update function is needed to advance the state of the Kalman Filters inside the tracker.\n        period : int, optional\n            The user can chose not to run their detector on all frames, so as to process video faster.\n            This parameter sets every how many frames the detector is getting ran,\n            so that the tracker is aware of this situation and can handle it properly.\n\n            This argument can be reset on each frame processed,\n            which is useful if the user is dynamically changing how many frames the detector is skipping on a video when working in real-time.\n        coord_transformations: Optional[CoordinatesTransformation]\n            The coordinate transformation calculated by the [MotionEstimator][norfair.camera_motion.MotionEstimator].\n\n        Returns\n        -------\n        List[TrackedObject]\n            The list of active tracked objects.\n        \"\"\"\n        if coord_transformations is not None:\n            for det in detections:\n                det.update_coordinate_transformation(coord_transformations)\n\n        # Remove stale trackers and make candidate object real if the hit counter is positive\n        alive_objects = []\n        dead_objects = []\n        if self.reid_hit_counter_max is None:\n            self.tracked_objects = [\n                o for o in self.tracked_objects if o.hit_counter_is_positive\n            ]\n            alive_objects = self.tracked_objects\n        else:\n            tracked_objects = []\n            for o in self.tracked_objects:\n                if o.reid_hit_counter_is_positive:\n                    tracked_objects.append(o)\n                    if o.hit_counter_is_positive:\n                        alive_objects.append(o)\n                    else:\n                        dead_objects.append(o)\n            self.tracked_objects = tracked_objects\n\n        # Update tracker\n        for obj in self.tracked_objects:\n            obj.tracker_step()\n            obj.update_coordinate_transformation(coord_transformations)\n\n        # Update initialized tracked objects with detections\n        (\n            unmatched_detections,\n            _,\n            unmatched_init_trackers,\n        ) = self._update_objects_in_place(\n            self.distance_function,\n            self.distance_threshold,\n            [o for o in alive_objects if not o.is_initializing],\n            detections,\n            period,\n        )\n\n        # Update not yet initialized tracked objects with yet unmatched detections\n        (\n            unmatched_detections,\n            matched_not_init_trackers,\n            _,\n        ) = self._update_objects_in_place(\n            self.distance_function,\n            self.distance_threshold,\n            [o for o in alive_objects if o.is_initializing],\n            unmatched_detections,\n            period,\n        )\n\n        if self.reid_distance_function is not None:\n            # Match unmatched initialized tracked objects with not yet initialized tracked objects\n            _, _, _ = self._update_objects_in_place(\n                self.reid_distance_function,\n                self.reid_distance_threshold,\n                unmatched_init_trackers + dead_objects,\n                matched_not_init_trackers,\n                period,\n            )\n\n        # Create new tracked objects from remaining unmatched detections\n        for detection in unmatched_detections:\n            self.tracked_objects.append(\n                self._obj_factory.create(\n                    initial_detection=detection,\n                    hit_counter_max=self.hit_counter_max,\n                    initialization_delay=self.initialization_delay,\n                    pointwise_hit_counter_max=self.pointwise_hit_counter_max,\n                    detection_threshold=self.detection_threshold,\n                    period=period,\n                    filter_factory=self.filter_factory,\n                    past_detections_length=self.past_detections_length,\n                    reid_hit_counter_max=self.reid_hit_counter_max,\n                    coord_transformations=coord_transformations,\n                )\n            )\n\n        return self.get_active_objects()\n\n    @property\n    def current_object_count(self) -&gt; int:\n        \"\"\"Number of active TrackedObjects\"\"\"\n        return len(self.get_active_objects())\n\n    @property\n    def total_object_count(self) -&gt; int:\n        \"\"\"Total number of TrackedObjects initialized in the by this Tracker\"\"\"\n        return self._obj_factory.count\n\n    def get_active_objects(self) -&gt; List[\"TrackedObject\"]:\n        \"\"\"Get the list of active objects\n\n        Returns\n        -------\n        List[\"TrackedObject\"]\n            The list of active objects\n        \"\"\"\n        return [\n            o\n            for o in self.tracked_objects\n            if not o.is_initializing and o.hit_counter_is_positive\n        ]\n\n    def _update_objects_in_place(\n        self,\n        distance_function,\n        distance_threshold,\n        objects: Sequence[\"TrackedObject\"],\n        candidates: Optional[Union[List[\"Detection\"], List[\"TrackedObject\"]]],\n        period: int,\n    ):\n        if candidates is not None and len(candidates) &gt; 0:\n            distance_matrix = distance_function.get_distances(objects, candidates)\n            if np.isnan(distance_matrix).any():\n                raise ValueError(\n                    \"\\nReceived nan values from distance function, please check your distance function for errors!\"\n                )\n\n            # Used just for debugging distance function\n            if distance_matrix.any():\n                for i, minimum in enumerate(distance_matrix.min(axis=0)):\n                    objects[i].current_min_distance = (\n                        minimum if minimum &lt; distance_threshold else None\n                    )\n\n            matched_cand_indices, matched_obj_indices = self.match_dets_and_objs(\n                distance_matrix, distance_threshold\n            )\n            if len(matched_cand_indices) &gt; 0:\n                unmatched_candidates = [\n                    d for i, d in enumerate(candidates) if i not in matched_cand_indices\n                ]\n                unmatched_objects = [\n                    d for i, d in enumerate(objects) if i not in matched_obj_indices\n                ]\n                matched_objects = []\n\n                # Handle matched people/detections\n                for (match_cand_idx, match_obj_idx) in zip(\n                    matched_cand_indices, matched_obj_indices\n                ):\n                    match_distance = distance_matrix[match_cand_idx, match_obj_idx]\n                    matched_candidate = candidates[match_cand_idx]\n                    matched_object = objects[match_obj_idx]\n                    if match_distance &lt; distance_threshold:\n                        if isinstance(matched_candidate, Detection):\n                            matched_object.hit(matched_candidate, period=period)\n                            matched_object.last_distance = match_distance\n                            matched_objects.append(matched_object)\n                        elif isinstance(matched_candidate, TrackedObject):\n                            # Merge new TrackedObject with the old one\n                            matched_object.merge(matched_candidate)\n                            # If we are matching TrackedObject instances we want to get rid of the\n                            # already matched candidate to avoid matching it again in future frames\n                            self.tracked_objects.remove(matched_candidate)\n                    else:\n                        unmatched_candidates.append(matched_candidate)\n                        unmatched_objects.append(matched_object)\n            else:\n                unmatched_candidates, matched_objects, unmatched_objects = (\n                    candidates,\n                    [],\n                    objects,\n                )\n        else:\n            unmatched_candidates, matched_objects, unmatched_objects = [], [], objects\n\n        return unmatched_candidates, matched_objects, unmatched_objects\n\n    def match_dets_and_objs(self, distance_matrix: np.ndarray, distance_threshold):\n        \"\"\"Matches detections with tracked_objects from a distance matrix\n\n        I used to match by minimizing the global distances, but found several\n        cases in which this was not optimal. So now I just match by starting\n        with the global minimum distance and matching the det-obj corresponding\n        to that distance, then taking the second minimum, and so on until we\n        reach the distance_threshold.\n\n        This avoids the the algorithm getting cute with us and matching things\n        that shouldn't be matching just for the sake of minimizing the global\n        distance, which is what used to happen\n        \"\"\"\n        # NOTE: This implementation is terribly inefficient, but it doesn't\n        #       seem to affect the fps at all.\n        distance_matrix = distance_matrix.copy()\n        if distance_matrix.size &gt; 0:\n            det_idxs = []\n            obj_idxs = []\n            current_min = distance_matrix.min()\n\n            while current_min &lt; distance_threshold:\n                flattened_arg_min = distance_matrix.argmin()\n                det_idx = flattened_arg_min // distance_matrix.shape[1]\n                obj_idx = flattened_arg_min % distance_matrix.shape[1]\n                det_idxs.append(det_idx)\n                obj_idxs.append(obj_idx)\n                distance_matrix[det_idx, :] = distance_threshold + 1\n                distance_matrix[:, obj_idx] = distance_threshold + 1\n                current_min = distance_matrix.min()\n\n            return det_idxs, obj_idxs\n        else:\n            return [], []\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.Tracker.current_object_count","title":"<code>current_object_count: int</code>  <code>property</code>","text":"<p>Number of active TrackedObjects</p>"},{"location":"reference/tracker/#norfair.tracker.Tracker.total_object_count","title":"<code>total_object_count: int</code>  <code>property</code>","text":"<p>Total number of TrackedObjects initialized in the by this Tracker</p>"},{"location":"reference/tracker/#norfair.tracker.Tracker.update","title":"<code>update(detections=None, period=1, coord_transformations=None)</code>","text":"<p>Process detections found in each frame.</p> <p>The detections can be matched to previous tracked objects or new ones will be created according to the configuration of the Tracker. The currently alive and initialized tracked objects are returned</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Optional[List[Detection]]</code> <p>A list of <code>Detection</code> which represent the detections found in the current frame being processed.</p> <p>If no detections have been found in the current frame, or the user is purposely skipping frames to improve video processing time, this argument should be set to None or ignored, as the update function is needed to advance the state of the Kalman Filters inside the tracker.</p> <code>None</code> <code>period</code> <code>int</code> <p>The user can chose not to run their detector on all frames, so as to process video faster. This parameter sets every how many frames the detector is getting ran, so that the tracker is aware of this situation and can handle it properly.</p> <p>This argument can be reset on each frame processed, which is useful if the user is dynamically changing how many frames the detector is skipping on a video when working in real-time.</p> <code>1</code> <code>coord_transformations</code> <code>Optional[CoordinatesTransformation]</code> <p>The coordinate transformation calculated by the MotionEstimator.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[TrackedObject]</code> <p>The list of active tracked objects.</p> Source code in <code>norfair/tracker.py</code> <pre><code>def update(\n    self,\n    detections: Optional[List[\"Detection\"]] = None,\n    period: int = 1,\n    coord_transformations: Optional[CoordinatesTransformation] = None,\n) -&gt; List[\"TrackedObject\"]:\n    \"\"\"\n    Process detections found in each frame.\n\n    The detections can be matched to previous tracked objects or new ones will be created\n    according to the configuration of the Tracker.\n    The currently alive and initialized tracked objects are returned\n\n    Parameters\n    ----------\n    detections : Optional[List[Detection]], optional\n        A list of [`Detection`][norfair.tracker.Detection] which represent the detections found in the current frame being processed.\n\n        If no detections have been found in the current frame, or the user is purposely skipping frames to improve video processing time,\n        this argument should be set to None or ignored, as the update function is needed to advance the state of the Kalman Filters inside the tracker.\n    period : int, optional\n        The user can chose not to run their detector on all frames, so as to process video faster.\n        This parameter sets every how many frames the detector is getting ran,\n        so that the tracker is aware of this situation and can handle it properly.\n\n        This argument can be reset on each frame processed,\n        which is useful if the user is dynamically changing how many frames the detector is skipping on a video when working in real-time.\n    coord_transformations: Optional[CoordinatesTransformation]\n        The coordinate transformation calculated by the [MotionEstimator][norfair.camera_motion.MotionEstimator].\n\n    Returns\n    -------\n    List[TrackedObject]\n        The list of active tracked objects.\n    \"\"\"\n    if coord_transformations is not None:\n        for det in detections:\n            det.update_coordinate_transformation(coord_transformations)\n\n    # Remove stale trackers and make candidate object real if the hit counter is positive\n    alive_objects = []\n    dead_objects = []\n    if self.reid_hit_counter_max is None:\n        self.tracked_objects = [\n            o for o in self.tracked_objects if o.hit_counter_is_positive\n        ]\n        alive_objects = self.tracked_objects\n    else:\n        tracked_objects = []\n        for o in self.tracked_objects:\n            if o.reid_hit_counter_is_positive:\n                tracked_objects.append(o)\n                if o.hit_counter_is_positive:\n                    alive_objects.append(o)\n                else:\n                    dead_objects.append(o)\n        self.tracked_objects = tracked_objects\n\n    # Update tracker\n    for obj in self.tracked_objects:\n        obj.tracker_step()\n        obj.update_coordinate_transformation(coord_transformations)\n\n    # Update initialized tracked objects with detections\n    (\n        unmatched_detections,\n        _,\n        unmatched_init_trackers,\n    ) = self._update_objects_in_place(\n        self.distance_function,\n        self.distance_threshold,\n        [o for o in alive_objects if not o.is_initializing],\n        detections,\n        period,\n    )\n\n    # Update not yet initialized tracked objects with yet unmatched detections\n    (\n        unmatched_detections,\n        matched_not_init_trackers,\n        _,\n    ) = self._update_objects_in_place(\n        self.distance_function,\n        self.distance_threshold,\n        [o for o in alive_objects if o.is_initializing],\n        unmatched_detections,\n        period,\n    )\n\n    if self.reid_distance_function is not None:\n        # Match unmatched initialized tracked objects with not yet initialized tracked objects\n        _, _, _ = self._update_objects_in_place(\n            self.reid_distance_function,\n            self.reid_distance_threshold,\n            unmatched_init_trackers + dead_objects,\n            matched_not_init_trackers,\n            period,\n        )\n\n    # Create new tracked objects from remaining unmatched detections\n    for detection in unmatched_detections:\n        self.tracked_objects.append(\n            self._obj_factory.create(\n                initial_detection=detection,\n                hit_counter_max=self.hit_counter_max,\n                initialization_delay=self.initialization_delay,\n                pointwise_hit_counter_max=self.pointwise_hit_counter_max,\n                detection_threshold=self.detection_threshold,\n                period=period,\n                filter_factory=self.filter_factory,\n                past_detections_length=self.past_detections_length,\n                reid_hit_counter_max=self.reid_hit_counter_max,\n                coord_transformations=coord_transformations,\n            )\n        )\n\n    return self.get_active_objects()\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.Tracker.get_active_objects","title":"<code>get_active_objects()</code>","text":"<p>Get the list of active objects</p> <p>Returns:</p> Type Description <code>List[TrackedObject]</code> <p>The list of active objects</p> Source code in <code>norfair/tracker.py</code> <pre><code>def get_active_objects(self) -&gt; List[\"TrackedObject\"]:\n    \"\"\"Get the list of active objects\n\n    Returns\n    -------\n    List[\"TrackedObject\"]\n        The list of active objects\n    \"\"\"\n    return [\n        o\n        for o in self.tracked_objects\n        if not o.is_initializing and o.hit_counter_is_positive\n    ]\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.Tracker.match_dets_and_objs","title":"<code>match_dets_and_objs(distance_matrix, distance_threshold)</code>","text":"<p>Matches detections with tracked_objects from a distance matrix</p> <p>I used to match by minimizing the global distances, but found several cases in which this was not optimal. So now I just match by starting with the global minimum distance and matching the det-obj corresponding to that distance, then taking the second minimum, and so on until we reach the distance_threshold.</p> <p>This avoids the the algorithm getting cute with us and matching things that shouldn't be matching just for the sake of minimizing the global distance, which is what used to happen</p> Source code in <code>norfair/tracker.py</code> <pre><code>def match_dets_and_objs(self, distance_matrix: np.ndarray, distance_threshold):\n    \"\"\"Matches detections with tracked_objects from a distance matrix\n\n    I used to match by minimizing the global distances, but found several\n    cases in which this was not optimal. So now I just match by starting\n    with the global minimum distance and matching the det-obj corresponding\n    to that distance, then taking the second minimum, and so on until we\n    reach the distance_threshold.\n\n    This avoids the the algorithm getting cute with us and matching things\n    that shouldn't be matching just for the sake of minimizing the global\n    distance, which is what used to happen\n    \"\"\"\n    # NOTE: This implementation is terribly inefficient, but it doesn't\n    #       seem to affect the fps at all.\n    distance_matrix = distance_matrix.copy()\n    if distance_matrix.size &gt; 0:\n        det_idxs = []\n        obj_idxs = []\n        current_min = distance_matrix.min()\n\n        while current_min &lt; distance_threshold:\n            flattened_arg_min = distance_matrix.argmin()\n            det_idx = flattened_arg_min // distance_matrix.shape[1]\n            obj_idx = flattened_arg_min % distance_matrix.shape[1]\n            det_idxs.append(det_idx)\n            obj_idxs.append(obj_idx)\n            distance_matrix[det_idx, :] = distance_threshold + 1\n            distance_matrix[:, obj_idx] = distance_threshold + 1\n            current_min = distance_matrix.min()\n\n        return det_idxs, obj_idxs\n    else:\n        return [], []\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.TrackedObject","title":"<code>TrackedObject</code>","text":"<p>The objects returned by the tracker's <code>update</code> function on each iteration.</p> <p>They represent the objects currently being tracked by the tracker.</p> <p>Users should not instantiate TrackedObjects manually; the Tracker will be in charge of creating them.</p> <p>Attributes:</p> Name Type Description <code>estimate</code> <code>ndarray</code> <p>Where the tracker predicts the point will be in the current frame based on past detections. A numpy array with the same shape as the detections being fed to the tracker that produced it.</p> <code>id</code> <code>Optional[int]</code> <p>The unique identifier assigned to this object by the tracker. Set to <code>None</code> if the object is initializing.</p> <code>global_id</code> <code>Optional[int]</code> <p>The globally unique identifier assigned to this object. Set to <code>None</code> if the object is initializing</p> <code>last_detection</code> <code>Detection</code> <p>The last detection that matched with this tracked object. Useful if you are storing embeddings in your detections and want to do metric learning, or for debugging.</p> <code>last_distance</code> <code>Optional[float]</code> <p>The distance the tracker had with the last object it matched with.</p> <code>age</code> <code>int</code> <p>The age of this object measured in number of frames.</p> <code>live_points</code> <p>A boolean mask with shape <code>(n_points,)</code>. Points marked as <code>True</code> have recently been matched with detections. Points marked as <code>False</code> haven't and are to be considered stale, and should be ignored.</p> <p>Functions like <code>draw_tracked_objects</code> use this property to determine which points not to draw.</p> <code>initializing_id</code> <code>int</code> <p>On top of <code>id</code>, objects also have an <code>initializing_id</code> which is the id they are given internally by the <code>Tracker</code>; this id is used solely for debugging.</p> <p>Each new object created by the <code>Tracker</code> starts as an uninitialized <code>TrackedObject</code>, which needs to reach a certain match rate to be converted into a full blown <code>TrackedObject</code>. <code>initializing_id</code> is the id temporarily assigned to <code>TrackedObject</code> while they are getting initialized.</p> Source code in <code>norfair/tracker.py</code> <pre><code>class TrackedObject:\n    \"\"\"\n    The objects returned by the tracker's `update` function on each iteration.\n\n    They represent the objects currently being tracked by the tracker.\n\n    Users should not instantiate TrackedObjects manually;\n    the Tracker will be in charge of creating them.\n\n    Attributes\n    ----------\n    estimate : np.ndarray\n        Where the tracker predicts the point will be in the current frame based on past detections.\n        A numpy array with the same shape as the detections being fed to the tracker that produced it.\n    id : Optional[int]\n        The unique identifier assigned to this object by the tracker. Set to `None` if the object is initializing.\n    global_id : Optional[int]\n        The globally unique identifier assigned to this object. Set to `None` if the object is initializing\n    last_detection : Detection\n        The last detection that matched with this tracked object.\n        Useful if you are storing embeddings in your detections and want to do metric learning, or for debugging.\n    last_distance : Optional[float]\n        The distance the tracker had with the last object it matched with.\n    age : int\n        The age of this object measured in number of frames.\n    live_points :\n        A boolean mask with shape `(n_points,)`. Points marked as `True` have recently been matched with detections.\n        Points marked as `False` haven't and are to be considered stale, and should be ignored.\n\n        Functions like [`draw_tracked_objects`][norfair.drawing.draw_tracked_objects] use this property to determine which points not to draw.\n    initializing_id : int\n        On top of `id`, objects also have an `initializing_id` which is the id they are given internally by the `Tracker`;\n        this id is used solely for debugging.\n\n        Each new object created by the `Tracker` starts as an uninitialized `TrackedObject`,\n        which needs to reach a certain match rate to be converted into a full blown `TrackedObject`.\n        `initializing_id` is the id temporarily assigned to `TrackedObject` while they are getting initialized.\n    \"\"\"\n\n    def __init__(\n        self,\n        obj_factory: _TrackedObjectFactory,\n        initial_detection: \"Detection\",\n        hit_counter_max: int,\n        initialization_delay: int,\n        pointwise_hit_counter_max: int,\n        detection_threshold: float,\n        period: int,\n        filter_factory: \"FilterFactory\",\n        past_detections_length: int,\n        reid_hit_counter_max: Optional[int],\n        coord_transformations: Optional[CoordinatesTransformation] = None,\n    ):\n        if not isinstance(initial_detection, Detection):\n            raise ValueError(\n                f\"\\n[red]ERROR[/red]: The detection list fed into `tracker.update()` should be composed of {Detection} objects not {type(initial_detection)}.\\n\"\n            )\n        self._obj_factory = obj_factory\n        self.dim_points = initial_detection.absolute_points.shape[1]\n        self.num_points = initial_detection.absolute_points.shape[0]\n        self.hit_counter_max: int = hit_counter_max\n        self.pointwise_hit_counter_max: int = max(pointwise_hit_counter_max, period)\n        self.initialization_delay = initialization_delay\n        self.detection_threshold: float = detection_threshold\n        self.initial_period: int = period\n        self.hit_counter: int = period\n        self.reid_hit_counter_max = reid_hit_counter_max\n        self.reid_hit_counter: Optional[int] = None\n        self.last_distance: Optional[float] = None\n        self.current_min_distance: Optional[float] = None\n        self.last_detection: \"Detection\" = initial_detection\n        self.age: int = 0\n        self.is_initializing: bool = self.hit_counter &lt;= self.initialization_delay\n\n        self.initializing_id: Optional[int] = self._obj_factory.get_initializing_id()\n        self.id: Optional[int] = None\n        self.global_id: Optional[int] = None\n        if not self.is_initializing:\n            self._acquire_ids()\n\n        if initial_detection.scores is None:\n            self.detected_at_least_once_points = np.array([True] * self.num_points)\n        else:\n            self.detected_at_least_once_points = (\n                initial_detection.scores &gt; self.detection_threshold\n            )\n        self.point_hit_counter: np.ndarray = self.detected_at_least_once_points.astype(\n            int\n        )\n        initial_detection.age = self.age\n        self.past_detections_length = past_detections_length\n        if past_detections_length &gt; 0:\n            self.past_detections: Sequence[\"Detection\"] = [initial_detection]\n        else:\n            self.past_detections: Sequence[\"Detection\"] = []\n\n        # Create Kalman Filter\n        self.filter = filter_factory.create_filter(initial_detection.absolute_points)\n        self.dim_z = self.dim_points * self.num_points\n        self.label = initial_detection.label\n        self.abs_to_rel = None\n        if coord_transformations is not None:\n            self.update_coordinate_transformation(coord_transformations)\n\n    def tracker_step(self):\n        if self.reid_hit_counter is None:\n            if self.hit_counter &lt;= 0:\n                self.reid_hit_counter = self.reid_hit_counter_max\n        else:\n            self.reid_hit_counter -= 1\n        self.hit_counter -= 1\n        self.point_hit_counter -= 1\n        self.age += 1\n        # Advances the tracker's state\n        self.filter.predict()\n\n    @property\n    def hit_counter_is_positive(self):\n        return self.hit_counter &gt;= 0\n\n    @property\n    def reid_hit_counter_is_positive(self):\n        return self.reid_hit_counter is None or self.reid_hit_counter &gt;= 0\n\n    @property\n    def estimate_velocity(self) -&gt; np.ndarray:\n        \"\"\"Get the velocity estimate of the object from the Kalman filter. This velocity is in the absolute coordinate system.\n\n        Returns\n        -------\n        np.ndarray\n            An array of shape (self.num_points, self.dim_points) containing the velocity estimate of the object on each axis.\n        \"\"\"\n        return self.filter.x.T.flatten()[self.dim_z :].reshape(-1, self.dim_points)\n\n    @property\n    def estimate(self) -&gt; np.ndarray:\n        \"\"\"Get the position estimate of the object from the Kalman filter.\n\n        Returns\n        -------\n        np.ndarray\n            An array of shape (self.num_points, self.dim_points) containing the position estimate of the object on each axis.\n        \"\"\"\n        return self.get_estimate()\n\n    def get_estimate(self, absolute=False) -&gt; np.ndarray:\n        \"\"\"Get the position estimate of the object from the Kalman filter in an absolute or relative format.\n\n        Parameters\n        ----------\n        absolute : bool, optional\n            If true the coordinates are returned in absolute format, by default False, by default False.\n\n        Returns\n        -------\n        np.ndarray\n            An array of shape (self.num_points, self.dim_points) containing the position estimate of the object on each axis.\n\n        Raises\n        ------\n        ValueError\n            Alert if the coordinates are requested in absolute format but the tracker has no coordinate transformation.\n        \"\"\"\n        positions = self.filter.x.T.flatten()[: self.dim_z].reshape(-1, self.dim_points)\n        if self.abs_to_rel is None:\n            if not absolute:\n                return positions\n            else:\n                raise ValueError(\n                    \"You must provide 'coord_transformations' to the tracker to get absolute coordinates\"\n                )\n        else:\n            if absolute:\n                return positions\n            else:\n                return self.abs_to_rel(positions)\n\n    @property\n    def live_points(self):\n        return self.point_hit_counter &gt; 0\n\n    def hit(self, detection: \"Detection\", period: int = 1):\n        \"\"\"Update tracked object with a new detection\n\n        Parameters\n        ----------\n        detection : Detection\n            the new detection matched to this tracked object\n        period : int, optional\n            frames corresponding to the period of time since last update.\n        \"\"\"\n        self._conditionally_add_to_past_detections(detection)\n\n        self.last_detection = detection\n        self.hit_counter = min(self.hit_counter + 2 * period, self.hit_counter_max)\n\n        if self.is_initializing and self.hit_counter &gt; self.initialization_delay:\n            self.is_initializing = False\n            self._acquire_ids()\n\n        # We use a kalman filter in which we consider each coordinate on each point as a sensor.\n        # This is a hacky way to update only certain sensors (only x, y coordinates for\n        # points which were detected).\n        # TODO: Use keypoint confidence information to change R on each sensor instead?\n        if detection.scores is not None:\n            assert len(detection.scores.shape) == 1\n            points_over_threshold_mask = detection.scores &gt; self.detection_threshold\n            matched_sensors_mask = np.array(\n                [(m,) * self.dim_points for m in points_over_threshold_mask]\n            ).flatten()\n            H_pos = np.diag(matched_sensors_mask).astype(\n                float\n            )  # We measure x, y positions\n            self.point_hit_counter[points_over_threshold_mask] += 2 * period\n        else:\n            points_over_threshold_mask = np.array([True] * self.num_points)\n            H_pos = np.identity(self.num_points * self.dim_points)\n            self.point_hit_counter += 2 * period\n        self.point_hit_counter[\n            self.point_hit_counter &gt;= self.pointwise_hit_counter_max\n        ] = self.pointwise_hit_counter_max\n        self.point_hit_counter[self.point_hit_counter &lt; 0] = 0\n        H_vel = np.zeros(H_pos.shape)  # But we don't directly measure velocity\n        H = np.hstack([H_pos, H_vel])\n        self.filter.update(\n            np.expand_dims(detection.absolute_points.flatten(), 0).T, None, H\n        )\n\n        detected_at_least_once_mask = np.array(\n            [(m,) * self.dim_points for m in self.detected_at_least_once_points]\n        ).flatten()\n        now_detected_mask = np.hstack(\n            (points_over_threshold_mask,) * self.dim_points\n        ).flatten()\n        first_detection_mask = np.logical_and(\n            now_detected_mask, np.logical_not(detected_at_least_once_mask)\n        )\n\n        self.filter.x[: self.dim_z][first_detection_mask] = np.expand_dims(\n            detection.absolute_points.flatten(), 0\n        ).T[first_detection_mask]\n\n        # Force points being detected for the first time to have velocity = 0\n        # This is needed because some detectors (like OpenPose) set points with\n        # low confidence to coordinates (0, 0). And when they then get their first\n        # real detection this creates a huge velocity vector in our KalmanFilter\n        # and causes the tracker to start with wildly inaccurate estimations which\n        # eventually coverge to the real detections.\n        self.filter.x[self.dim_z :][np.logical_not(detected_at_least_once_mask)] = 0\n        self.detected_at_least_once_points = np.logical_or(\n            self.detected_at_least_once_points, points_over_threshold_mask\n        )\n\n    def __repr__(self):\n        if self.last_distance is None:\n            placeholder_text = \"\\033[1mObject_{}\\033[0m(age: {}, hit_counter: {}, last_distance: {}, init_id: {})\"\n        else:\n            placeholder_text = \"\\033[1mObject_{}\\033[0m(age: {}, hit_counter: {}, last_distance: {:.2f}, init_id: {})\"\n        return placeholder_text.format(\n            self.id,\n            self.age,\n            self.hit_counter,\n            self.last_distance,\n            self.initializing_id,\n        )\n\n    def _conditionally_add_to_past_detections(self, detection):\n        \"\"\"Adds detections into (and pops detections away) from `past_detections`\n\n        It does so by keeping a fixed amount of past detections saved into each\n        TrackedObject, while maintaining them distributed uniformly through the object's\n        lifetime.\n        \"\"\"\n        if self.past_detections_length == 0:\n            return\n        if len(self.past_detections) &lt; self.past_detections_length:\n            detection.age = self.age\n            self.past_detections.append(detection)\n        elif self.age &gt;= self.past_detections[0].age * self.past_detections_length:\n            self.past_detections.pop(0)\n            detection.age = self.age\n            self.past_detections.append(detection)\n\n    def merge(self, tracked_object):\n        \"\"\"Merge with a not yet initialized TrackedObject instance\"\"\"\n        self.reid_hit_counter = None\n        self.hit_counter = self.initial_period * 2\n        self.point_hit_counter = tracked_object.point_hit_counter\n        self.last_distance = tracked_object.last_distance\n        self.current_min_distance = tracked_object.current_min_distance\n        self.last_detection = tracked_object.last_detection\n        self.detected_at_least_once_points = (\n            tracked_object.detected_at_least_once_points\n        )\n        self.filter = tracked_object.filter\n\n        for past_detection in tracked_object.past_detections:\n            self._conditionally_add_to_past_detections(past_detection)\n\n    def update_coordinate_transformation(\n        self, coordinate_transformation: CoordinatesTransformation\n    ):\n        if coordinate_transformation is not None:\n            self.abs_to_rel = coordinate_transformation.abs_to_rel\n\n    def _acquire_ids(self):\n        self.id, self.global_id = self._obj_factory.get_ids()\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.TrackedObject.estimate_velocity","title":"<code>estimate_velocity: np.ndarray</code>  <code>property</code>","text":"<p>Get the velocity estimate of the object from the Kalman filter. This velocity is in the absolute coordinate system.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of shape (self.num_points, self.dim_points) containing the velocity estimate of the object on each axis.</p>"},{"location":"reference/tracker/#norfair.tracker.TrackedObject.estimate","title":"<code>estimate: np.ndarray</code>  <code>property</code>","text":"<p>Get the position estimate of the object from the Kalman filter.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of shape (self.num_points, self.dim_points) containing the position estimate of the object on each axis.</p>"},{"location":"reference/tracker/#norfair.tracker.TrackedObject.get_estimate","title":"<code>get_estimate(absolute=False)</code>","text":"<p>Get the position estimate of the object from the Kalman filter in an absolute or relative format.</p> <p>Parameters:</p> Name Type Description Default <code>absolute</code> <code>bool</code> <p>If true the coordinates are returned in absolute format, by default False, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of shape (self.num_points, self.dim_points) containing the position estimate of the object on each axis.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Alert if the coordinates are requested in absolute format but the tracker has no coordinate transformation.</p> Source code in <code>norfair/tracker.py</code> <pre><code>def get_estimate(self, absolute=False) -&gt; np.ndarray:\n    \"\"\"Get the position estimate of the object from the Kalman filter in an absolute or relative format.\n\n    Parameters\n    ----------\n    absolute : bool, optional\n        If true the coordinates are returned in absolute format, by default False, by default False.\n\n    Returns\n    -------\n    np.ndarray\n        An array of shape (self.num_points, self.dim_points) containing the position estimate of the object on each axis.\n\n    Raises\n    ------\n    ValueError\n        Alert if the coordinates are requested in absolute format but the tracker has no coordinate transformation.\n    \"\"\"\n    positions = self.filter.x.T.flatten()[: self.dim_z].reshape(-1, self.dim_points)\n    if self.abs_to_rel is None:\n        if not absolute:\n            return positions\n        else:\n            raise ValueError(\n                \"You must provide 'coord_transformations' to the tracker to get absolute coordinates\"\n            )\n    else:\n        if absolute:\n            return positions\n        else:\n            return self.abs_to_rel(positions)\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.TrackedObject.hit","title":"<code>hit(detection, period=1)</code>","text":"<p>Update tracked object with a new detection</p> <p>Parameters:</p> Name Type Description Default <code>detection</code> <code>Detection</code> <p>the new detection matched to this tracked object</p> required <code>period</code> <code>int</code> <p>frames corresponding to the period of time since last update.</p> <code>1</code> Source code in <code>norfair/tracker.py</code> <pre><code>def hit(self, detection: \"Detection\", period: int = 1):\n    \"\"\"Update tracked object with a new detection\n\n    Parameters\n    ----------\n    detection : Detection\n        the new detection matched to this tracked object\n    period : int, optional\n        frames corresponding to the period of time since last update.\n    \"\"\"\n    self._conditionally_add_to_past_detections(detection)\n\n    self.last_detection = detection\n    self.hit_counter = min(self.hit_counter + 2 * period, self.hit_counter_max)\n\n    if self.is_initializing and self.hit_counter &gt; self.initialization_delay:\n        self.is_initializing = False\n        self._acquire_ids()\n\n    # We use a kalman filter in which we consider each coordinate on each point as a sensor.\n    # This is a hacky way to update only certain sensors (only x, y coordinates for\n    # points which were detected).\n    # TODO: Use keypoint confidence information to change R on each sensor instead?\n    if detection.scores is not None:\n        assert len(detection.scores.shape) == 1\n        points_over_threshold_mask = detection.scores &gt; self.detection_threshold\n        matched_sensors_mask = np.array(\n            [(m,) * self.dim_points for m in points_over_threshold_mask]\n        ).flatten()\n        H_pos = np.diag(matched_sensors_mask).astype(\n            float\n        )  # We measure x, y positions\n        self.point_hit_counter[points_over_threshold_mask] += 2 * period\n    else:\n        points_over_threshold_mask = np.array([True] * self.num_points)\n        H_pos = np.identity(self.num_points * self.dim_points)\n        self.point_hit_counter += 2 * period\n    self.point_hit_counter[\n        self.point_hit_counter &gt;= self.pointwise_hit_counter_max\n    ] = self.pointwise_hit_counter_max\n    self.point_hit_counter[self.point_hit_counter &lt; 0] = 0\n    H_vel = np.zeros(H_pos.shape)  # But we don't directly measure velocity\n    H = np.hstack([H_pos, H_vel])\n    self.filter.update(\n        np.expand_dims(detection.absolute_points.flatten(), 0).T, None, H\n    )\n\n    detected_at_least_once_mask = np.array(\n        [(m,) * self.dim_points for m in self.detected_at_least_once_points]\n    ).flatten()\n    now_detected_mask = np.hstack(\n        (points_over_threshold_mask,) * self.dim_points\n    ).flatten()\n    first_detection_mask = np.logical_and(\n        now_detected_mask, np.logical_not(detected_at_least_once_mask)\n    )\n\n    self.filter.x[: self.dim_z][first_detection_mask] = np.expand_dims(\n        detection.absolute_points.flatten(), 0\n    ).T[first_detection_mask]\n\n    # Force points being detected for the first time to have velocity = 0\n    # This is needed because some detectors (like OpenPose) set points with\n    # low confidence to coordinates (0, 0). And when they then get their first\n    # real detection this creates a huge velocity vector in our KalmanFilter\n    # and causes the tracker to start with wildly inaccurate estimations which\n    # eventually coverge to the real detections.\n    self.filter.x[self.dim_z :][np.logical_not(detected_at_least_once_mask)] = 0\n    self.detected_at_least_once_points = np.logical_or(\n        self.detected_at_least_once_points, points_over_threshold_mask\n    )\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.TrackedObject.merge","title":"<code>merge(tracked_object)</code>","text":"<p>Merge with a not yet initialized TrackedObject instance</p> Source code in <code>norfair/tracker.py</code> <pre><code>def merge(self, tracked_object):\n    \"\"\"Merge with a not yet initialized TrackedObject instance\"\"\"\n    self.reid_hit_counter = None\n    self.hit_counter = self.initial_period * 2\n    self.point_hit_counter = tracked_object.point_hit_counter\n    self.last_distance = tracked_object.last_distance\n    self.current_min_distance = tracked_object.current_min_distance\n    self.last_detection = tracked_object.last_detection\n    self.detected_at_least_once_points = (\n        tracked_object.detected_at_least_once_points\n    )\n    self.filter = tracked_object.filter\n\n    for past_detection in tracked_object.past_detections:\n        self._conditionally_add_to_past_detections(past_detection)\n</code></pre>"},{"location":"reference/tracker/#norfair.tracker.Detection","title":"<code>Detection</code>","text":"<p>Detections returned by the detector must be converted to a <code>Detection</code> object before being used by Norfair.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>Points detected. Must be a rank 2 array with shape <code>(n_points, n_dimensions)</code> where n_dimensions is 2 or 3.</p> required <code>scores</code> <code>ndarray</code> <p>An array of length <code>n_points</code> which assigns a score to each of the points defined in <code>points</code>.</p> <p>This is used to inform the tracker of which points to ignore; any point with a score below <code>detection_threshold</code> will be ignored.</p> <p>This useful for cases in which detections don't always have every point present, as is often the case in pose estimators.</p> <code>None</code> <code>data</code> <code>Any</code> <p>The place to store any extra data which may be useful when calculating the distance function. Anything stored here will be available to use inside the distance function.</p> <p>This enables the development of more interesting trackers which can do things like assign an appearance embedding to each detection to aid in its tracking.</p> <code>None</code> <code>label</code> <code>Hashable</code> <p>When working with multiple classes the detection's label can be stored to be used as a matching condition when associating tracked objects with new detections. Label's type must be hashable for drawing purposes.</p> <code>None</code> <code>embedding</code> <code>Any</code> <p>The embedding for the reid_distance.</p> <code>None</code> Source code in <code>norfair/tracker.py</code> <pre><code>class Detection:\n    \"\"\"Detections returned by the detector must be converted to a `Detection` object before being used by Norfair.\n\n    Parameters\n    ----------\n    points : np.ndarray\n        Points detected. Must be a rank 2 array with shape `(n_points, n_dimensions)` where n_dimensions is 2 or 3.\n    scores : np.ndarray, optional\n        An array of length `n_points` which assigns a score to each of the points defined in `points`.\n\n        This is used to inform the tracker of which points to ignore;\n        any point with a score below `detection_threshold` will be ignored.\n\n        This useful for cases in which detections don't always have every point present, as is often the case in pose estimators.\n    data : Any, optional\n        The place to store any extra data which may be useful when calculating the distance function.\n        Anything stored here will be available to use inside the distance function.\n\n        This enables the development of more interesting trackers which can do things like assign an appearance embedding to each\n        detection to aid in its tracking.\n    label : Hashable, optional\n        When working with multiple classes the detection's label can be stored to be used as a matching condition when associating\n        tracked objects with new detections. Label's type must be hashable for drawing purposes.\n    embedding : Any, optional\n        The embedding for the reid_distance.\n    \"\"\"\n\n    def __init__(\n        self,\n        points: np.ndarray,\n        scores: np.ndarray = None,\n        data: Any = None,\n        label: Hashable = None,\n        embedding=None,\n    ):\n        self.points = validate_points(points)\n        self.scores = scores\n        self.data = data\n        self.label = label\n        self.absolute_points = self.points.copy()\n        self.embedding = embedding\n        self.age = None\n\n    def update_coordinate_transformation(\n        self, coordinate_transformation: CoordinatesTransformation\n    ):\n        if coordinate_transformation is not None:\n            self.absolute_points = coordinate_transformation.rel_to_abs(\n                self.absolute_points\n            )\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":""},{"location":"reference/utils/#norfair.utils.print_objects_as_table","title":"<code>print_objects_as_table(tracked_objects)</code>","text":"<p>Used for helping in debugging</p> Source code in <code>norfair/utils.py</code> <pre><code>def print_objects_as_table(tracked_objects: Sequence):\n    \"\"\"Used for helping in debugging\"\"\"\n    print()\n    console = Console()\n    table = Table(show_header=True, header_style=\"bold magenta\")\n    table.add_column(\"Id\", style=\"yellow\", justify=\"center\")\n    table.add_column(\"Age\", justify=\"right\")\n    table.add_column(\"Hit Counter\", justify=\"right\")\n    table.add_column(\"Last distance\", justify=\"right\")\n    table.add_column(\"Init Id\", justify=\"center\")\n    for obj in tracked_objects:\n        table.add_row(\n            str(obj.id),\n            str(obj.age),\n            str(obj.hit_counter),\n            f\"{obj.last_distance:.4f}\",\n            str(obj.initializing_id),\n        )\n    console.print(table)\n</code></pre>"},{"location":"reference/utils/#norfair.utils.get_cutout","title":"<code>get_cutout(points, image)</code>","text":"<p>Returns a rectangular cut-out from a set of points on an image</p> Source code in <code>norfair/utils.py</code> <pre><code>def get_cutout(points, image):\n    \"\"\"Returns a rectangular cut-out from a set of points on an image\"\"\"\n    max_x = int(max(points[:, 0]))\n    min_x = int(min(points[:, 0]))\n    max_y = int(max(points[:, 1]))\n    min_y = int(min(points[:, 1]))\n    return image[min_y:max_y, min_x:max_x]\n</code></pre>"},{"location":"reference/utils/#norfair.utils.warn_once","title":"<code>warn_once(message)</code>  <code>cached</code>","text":"<p>Write a warning message only once.</p> Source code in <code>norfair/utils.py</code> <pre><code>@lru_cache(maxsize=None)\ndef warn_once(message):\n    \"\"\"\n    Write a warning message only once.\n    \"\"\"\n    warn(message)\n</code></pre>"},{"location":"reference/video/","title":"Video","text":""},{"location":"reference/video/#norfair.video.Video","title":"<code>Video</code>","text":"<p>Class that provides a simple and pythonic way to interact with video.</p> <p>It returns regular OpenCV frames which enables the usage of the huge number of tools OpenCV provides to modify images.</p> <p>Parameters:</p> Name Type Description Default <code>camera</code> <code>Optional[int]</code> <p>An integer representing the device id of the camera to be used as the video source.</p> <p>Webcams tend to have an id of <code>0</code>. Arguments <code>camera</code> and <code>input_path</code> can't be used at the same time, one must be chosen.</p> <code>None</code> <code>input_path</code> <code>Optional[str]</code> <p>A string consisting of the path to the video file to be used as the video source.</p> <p>Arguments <code>camera</code> and <code>input_path</code> can't be used at the same time, one must be chosen.</p> <code>None</code> <code>output_path</code> <code>str</code> <p>The path to the output video to be generated. Can be a folder were the file will be created or a full path with a file name.</p> <code>'.'</code> <code>output_fps</code> <code>Optional[float]</code> <p>The frames per second at which to encode the output video file.</p> <p>If not provided it is set to be equal to the input video source's fps. This argument is useful when using live video cameras as a video source, where the user may know the input fps, but where the frames are being fed to the output video at a rate that is lower than the video source's fps, due to the latency added by the detector.</p> <code>None</code> <code>label</code> <code>str</code> <p>Label to add to the progress bar that appears when processing the current video.</p> <code>''</code> <code>output_fourcc</code> <code>Optional[str]</code> <p>OpenCV encoding for output video file. By default we use <code>mp4v</code> for <code>.mp4</code> and <code>XVID</code> for <code>.avi</code>. This is a combination that works on most systems but it results in larger files. To get smaller files use <code>avc1</code> or <code>H264</code> if available. Notice that some fourcc are not compatible with some extensions.</p> <code>None</code> <code>output_extension</code> <code>str</code> <p>File extension used for the output video. Ignored if <code>output_path</code> is not a folder.</p> <code>'mp4'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; video = Video(input_path=\"video.mp4\")\n&gt;&gt;&gt; for frame in video:\n&gt;&gt;&gt;     # &lt;&lt; Your modifications to the frame would go here &gt;&gt;\n&gt;&gt;&gt;     video.write(frame)\n</code></pre> Source code in <code>norfair/video.py</code> <pre><code>class Video:\n    \"\"\"\n    Class that provides a simple and pythonic way to interact with video.\n\n    It returns regular OpenCV frames which enables the usage of the huge number of tools OpenCV provides to modify images.\n\n    Parameters\n    ----------\n    camera : Optional[int], optional\n        An integer representing the device id of the camera to be used as the video source.\n\n        Webcams tend to have an id of `0`. Arguments `camera` and `input_path` can't be used at the same time, one must be chosen.\n    input_path : Optional[str], optional\n        A string consisting of the path to the video file to be used as the video source.\n\n        Arguments `camera` and `input_path` can't be used at the same time, one must be chosen.\n    output_path : str, optional\n        The path to the output video to be generated.\n        Can be a folder were the file will be created or a full path with a file name.\n    output_fps : Optional[float], optional\n        The frames per second at which to encode the output video file.\n\n        If not provided it is set to be equal to the input video source's fps.\n        This argument is useful when using live video cameras as a video source,\n        where the user may know the input fps,\n        but where the frames are being fed to the output video at a rate that is lower than the video source's fps,\n        due to the latency added by the detector.\n    label : str, optional\n        Label to add to the progress bar that appears when processing the current video.\n    output_fourcc : Optional[str], optional\n        OpenCV encoding for output video file.\n        By default we use `mp4v` for `.mp4` and `XVID` for `.avi`. This is a combination that works on most systems but\n        it results in larger files. To get smaller files use `avc1` or `H264` if available.\n        Notice that some fourcc are not compatible with some extensions.\n    output_extension : str, optional\n        File extension used for the output video. Ignored if `output_path` is not a folder.\n\n    Examples\n    --------\n    &gt;&gt;&gt; video = Video(input_path=\"video.mp4\")\n    &gt;&gt;&gt; for frame in video:\n    &gt;&gt;&gt;     # &lt;&lt; Your modifications to the frame would go here &gt;&gt;\n    &gt;&gt;&gt;     video.write(frame)\n    \"\"\"\n\n    def __init__(\n        self,\n        camera: Optional[int] = None,\n        input_path: Optional[str] = None,\n        output_path: str = \".\",\n        output_fps: Optional[float] = None,\n        label: str = \"\",\n        output_fourcc: Optional[str] = None,\n        output_extension: str = \"mp4\",\n    ):\n        self.camera = camera\n        self.input_path = input_path\n        self.output_path = output_path\n        self.label = label\n        self.output_fourcc = output_fourcc\n        self.output_extension = output_extension\n        self.output_video: Optional[cv2.VideoWriter] = None\n\n        # Input validation\n        if (input_path is None and camera is None) or (\n            input_path is not None and camera is not None\n        ):\n            raise ValueError(\n                \"You must set either 'camera' or 'input_path' arguments when setting 'Video' class\"\n            )\n        if camera is not None and type(camera) is not int:\n            raise ValueError(\n                \"Argument 'camera' refers to the device-id of your camera, and must be an int. Setting it to 0 usually works if you don't know the id.\"\n            )\n\n        # Read Input Video\n        if self.input_path is not None:\n            if \"~\" in self.input_path:\n                self.input_path = os.path.expanduser(self.input_path)\n            if not os.path.isfile(self.input_path):\n                self._fail(\n                    f\"[bold red]Error:[/bold red] File '{self.input_path}' does not exist.\"\n                )\n            self.video_capture = cv2.VideoCapture(self.input_path)\n            total_frames = int(self.video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n            if total_frames == 0:\n                self._fail(\n                    f\"[bold red]Error:[/bold red] '{self.input_path}' does not seem to be a video file supported by OpenCV. If the video file is not the problem, please check that your OpenCV installation is working correctly.\"\n                )\n            description = os.path.basename(self.input_path)\n        else:\n            self.video_capture = cv2.VideoCapture(self.camera)\n            total_frames = 0\n            description = f\"Camera({self.camera})\"\n        self.output_fps = (\n            output_fps\n            if output_fps is not None\n            else self.video_capture.get(cv2.CAP_PROP_FPS)\n        )\n        self.input_height = self.video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n        self.input_width = self.video_capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n        self.frame_counter = 0\n\n        # Setup progressbar\n        if self.label:\n            description += f\" | {self.label}\"\n        progress_bar_fields: List[Union[str, ProgressColumn]] = [\n            \"[progress.description]{task.description}\",\n            BarColumn(),\n            \"[yellow]{task.fields[process_fps]:.2f}fps[/yellow]\",\n        ]\n        if self.input_path is not None:\n            progress_bar_fields.insert(\n                2, \"[progress.percentage]{task.percentage:&gt;3.0f}%\"\n            )\n            progress_bar_fields.insert(\n                3,\n                TimeRemainingColumn(),\n            )\n        self.progress_bar = Progress(\n            *progress_bar_fields,\n            auto_refresh=False,\n            redirect_stdout=False,\n            redirect_stderr=False,\n        )\n        self.task = self.progress_bar.add_task(\n            self.abbreviate_description(description),\n            total=total_frames,\n            start=self.input_path is not None,\n            process_fps=0,\n        )\n\n    # This is a generator, note the yield keyword below.\n    def __iter__(self):\n        with self.progress_bar as progress_bar:\n            start = time.time()\n\n            # Iterate over video\n            while True:\n                self.frame_counter += 1\n                ret, frame = self.video_capture.read()\n                if ret is False or frame is None:\n                    break\n                process_fps = self.frame_counter / (time.time() - start)\n                progress_bar.update(\n                    self.task, advance=1, refresh=True, process_fps=process_fps\n                )\n                yield frame\n\n        # Cleanup\n        if self.output_video is not None:\n            self.output_video.release()\n            print(\n                f\"[white]Output video file saved to: {self.get_output_file_path()}[/white]\"\n            )\n        self.video_capture.release()\n        cv2.destroyAllWindows()\n\n    def _fail(self, msg: str):\n        raise RuntimeError(msg)\n\n    def write(self, frame: np.ndarray) -&gt; int:\n        \"\"\"\n        Write one frame to the output video.\n\n        Parameters\n        ----------\n        frame : np.ndarray\n            The OpenCV frame to write to file.\n\n        Returns\n        -------\n        int\n            _description_\n        \"\"\"\n        if self.output_video is None:\n            # The user may need to access the output file path on their code\n            output_file_path = self.get_output_file_path()\n            fourcc = cv2.VideoWriter_fourcc(*self.get_codec_fourcc(output_file_path))\n            # Set on first frame write in case the user resizes the frame in some way\n            output_size = (\n                frame.shape[1],\n                frame.shape[0],\n            )  # OpenCV format is (width, height)\n            self.output_video = cv2.VideoWriter(\n                output_file_path,\n                fourcc,\n                self.output_fps,\n                output_size,\n            )\n\n        self.output_video.write(frame)\n        return cv2.waitKey(1)\n\n    def show(self, frame: np.ndarray, downsample_ratio: float = 1.0) -&gt; int:\n        \"\"\"\n        Display a frame through a GUI. Usually used inside a video inference loop to show the output video.\n\n        Parameters\n        ----------\n        frame : np.ndarray\n            The OpenCV frame to be displayed.\n        downsample_ratio : float, optional\n            How much to downsample the frame being show.\n\n            Useful when streaming the GUI video display through a slow internet connection using something like X11 forwarding on an ssh connection.\n\n        Returns\n        -------\n        int\n            _description_\n        \"\"\"\n        # Resize to lower resolution for faster streaming over slow connections\n        if downsample_ratio != 1.0:\n            frame = cv2.resize(\n                frame,\n                (\n                    frame.shape[1] // downsample_ratio,\n                    frame.shape[0] // downsample_ratio,\n                ),\n            )\n        cv2.imshow(\"Output\", frame)\n        return cv2.waitKey(1)\n\n    def get_output_file_path(self) -&gt; str:\n        \"\"\"\n        Calculate the output path being used in case you are writing your frames to a video file.\n\n        Useful if you didn't set `output_path`, and want to know what the autogenerated output file path by Norfair will be.\n\n        Returns\n        -------\n        str\n            The path to the file.\n        \"\"\"\n        if not os.path.isdir(self.output_path):\n            return self.output_path\n\n        if self.input_path is not None:\n            file_name = self.input_path.split(\"/\")[-1].split(\".\")[0]\n        else:\n            file_name = \"camera_{self.camera}\"\n        file_name = f\"{file_name}_out.{self.output_extension}\"\n\n        return os.path.join(self.output_path, file_name)\n\n    def get_codec_fourcc(self, filename: str) -&gt; Optional[str]:\n        if self.output_fourcc is not None:\n            return self.output_fourcc\n\n        # Default codecs for each extension\n        extension = filename[-3:].lower()\n        if \"avi\" == extension:\n            return \"XVID\"\n        elif \"mp4\" == extension:\n            return \"mp4v\"  # When available, \"avc1\" is better\n        else:\n            self._fail(\n                f\"[bold red]Could not determine video codec for the provided output filename[/bold red]: \"\n                f\"[yellow]{filename}[/yellow]\\n\"\n                f\"Please use '.mp4', '.avi', or provide a custom OpenCV fourcc codec name.\"\n            )\n            return (\n                None  # Had to add this return to make mypya happy. I don't like this.\n            )\n\n    def abbreviate_description(self, description: str) -&gt; str:\n        \"\"\"Conditionally abbreviate description so that progress bar fits in small terminals\"\"\"\n        terminal_columns, _ = get_terminal_size()\n        space_for_description = (\n            int(terminal_columns) - 25\n        )  # Leave 25 space for progressbar\n        if len(description) &lt; space_for_description:\n            return description\n        else:\n            return \"{} ... {}\".format(\n                description[: space_for_description // 2 - 3],\n                description[-space_for_description // 2 + 3 :],\n            )\n</code></pre>"},{"location":"reference/video/#norfair.video.Video.write","title":"<code>write(frame)</code>","text":"<p>Write one frame to the output video.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The OpenCV frame to write to file.</p> required <p>Returns:</p> Type Description <code>int</code> <p>description</p> Source code in <code>norfair/video.py</code> <pre><code>def write(self, frame: np.ndarray) -&gt; int:\n    \"\"\"\n    Write one frame to the output video.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The OpenCV frame to write to file.\n\n    Returns\n    -------\n    int\n        _description_\n    \"\"\"\n    if self.output_video is None:\n        # The user may need to access the output file path on their code\n        output_file_path = self.get_output_file_path()\n        fourcc = cv2.VideoWriter_fourcc(*self.get_codec_fourcc(output_file_path))\n        # Set on first frame write in case the user resizes the frame in some way\n        output_size = (\n            frame.shape[1],\n            frame.shape[0],\n        )  # OpenCV format is (width, height)\n        self.output_video = cv2.VideoWriter(\n            output_file_path,\n            fourcc,\n            self.output_fps,\n            output_size,\n        )\n\n    self.output_video.write(frame)\n    return cv2.waitKey(1)\n</code></pre>"},{"location":"reference/video/#norfair.video.Video.show","title":"<code>show(frame, downsample_ratio=1.0)</code>","text":"<p>Display a frame through a GUI. Usually used inside a video inference loop to show the output video.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The OpenCV frame to be displayed.</p> required <code>downsample_ratio</code> <code>float</code> <p>How much to downsample the frame being show.</p> <p>Useful when streaming the GUI video display through a slow internet connection using something like X11 forwarding on an ssh connection.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>int</code> <p>description</p> Source code in <code>norfair/video.py</code> <pre><code>def show(self, frame: np.ndarray, downsample_ratio: float = 1.0) -&gt; int:\n    \"\"\"\n    Display a frame through a GUI. Usually used inside a video inference loop to show the output video.\n\n    Parameters\n    ----------\n    frame : np.ndarray\n        The OpenCV frame to be displayed.\n    downsample_ratio : float, optional\n        How much to downsample the frame being show.\n\n        Useful when streaming the GUI video display through a slow internet connection using something like X11 forwarding on an ssh connection.\n\n    Returns\n    -------\n    int\n        _description_\n    \"\"\"\n    # Resize to lower resolution for faster streaming over slow connections\n    if downsample_ratio != 1.0:\n        frame = cv2.resize(\n            frame,\n            (\n                frame.shape[1] // downsample_ratio,\n                frame.shape[0] // downsample_ratio,\n            ),\n        )\n    cv2.imshow(\"Output\", frame)\n    return cv2.waitKey(1)\n</code></pre>"},{"location":"reference/video/#norfair.video.Video.get_output_file_path","title":"<code>get_output_file_path()</code>","text":"<p>Calculate the output path being used in case you are writing your frames to a video file.</p> <p>Useful if you didn't set <code>output_path</code>, and want to know what the autogenerated output file path by Norfair will be.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path to the file.</p> Source code in <code>norfair/video.py</code> <pre><code>def get_output_file_path(self) -&gt; str:\n    \"\"\"\n    Calculate the output path being used in case you are writing your frames to a video file.\n\n    Useful if you didn't set `output_path`, and want to know what the autogenerated output file path by Norfair will be.\n\n    Returns\n    -------\n    str\n        The path to the file.\n    \"\"\"\n    if not os.path.isdir(self.output_path):\n        return self.output_path\n\n    if self.input_path is not None:\n        file_name = self.input_path.split(\"/\")[-1].split(\".\")[0]\n    else:\n        file_name = \"camera_{self.camera}\"\n    file_name = f\"{file_name}_out.{self.output_extension}\"\n\n    return os.path.join(self.output_path, file_name)\n</code></pre>"},{"location":"reference/video/#norfair.video.Video.abbreviate_description","title":"<code>abbreviate_description(description)</code>","text":"<p>Conditionally abbreviate description so that progress bar fits in small terminals</p> Source code in <code>norfair/video.py</code> <pre><code>def abbreviate_description(self, description: str) -&gt; str:\n    \"\"\"Conditionally abbreviate description so that progress bar fits in small terminals\"\"\"\n    terminal_columns, _ = get_terminal_size()\n    space_for_description = (\n        int(terminal_columns) - 25\n    )  # Leave 25 space for progressbar\n    if len(description) &lt; space_for_description:\n        return description\n    else:\n        return \"{} ... {}\".format(\n            description[: space_for_description // 2 - 3],\n            description[-space_for_description // 2 + 3 :],\n        )\n</code></pre>"}]}